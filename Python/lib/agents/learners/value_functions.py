# -*- coding: utf-8 -*-
"""
Created on Mon Mar 30 19:57:16 2020

@author: Daniel Mastropietro
@description: Definition of value function estimators.

The value functions used by the learner classes are assumed to:
a) Be defined in terms of weights, e.g. the state value function for state s is V(s,w),
where w is the vector of weights applied to a set of features X.
Note that this assumption does NOT offer any limitation, since a tabular value function
can be defined using binary/dummy features. 

b) Have the following methods defined:
- reset(): resets the vector w of weigths to their initial estimates 
- getWeights(): reads the vector w of weights
- setWeights(): updates the vector w of weights
- setWeight(): updates the value of the weight for a particular state
- getValue(): reads the value function for a particular state or state-action
- getValues(): reads the value function for ALL states or state-actions
"""

import warnings

import numpy as np

# TODO: (2020/04/10) This class should cover ALL value functions whose estimation is done via approximation (linear or non-linear)
# (i.e. using a parameterized expression whose parameters are materialized as a vector of weights)
# So the constructor should receive:
# - the dimension of the weights
# - the features x that are affected by the weights (possibly in a linear manner or perhaps in a nonlinear manner as well!)
class ValueFunctionApprox:
    "Class that contains information about the estimation of the state value function"

    def __init__(self, nS: int):
        "nS is the number of states"
        self.nS = nS
        self.weights = np.zeros(nS)
        # Here we implement the tabular value function, where the features x(s) of each state s
        # are dummy or indicator variables, i.e. all equal to 0 except at the coordinate corresponding to state s.
        # These dummy variables are stored for all states in the feature matrix X, where states and features
        # are laid out so that the value of a state s is computed as w'x(s), i.e. the inner product between the
        # vector of weights w and the vector of features for state s, x(s).
        # Therefore the feature matrix X should have the dimension (#features) x (#states),
        # meaning that features vary along the rows of X and states vary along the columns of X.
        # In the dummy feature matrix case, where each feature is the indicator variable of a state
        # the X matrix is a diagonal matrix.
        self.X = np.eye(self.nS)

        # Reset the weights to their initial estimation (e.g. all zeros) 
        self.reset()

    def reset(self):
        "Resets the weights to their initial estimates (i.e. erase all learning memory!)"
        self.weights[:] = 0

    #--- GETTERS
    def getWeights(self):
        return self.weights

    def getValue(self, state: int):
        if not self.isValidState(state):
            return None
        return np.dot(self.weights, self.X[:,state])

    def getValues(self):
        return np.dot(self.weights, self.X)

    #--- SETTERS
    def setWeight(self, state: int, weight: float):
        if not self.isValidState(state):
            return -1
        self.weights[state] = weight

    def setWeights(self, weights: np.ndarray):
        self.weights = weights

    def setValue(self, state: int, value: float):
        """
        Sets the value of the state to the given value.

        Since the value function is computed as the inner product between the weights and the features vector,
        in the general non-dummy-features case, there are several options for the weights so that its inner
        product with the features vector is equal to the given value.

        The choice is done to set all weights to 0 except the weight for the "first" feature in the features vector
        of the given state, which is set equal to value / first_non_zero_feature_value. The "first" condition is
        determined by sweeping the features in the order they are stored in the feature matrix X stored in the object.

        Context for this function: in the 1D gridworld we want to set the estimated value function of terminal states
        to be equal to the reward received when *reaching* those terminal states. The main goal is to improve the visual
        experience generated by the plot of the true and the estimated value functions.
        NOTE however that this change of the estimated value function should be done ONLY at the end of the episode as
        during learning the value of terminal states MUST be 0. In fact, if this is not the case, its value will be
        added to the reward observed when transitioning to the terminal state (in the calculation of the TD error =
        R(T) + gamma*V(S(T)) - V(S(T-1)), thus making the TD error equal to R(T) + R(T) - V(S(T-1)) = 2R(T) - V(S(T-1))
        --when gamma = 1-- and this doesn't look right because R(T) appears twice!

        Arguments:
        state: int
            State for which the value should be set.

        value: float
            Value to set.
        """
        if self.isValidState(state):
            # Look for the first non-zero feature component for the given state (i.e. the first non-zero value of X[:,state])
            # and use it to set its weight equal to value / X[feature, state]
            is_weight_set = False
            for feature in range(self.X.shape[0]):
                if self.X[feature, state] != 0.0:
                    self.weights[feature] = value / self.X[feature, state]
                    is_weight_set = True
                    break

            if not is_weight_set:
                warnings.warn("No feature for state {} was found to be non-zero. The state value was NOT set to {}." \
                              .format(state, value))

    def isValidState(self, state):
        "Checks if a state is valid by assuming it is an index between 0 and one less the number of states"
        if not (0 <= state < self.nS):
            warnings.warn("Invalid state ({}). It should be between 0 and {}. Nothing to do.".format(state, self.nS-1))
            return False
        return True
