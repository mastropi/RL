# -*- coding: utf-8 -*-
"""
Created on Mon Mar 30 19:57:16 2020

@author: Daniel Mastropietro
@description: Definition of value function estimators.

The value functions used by the learner classes are assumed to:
a) Be defined in terms of weights, e.g. the state value function for state s is V(s,w),
where w is the vector of weights applied to a set of features X.
Note that this assumption does NOT offer any limitation, since a tabular value function
can be defined using binary/dummy features. 

b) Have the following methods defined:
- reset(): resets the vector w of weigths to their initial estimates 
- getWeights(): reads the vector w of weights
- setWeights(): updates the vector w of weights
- _setWeight(): updates the value of the weight for a particular state or state-action assuming dummy features
- getValue(): reads the value function for a particular state or state-action
- getValues(): reads the value function for ALL states or state-actions
"""

import warnings

import numpy as np

from Python.lib.agents.learners import ResetMethod


class LinearValueFunctionApprox:
    """
    Generic class for classes implementing linear value function approximations on concepts of interest (e.g. normally states and actions)

    Arguments:
    nS: int
        Number of states on which the approximation function is defined.

    terminal_states: list
        List containing the indices of the terminal states, whose value should always be 0.
        This is used by the reset() method which can reset the value function to different initial guesses
        (e.g. random values), so that the value of terminal states is always reset to 0.
    """
    def __init__(self, nS: int, terminal_states: list):
        self.nS = nS
        self.terminal_states = terminal_states

        # Attributes to be implemented by the subclasses
        self.weights = None
        self.X = None

    def reset(self, method=ResetMethod.ALLZEROS, params_random: dict=None, seed: int=None):
        """
        Resets the weights of the linear function approximation using the specified reset method

        Arguments:
        method: ResetMethod
            Method to use to reset the weights.
            Currently the following methods are implemented: ResetMethod.ALLZEROS, ResetMethod.RANDOM_UNIFORM,
            ResetMethod.RANDOM_NORMAL.
            `None` is equivalent to ResetMethod.ALLZEROS.
            If none of these methods is given, the np.random.rand() method is used which generates a random number
            in [0, 1).

        params_random: (opt) dict
            Dictionary with the relevant parameters for the distribution to use for the pseudo-random generator
            when method is not ResetMethod.ALLZEROS.
            For ResetMethod.RANDOM_UNIFORM: 'min', 'max', with default values 0.0 and 1.0
            For ResetMethod.RANDOM_NORMAL: 'loc', 'scale', with default values 0.0 and 1.0
            default: None, in which case the default values of the pseudo-random generation method specified above are used.

        seed: (opt) int
            Seed to use for the pseudo-random number generator.
            If the seed is None, no seed is set by this method.
            Note that, when seed != None, the set of random numbers generated by this method is always the same.
            This useful so that the state values can be always reset to the same set of values at e.g. the beginning
            of an experiment.
        """
        if method is None or method == ResetMethod.ALLZEROS:
            self.weights[:] = 0.0
        else:
            # Random weights generation
            # NOTE: We use np.random() as opposed to e.g. np_random() because the latter function
            # (defined in gym.utils.seeding) is mostly called from within environments,
            # e.g. in gym.envs.toy_text.discrete.seed(), and this class defines value functions approximations.
            # In addition, we keep separately the pseudo-random number generation affecting the trajectories
            # from the pseudo-random generation affecting the value function initialization.
            if seed is not None:
                np.random.seed(seed)
            if method == ResetMethod.RANDOM_UNIFORM:
                min = params_random.get('min', 0.0) if params_random is not None else 0.0
                max = params_random.get('max', 1.0) if params_random is not None else 1.0
                self.weights[:] = np.random.uniform(min, max, size=len(self.weights))
            elif method == ResetMethod.RANDOM_NORMAL:
                loc = params_random.get('loc', 0.0) if params_random is not None else 0.0
                scale = params_random.get('scale', 1.0) if params_random is not None else 1.0
                self.weights[:] = np.random.normal(loc, scale, size=len(self.weights))
            else:
                self.weights[:] = np.random.rand(len(self.weights))

    #--- GETTERS
    def getWeights(self):
        return self.weights

    def getValues(self):
        "Returns the function values"
        return np.dot(self.weights, self.X)

    def _getValue(self, indcol: int):
        "Returns the value of the concept (e.g. normally a state or a state-action) indexed by the given `indcol` (which indexes a column of the feature matrix X)"
        # Inner product between the weights and all the features associated to the given
        return np.dot(self.weights, self.X[:, indcol])

    #--- SETTERS
    def setWeights(self, weights: np.ndarray):
        self.weights = weights

    def _setValue(self, indcol: int, value: float):
        """
        Sets the value of the concept indexed by the given `indcol` (which indexes a column of the feature matrix X) to the given value

        It returns True if the value could be set, o.w. it returns False. This happens when no feature in the feature matrix X for the given `indcol` column
        is different from zero.
        """
        # Look for the first non-zero feature component for the given concept (referenced by `indcol`) (i.e. the first non-zero value of X[:, indcol])
        # and use it to set its weight equal to value / X[feature, indcol], so that we get the given value when calling self.getValue(indcol),
        # i.e. we get the given value when multiplying the weights with X[feature, indcol].
        is_weight_set = False
        for feature in range(self.X.shape[0]):
            if self.X[feature, indcol] != 0.0:
                self.weights[feature] = value / self.X[feature, indcol]
                is_weight_set = True
                break

        return is_weight_set

# TODO: (2020/04/10) This class should cover ALL state value functions whose estimation is done via approximation (linear or non-linear)
# (i.e. using a parameterized expression whose parameters are materialized as a vector of weights)
# So the constructor should receive:
# - the dimension of the weights
# - the features x that are affected by the weights (possibly in a linear manner or perhaps in a nonlinear manner as well!)
class StateValueFunctionApprox(LinearValueFunctionApprox):
    """
    Class that contains information about the estimation of a state value function, V(s)

    Arguments:
    nS: int
        Number of states on which the value function is defined.

    terminal_states: list
        List containing the indices of the terminal states, whose value should always be 0.
        This is used by the reset() method which can reset the value function to different initial guesses
        (e.g. random values), so that the value of terminal states is always reset to 0.
    """
    def __init__(self, nS: int, terminal_states: list):
        super().__init__(nS, terminal_states)

        self.weights = np.zeros(nS)
        # Here we implement the tabular value function, where the features x(s) of each state s
        # are dummy or indicator variables, i.e. all equal to 0 except at the coordinate corresponding to state s.
        # These dummy variables are stored for all states in the feature matrix X, where states and features
        # are laid out so that the value of a state s is computed as w'x(s), i.e. the inner product between the
        # vector of weights w and the vector of features for state s, x(s).
        # Therefore the feature matrix X should have the dimension (#features) x (#states),
        # meaning that features vary along the rows of X and states vary along the columns of X.
        # In the dummy feature matrix case, where each feature is the indicator variable of a state
        # the X matrix is a diagonal matrix.
        self.X = np.eye(self.nS)

    def reset(self, method=ResetMethod.ALLZEROS, params_random: dict=None, seed: int=None):
        "Resets the weights using the specified reset method. See the super class documentation for more details"
        super().reset(method=method, params_random=params_random, seed=seed)

        # Set the value of terminal states to 0 (this is the definition of the value of terminal states)
        for s in self.terminal_states:
            self.setValue(s, 0.0)

    #--- GETTERS
    def getValue(self, state: int):
        if not self.isValidState(state):
            return None
        return super()._getValue(state)

    #--- SETTERS
    def _setWeight(self, state: int, weight: float):
        """
        Sets the weight of the given state in the case where the features are dummy features

        NOTE that in the general case of function approximation, the weight would have a reduced dimension
        and thus would be indexed by something else, whose meaning depends on what the reduction dimension consists of.
        In those cases we should use the setWeights() method which does not assume dummy features.

        Because of the assumption of dummy features by this method, we use an underscore at the front of its name.
        """
        if not self.isValidState(state):
            return -1
        self.weights[state] = weight

    def setValue(self, state: int, value: float):
        """
        Sets the value of the state to the given value.

        Since the value function is computed as the inner product between the weights and the features vector,
        in the general non-dummy-features case, there are several options for the weights so that its inner
        product with the features vector is equal to the given value.

        The choice is done to set all weights to 0 except the weight for the "first" feature in the features vector
        of the given state, which is set equal to value / first_non_zero_feature_value. The "first" condition is
        determined by sweeping the features in the order they are stored in the feature matrix X stored in the object.

        Context for this function: in the 1D gridworld we want to set the estimated value function of terminal states
        to be equal to the reward received when *reaching* those terminal states. The main goal is to improve the visual
        experience generated by the plot of the true and the estimated value functions.
        NOTE however that this change of the estimated value function should be done ONLY at the end of the episode as
        during learning the value of terminal states MUST be 0. In fact, if this is not the case, its value will be
        added to the reward observed when transitioning to the terminal state (in the calculation of the TD error =
        R(T) + gamma*V(S(T)) - V(S(T-1)), thus making the TD error equal to R(T) + R(T) - V(S(T-1)) = 2R(T) - V(S(T-1))
        --when gamma = 1-- and this doesn't look right because R(T) appears twice!

        Arguments:
        state: int
            State for which the value should be set.

        value: float
            Value to set.
        """
        if self.isValidState(state):
            is_weight_set = super()._setValue(state, value)
            if not is_weight_set:
                warnings.warn("No feature for state {} was found to be non-zero. The state value was NOT set to {}." \
                              .format(state, value))

    def isValidState(self, state):
        "Checks if a state is valid by assuming it is an index between 0 and one less the number of states"
        if not (0 <= state < self.nS):
            warnings.warn("Invalid state ({}). It should be between 0 and {}. Nothing to do.".format(state, self.nS-1))
            return False
        return True


class ActionValueFunctionApprox(LinearValueFunctionApprox):
    """
    Class that contains information about the estimation of an action value function, Q(s,a)

    The current implementation delivers only a TABULAR value function where the matrix of features X is
    a the identity matrix of size nS*nA x nS*nA.
    (In a general setting the number of columns in the X matrix represents the number of all possible state-actions
    and the number of rows represents the number of features, which is normally less than the number of all possible state-actions.)

    Likewise the vector of weights has size nS*nA (but normally it would have a smaller size) and each entry
    directly gives the action value for each state-action (s,a).

    The order in which the action values are stored is "grouped by state", e.g. if there are nS = 3 states and nA = 4 actions,
    the first 4 diagonal elements correspond to state = 0 and actions = 0, 1, 2, 3;
    the second 4 diagonal elements correspond to state = 1 and actions = 0, 1, 2, 3;
    and so forth.
    (This order is defined by the getLinearIndex() method.)

    Arguments:
    nS: int
        Number of states on which the action value function is defined.

    nA: int
        Number of actions on which the action value function is defined.

    terminal_states: list
        List containing the indices of the terminal states, whose value should always be 0.
        This is used by the reset() method which can reset the value function to different initial guesses
        (e.g. random values), so that the value of terminal states is always reset to 0.
    """
    def __init__(self, nS: int, nA: int, terminal_states: list):
        super().__init__(nS, terminal_states)

        self.nA = nA
        # For now we define full dimensional weights, i.e. as many as the number of all possible state-actions.
        # In general, the dimension of the weights would be smaller than the number of state-actions, but for now
        # we are implementing the tabular setting.
        self.weights = np.zeros(self.nS * self.nA)
        # Accordingly to the full dimensional weights, we define a full dimensional features matrix X
        self.X = np.eye(self.nS * self.nA)

    def reset(self, method=ResetMethod.ALLZEROS, params_random: dict=None, seed: int=None):
        "Resets the action value function using the specified reset method. See the super class documentation for more details"
        super().reset(method=method, params_random=params_random, seed=seed)

        # Set the value of terminal states to 0 (for all actions), as this is the definition of the value of terminal states
        for s in self.terminal_states:
            for a in range(self.nA):
                self.setValue(s, a, 0.0)

    #--- GETTERS
    def getLinearIndex(self, state, action):
        """
        Returns the linear index to access the feature vector associated to the given state-action in the X matrix of features,
        which is a COLUMN of X.
        """
        return state*self.nA + action

    def getValue(self, state: int, action: int):
        if not self.isValidState(state) or not self.isValidAction(state, action):
            return None
        return super()._getValue(self.getLinearIndex(state, action))

    #--- SETTERS
    def _setWeight(self, state: int, action: int, weight: float):
        """
        Sets the weight of the given state-action in the case where the features are dummy features

        NOTE that in the general case of function approximation, the weight would have a reduced dimension
        and thus would be indexed by something else, whose meaning depends on what the reduction dimension consists of.
        In those cases we should use the setWeights() method which does not assume dummy features.

        Because of the assumption of dummy features by this method, we use an underscore at the front of its name.
        """
        if not self.isValidState(state) or not self.isValidAction(state, action):
            return -1
        self.weights[self.getLinearIndex(state, action)] = weight

    def setValue(self, state: int, action: int, value: float):
        """
        Sets the value of the state-action to the given value.

        Since the value function is computed as the inner product between the weights and the features vector,
        in the general non-dummy-features case, there are several options for the weights so that its inner
        product with the features vector is equal to the given value.

        The choice is done to set all weights to 0 except the weight for the "first" feature in the features vector
        of the given state, which is set equal to value / first_non_zero_feature_value. The "first" condition is
        determined by sweeping the features in the order they are stored in the feature matrix X stored in the object.

        This function is used to set the value of terminal states to 0 (which is the case by definition of terminal states).

        Arguments:
        state: int
            State for which the value should be set.

        action: int
            Action for which the value should be set.

        value: float
            Value to set.
        """
        if self.isValidState(state) and self.isValidAction(state, action):
            is_weight_set = super()._setValue(self.getLinearIndex(state, action), value)
            if not is_weight_set:
                warnings.warn("No feature for state-action ({}, {}) was found to be non-zero. The state value was NOT set to {}." \
                              .format(state, action, value))

    def isValidState(self, state):
        "Checks if a state is valid by assuming it is an index between 0 and one less the number of states"
        if not (0 <= state < self.nS):
            warnings.warn("Invalid state ({}). It should be between 0 and {}. Nothing to do.".format(state, self.nS-1))
            return False
        return True

    def isValidAction(self, state, action):
        """
        Checks if an action is valid for the given state

        Currently, it is assumed that the possible actions are all the same for any states and that they
        are an index between 0 and one less the number of actions. So, the state information is NOT used.
        """
        if not (0 <= action < self.nA):
            warnings.warn("Invalid action ({}). It should be between 0 and {}. Nothing to do.".format(action, self.nA-1))
            return False
        return True
