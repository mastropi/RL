# -*- coding: utf-8 -*-
"""
Created on Mon Mar 30 19:57:16 2020

@author: Daniel Mastropietro
@description: Definition of value function estimators.

The value functions used by the learner classes are assumed to:
a) Be defined in terms of weights, e.g. the state value function for state s is V(s,w),
where w is the vector of weights applied to a set of features X.
Note that this assumption does NOT offer any limitation, since a tabular value function
can be defined using binary/dummy features. 

b) Have the following methods defined:
- reset(): resets the vector w of weigths to their initial estimates 
- getWeights(): reads the vector w of weights
- setWeights(): updates the vector w of weights
- _setWeight(): updates the value of the weight for a particular state or state-action assuming dummy features
- getValue(): reads the value function for a particular state or state-action
- getValues(): reads the value function for ALL states or state-actions
"""

import warnings
from typing import Union

import numpy as np

import torch

from Python.lib.agents.learners import ResetMethod

from Python.lib.estimators.nn_models import InputLayer, NNBackprop

from Python.lib.utils.basic import is_scalar


class LinearValueFunctionApprox:
    """
    Generic class for classes implementing linear value function approximations on concepts of interest (e.g. normally states and actions)

    Arguments:
    nS: int
        Number of states on which the approximation function is defined.

    terminal_states: list
        List containing the indices of the terminal states, whose value should always be 0.
        This is used by the reset() method which can reset the value function to different initial guesses
        (e.g. random values), so that the value of terminal states is always reset to 0.
    """
    def __init__(self, nS: int, terminal_states: list):
        self.nS = nS
        self.terminal_states = terminal_states

        # Attributes to be implemented by the subclasses
        self.weights = None
        self.X = None

    def reset(self, method=ResetMethod.ALLZEROS, params_random: dict=None, seed: int=None):
        """
        Resets the weights of the linear function approximation using the specified reset method

        Arguments:
        method: ResetMethod
            Method to use to reset the weights.
            Currently the following methods are implemented: ResetMethod.ALLZEROS, ResetMethod.RANDOM_UNIFORM,
            ResetMethod.RANDOM_NORMAL.
            `None` is equivalent to ResetMethod.ALLZEROS.
            If none of these methods is given, the np.random.rand() method is used which generates a random number
            in [0, 1).

        params_random: (opt) dict
            Dictionary with the relevant parameters for the distribution to use for the pseudo-random generator
            when method is not ResetMethod.ALLZEROS.
            For ResetMethod.RANDOM_UNIFORM: 'min', 'max', with default values 0.0 and 1.0
            For ResetMethod.RANDOM_NORMAL: 'loc', 'scale', with default values 0.0 and 1.0
            default: None, in which case the default values of the pseudo-random generation method specified above are used.

        seed: (opt) int
            Seed to use for the pseudo-random number generator.
            If the seed is None, no seed is set by this method.
            Note that, when seed != None, the set of random numbers generated by this method is always the same.
            This useful so that the state values can be always reset to the same set of values at e.g. the beginning
            of an experiment.
        """
        if method is None or method == ResetMethod.ALLZEROS:
            self.weights[:] = 0.0
        else:
            # Random weights generation
            # NOTE: We use np.random() as opposed to e.g. np_random() because the latter function
            # (defined in gym.utils.seeding) is mostly called from within environments,
            # e.g. in gym.envs.toy_text.discrete.seed(), and this class defines value functions approximations.
            # In addition, we keep separately the pseudo-random number generation affecting the trajectories
            # from the pseudo-random generation affecting the value function initialization.
            if seed is not None:
                np.random.seed(seed)
            if method == ResetMethod.RANDOM_UNIFORM:
                min = params_random.get('min', 0.0) if params_random is not None else 0.0
                max = params_random.get('max', 1.0) if params_random is not None else 1.0
                self.weights[:] = np.random.uniform(min, max, size=len(self.weights))
            elif method == ResetMethod.RANDOM_NORMAL:
                loc = params_random.get('loc', 0.0) if params_random is not None else 0.0
                scale = params_random.get('scale', 1.0) if params_random is not None else 1.0
                self.weights[:] = np.random.normal(loc, scale, size=len(self.weights))
            else:
                self.weights[:] = np.random.rand(len(self.weights))

    #--- GETTERS
    def getWeights(self):
        return self.weights

    def getValues(self):
        "Returns the function values"
        return np.dot(self.weights, self.X)

    def _getValue(self, indcol: int):
        "Returns the value of the concept (e.g. normally a state or a state-action) indexed by the given `indcol` (which indexes a column of the feature matrix X)"
        # Inner product between the weights and all the features associated to the given
        return np.dot(self.weights, self.X[:, indcol])

    #--- SETTERS
    def setWeights(self, weights: np.ndarray):
        self.weights = weights

    def _setValue(self, indcol: int, value: float):
        """
        Sets the value of the concept indexed by the given `indcol` (which indexes a column of the feature matrix X) to the given value

        It returns True if the value could be set, o.w. it returns False. This happens when no feature in the feature matrix X for the given `indcol` column
        is different from zero.
        """
        # Look for the first non-zero feature component for the given concept (referenced by `indcol`) (i.e. the first non-zero value of X[:, indcol])
        # and use it to set its weight equal to value / X[feature, indcol], so that we get the given value when calling self.getValue(indcol),
        # i.e. we get the given value when multiplying the weights with X[feature, indcol].
        is_weight_set = False
        for feature in range(self.X.shape[0]):
            if self.X[feature, indcol] != 0.0:
                self.weights[feature] = value / self.X[feature, indcol]
                is_weight_set = True
                break

        return is_weight_set

    def isTabular(self):
        return True


# TODO: (2020/04/10) This class should cover ALL state value functions whose estimation is done via approximation (linear or non-linear)
# (i.e. using a parameterized expression whose parameters are materialized as a vector of weights)
# So the constructor should receive:
# - the dimension of the weights
# - the features x that are affected by the weights (possibly in a linear manner or perhaps in a nonlinear manner as well!)
class StateValueFunctionApprox(LinearValueFunctionApprox):
    """
    Class that contains information about the estimation of a state value function, V(s)

    Arguments:
    nS: int
        Number of states on which the value function is defined.

    terminal_states: list
        List containing the indices of the terminal states, whose value should always be 0.
        This is used by the reset() method which can reset the value function to different initial guesses
        (e.g. random values), so that the value of terminal states is always reset to 0.
    """
    def __init__(self, nS: int, terminal_states: list):
        super().__init__(nS, terminal_states)

        self.weights = np.zeros(nS)
        # Here we implement the tabular value function, where the features x(s) of each state s
        # are dummy or indicator variables, i.e. all equal to 0 except at the coordinate corresponding to state s.
        # These dummy variables are stored for all states in the feature matrix X, where states and features
        # are laid out so that the value of a state s is computed as w'x(s), i.e. the inner product between the
        # vector of weights w and the vector of features for state s, x(s).
        # Therefore the feature matrix X should have the dimension (#features) x (#states),
        # meaning that features vary along the rows of X and states vary along the columns of X.
        # In the dummy feature matrix case, where each feature is the indicator variable of a state
        # the X matrix is a diagonal matrix.
        self.X = np.eye(self.nS)

    def reset(self, method=ResetMethod.ALLZEROS, params_random: dict=None, seed: int=None):
        "Resets the weights using the specified reset method. See the super class documentation for more details"
        super().reset(method=method, params_random=params_random, seed=seed)

        # Set the value of terminal states to 0 (this is the definition of the value of terminal states)
        for s in self.terminal_states:
            self.setValue(s, 0.0)

    #--- GETTERS
    def getValue(self, state: int):
        if not self.isValidState(state):
            return None
        return super()._getValue(state)

    #--- SETTERS
    def _setWeight(self, state: int, weight: float):
        """
        Sets the weight of the given state in the case where the features are dummy features

        NOTE that in the general case of function approximation, the weight would have a reduced dimension
        and thus would be indexed by something else, whose meaning depends on what the reduction dimension consists of.
        In those cases we should use the setWeights() method which does not assume dummy features.

        Because of the assumption of dummy features by this method, we use an underscore at the front of its name.
        """
        if not self.isValidState(state):
            return -1
        self.weights[state] = weight

    def setValue(self, state: int, value: float):
        """
        Sets the value of the state to the given value.

        Since the value function is computed as the inner product between the weights and the features vector,
        in the general non-dummy-features case, there are several options for the weights so that its inner
        product with the features vector is equal to the given value.

        The choice is done to set all weights to 0 except the weight for the "first" feature in the features vector
        of the given state, which is set equal to value / first_non_zero_feature_value. The "first" condition is
        determined by sweeping the features in the order they are stored in the feature matrix X stored in the object.

        Context for this function: in the 1D gridworld we want to set the estimated value function of terminal states
        to be equal to the reward received when *reaching* those terminal states. The main goal is to improve the visual
        experience generated by the plot of the true and the estimated value functions.
        NOTE however that this change of the estimated value function should be done ONLY at the end of the episode as
        during learning the value of terminal states MUST be 0. In fact, if this is not the case, its value will be
        added to the reward observed when transitioning to the terminal state (in the calculation of the TD error =
        R(T) + gamma*V(S(T)) - V(S(T-1)), thus making the TD error equal to R(T) + R(T) - V(S(T-1)) = 2R(T) - V(S(T-1))
        --when gamma = 1-- and this doesn't look right because R(T) appears twice!

        Arguments:
        state: int
            State for which the value should be set.

        value: float
            Value to set.
        """
        if self.isValidState(state):
            is_weight_set = super()._setValue(state, value)
            if not is_weight_set:
                warnings.warn("No feature for state {} was found to be non-zero. The state value was NOT set to {}." \
                              .format(state, value))

    def isValidState(self, state):
        "Checks if a state is valid by assuming it is an index between 0 and one less the number of states"
        if not (0 <= state < self.nS):
            warnings.warn("Invalid state ({}). It should be between 0 and {}. Nothing to do.".format(state, self.nS-1))
            return False
        return True


class ActionValueFunctionApprox(LinearValueFunctionApprox):
    """
    Class that contains information about the estimation of an action value function, Q(s,a)

    The current implementation delivers only a TABULAR value function where the matrix of features X is
    a the identity matrix of size nS*nA x nS*nA.
    (In a general setting the number of columns in the X matrix represents the number of all possible state-actions
    and the number of rows represents the number of features, which is normally less than the number of all possible state-actions.)

    Likewise the vector of weights has size nS*nA (but normally it would have a smaller size) and each entry
    directly gives the action value for each state-action (s,a).

    The order in which the action values are stored is "grouped by state", e.g. if there are nS = 3 states and nA = 4 actions,
    the first 4 diagonal elements correspond to state = 0 and actions = 0, 1, 2, 3;
    the second 4 diagonal elements correspond to state = 1 and actions = 0, 1, 2, 3;
    and so forth.
    (This order is defined by the getLinearIndex() method.)

    Arguments:
    nS: int
        Number of states on which the action value function is defined.

    nA: int
        Number of actions on which the action value function is defined.

    terminal_states: list
        List containing the indices of the terminal states, whose value should always be 0.
        This is used by the reset() method which can reset the value function to different initial guesses
        (e.g. random values), so that the value of terminal states is always reset to 0.
    """
    def __init__(self, nS: int, nA: int, terminal_states: list):
        super().__init__(nS, terminal_states)

        self.nA = nA
        # For now we define full dimensional weights, i.e. as many as the number of all possible state-actions.
        # In general, the dimension of the weights would be smaller than the number of state-actions, but for now
        # we are implementing the tabular setting.
        self.weights = np.zeros(self.nS * self.nA)
        # Accordingly to the full dimensional weights, we define a full dimensional features matrix X
        self.X = np.eye(self.nS * self.nA)

    def reset(self, method=ResetMethod.ALLZEROS, params_random: dict=None, seed: int=None):
        "Resets the action value function using the specified reset method. See the super class documentation for more details"
        super().reset(method=method, params_random=params_random, seed=seed)

        # Set the value of terminal states to 0 (for all actions), as this is the definition of the value of terminal states
        for s in self.terminal_states:
            for a in range(self.nA):
                self.setValue(s, a, 0.0)

    #--- GETTERS
    def getLinearIndex(self, state, action):
        """
        Returns the linear index to access the feature vector associated to the given state-action in the X matrix of features,
        which is a COLUMN of X.
        """
        return state*self.nA + action

    def getValue(self, state: int, action: int):
        if not self.isValidState(state) or not self.isValidAction(state, action):
            return None
        return super()._getValue(self.getLinearIndex(state, action))

    #--- SETTERS
    def _setWeight(self, state: int, action: int, weight: float):
        """
        Sets the weight of the given state-action in the case where the features are dummy features

        NOTE that in the general case of function approximation, the weight would have a reduced dimension
        and thus would be indexed by something else, whose meaning depends on what the reduction dimension consists of.
        In those cases we should use the setWeights() method which does not assume dummy features.

        Because of the assumption of dummy features by this method, we use an underscore at the front of its name.
        """
        if not self.isValidState(state) or not self.isValidAction(state, action):
            return -1
        self.weights[self.getLinearIndex(state, action)] = weight

    def setValue(self, state: int, action: int, value: float):
        """
        Sets the value of the state-action to the given value.

        Since the value function is computed as the inner product between the weights and the features vector,
        in the general non-dummy-features case, there are several options for the weights so that its inner
        product with the features vector is equal to the given value.

        The choice is done to set all weights to 0 except the weight for the "first" feature in the features vector
        of the given state, which is set equal to value / first_non_zero_feature_value. The "first" condition is
        determined by sweeping the features in the order they are stored in the feature matrix X stored in the object.

        This function is used to set the value of terminal states to 0 (which is the case by definition of terminal states).

        Arguments:
        state: int
            State for which the value should be set.

        action: int
            Action for which the value should be set.

        value: float
            Value to set.
        """
        if self.isValidState(state) and self.isValidAction(state, action):
            is_weight_set = super()._setValue(self.getLinearIndex(state, action), value)
            if not is_weight_set:
                warnings.warn("No feature for state-action ({}, {}) was found to be non-zero. The state value was NOT set to {}." \
                              .format(state, action, value))

    def isValidState(self, state):
        "Checks if a state is valid by assuming it is an index between 0 and one less the number of states"
        if not (0 <= state < self.nS):
            warnings.warn("Invalid state ({}). It should be between 0 and {}. Nothing to do.".format(state, self.nS-1))
            return False
        return True

    def isValidAction(self, state, action):
        """
        Checks if an action is valid for the given state

        Currently, it is assumed that the possible actions are all the same for any states and that they
        are an index between 0 and one less the number of actions. So, the state information is NOT used.
        """
        if not (0 <= action < self.nA):
            warnings.warn("Invalid action ({}). It should be between 0 and {}. Nothing to do.".format(action, self.nA-1))
            return False
        return True


class ValueFunctionApproxNN:
    """
    Class that can be used to approximate a value function, e.g. V(s) or Q(s,a), using a neural network

    Arguments:
    nn_input: int
        Number of input neurons.

    nn_hidden_layer_sizes: (opt) list
        List with the number of neurons in each hidden layer.
        default: []

    optimizer: (opt) torch.optim
        Torch optimizer to use.
        default: AdamW

    lr: (opt) float
        Learning rate for the optimizer.
        default: 0.001 (Adam's default)
    """
    def __init__(self, nn_input: Union[InputLayer, int], nn_hidden_layer_sizes: list=[], dropout=0.0, optimizer=torch.optim.AdamW, lr=0.001):
        # Neural network model
        self.nn_model = NNBackprop(nn_input, nn_hidden_layer_sizes, 1, dict_activation_functions=dict({'hidden': [torch.nn.ReLU] * len(nn_hidden_layer_sizes)}), dropout=dropout)
        self.loss = torch.nn.MSELoss()
        self.optimizer = optimizer(self.nn_model.parameters(), lr=lr)

    def reset(self, method=ResetMethod.ALLZEROS, params_random=None, seed=None):
        "Resets the value function to random values for every state or to the given initial values, optionally using a seed for the random initialization of the neural network weights"
        self.init_value(value=params_random, seed=seed)

    def init_value(self, value=None, eps=1E-2, seed=None):
        """
        Initializes the parameters of the neural network so that the output value is either almost the same for all input states
        or is almost equal to `value`, also for all input states.

        Neural network parameters (weights and biases) are initialized using a zero-mean normal distribution
        with a small standard deviation (compared to 1) given by `eps`, except for the bias of the neuron in the output layer which is initialized
        so that the output value is almost equal to the given `value`
        (we write "almost" because there is noise in the output value induced by the almost zero values
        to which the other weights and biases are initialized to, but this is perfectly ok for most purposes).

        We need to initialize parameters randomly because, if all parameters were initialized to 0, their values would never change during the learning process(!)
        as the gradient would be always 0 due to the chain rule (easy to prove).

        Arguments:
        value: (opt) float
            Value defining the output of the neural network for each state being modeled.
            default: None, in which case the output value is initialized using a standard normal distribution

        eps: (opt) positive float
            Small value defining the standard deviation of the normal distribution used to define the weights and biases of all layers except for
            the biases of the neurons in the output layer.
            default: 1E-2

        seed: (opt) int
            Seed to use in the random initialization of the neural network weights as described above.
            default: None
        """
        # Inspired by the weights_init() function in this code:
        # https://github.com/pytorch/examples/blob/main/dcgan/main.py
        # Note also the use of Module.apply() method which calls a function on each submodule of a module (e.g. of a neural network).

        if seed is not None:
            torch.nn.init.torch.manual_seed(seed)  # manual_seed() does not accept `None`

        # The initial parameters are set from a standard normal distribution around 0 with small variance so that all weights are about 0
        for p in self.nn_model.parameters():
            torch.nn.init.normal_(p, 0, eps)
        # Store in a variable the last parameter which is the bias of the output neuron
        bias_output_layer = p

        # If a specific output value is requested, it is set via the bias of the output neuron (which is the last parameter in nn_model.parameters() retrieved above)
        if value is not None:
            # Initialize the bias of the output neuron to `value`
            if not (is_scalar(value) or len(value) != 1):
                raise ValueError(f"Parameter `value` must be a scalar or have length 1: {value}")
            torch.nn.init.constant_(bias_output_layer, 0.1)

    def getModel(self):
        return self.nn_model

    def getValues(self):
        """
        Returns this object

        This method is defined so that we can still use this function approximation in simulators that work with discrete states,
        so that when the simulators call getValues(), instead of returning the state value for each possible state,
        it receives the object that enables them to compute the state value for any continuous-valued state.
        """
        return self

    def isTabular(self):
        return False


class StateValueFunctionApproxNN(ValueFunctionApproxNN):
    """
    Class that can be used to approximate a state value function V(s) using a neural network

    Arguments:
    nn_input: int
        Number of input neurons which represents the dimension of the environment state (e.g. 2 for 2D states (x, v)).

    nn_hidden_layer_sizes: (opt) list
        List with the number of neurons in each hidden layer.
        default: []
    """
    def __init__(self, nn_input: Union[InputLayer, int], nn_hidden_layer_sizes: list=[],  dropout=0.0, optimizer=torch.optim.AdamW, lr=0.001):
        super().__init__(nn_input, nn_hidden_layer_sizes, dropout=dropout, optimizer=optimizer, lr=lr)

    def _compute_loss(self, state, delta):
        """
        Computes the loss for a given state and a given delta value, an estimate of the error to minimize

        That is, the error to minimize is:
            error = target - pred(target)
        where target = v(s), the true state value function at state `s`.

        However, the target value is NOT known, therefore we estimate it using an estimate of the return, e.g. a one-step estimate of the return, G(t),
        as done by TD(0). So, in TD(0), "estimated target" = R(n+1) + gamma*V(S(n+1))
        which is used in the computation of the estimated error value, delta, i.e.:
            delta = "estimated error" = "estimated target" - pred(target)
        =>  "estimated target" = delta + pred(target)
        where pred(target) = V(s) = the value of the given (start) state before taking the action that gave rise to `delta`.

        We have to do the above calculation simply because the loss function being used is an error function that receives the target value and the predicted
        value as input, which are used to compute the error incurred by the predicted value in predicting the target, which is then used to compute the loss.
        If the loss function used received the error as input argument, we wouldn't need to do the above computation.

        Ref: Sutton, Chapter 9, pag. 201 "Stochastic gradient and Semi-gradient Methods"
        """
        pred_value = self._getValue(state)
        estimated_target_value = delta + pred_value
        loss = self.loss(estimated_target_value, pred_value)
        return loss

    def update_weights(self, state, delta):
        "Given the state, updates the weights of the neural network as alpha*delta*grad(V) (gradient ascent corresponding to the mean squared error loss function)"
        loss = self._compute_loss(state, delta)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    def _getValue(self, state):
        "Returns the value of a state"
        state_value = self.nn_model(torch.tensor(state))
        return state_value

    def getValue(self, state):
        return self._getValue(state).item()

    def getGradient(self, state, delta):
        loss = self._compute_loss(state, delta)
        # TODO: (2024/08/12) This does not return the gradient, need to iterate on all the layers of the network and compute the gradient on each the weights connecting it with the next layer
        # Ref: https://discuss.pytorch.org/t/how-to-print-the-computed-gradient-values-for-a-network/34179/8
        gradient = loss.grad
        return gradient


class ActionValueFunctionApproxNN(ValueFunctionApproxNN):
    """
    Class that can be used to approximate an action value function Q(s,a) using a neural network

    Arguments:
    nn_input: int
        Number of input neurons which represents the dimension of the environment state (e.g. 2 for 2D states (x, v)) + 1 for the action.

    nn_hidden_layer_sizes: (opt) list
        List with the number of neurons in each hidden layer.
        default: []
    """
    def __init__(self, nn_input: Union[InputLayer, int], nn_hidden_layer_sizes: list=[],  dropout=0.0, optimizer=torch.optim.AdamW, lr=0.001):
        super().__init__(nn_input, nn_hidden_layer_sizes, dropout=dropout, optimizer=optimizer, lr=lr)

    def _compute_loss(self, state, action, delta):
        """
        Computes the loss for a given state and action, and a given delta value, an estimation of the error to minimize.

        See the details in the documentation for StateValueFunctionApproxNN._compute_loss().
        """
        pred_value = self._getValue(state, action)
        estimated_target_value = delta + pred_value
        loss = self.loss(estimated_target_value, pred_value)
        return loss

    def update_weights(self, state, action, delta):
        "Given the state, updates the weights of the neural network as alpha*delta*grad(V) (gradient ascent corresponding to the mean squared error loss function)"
        loss = self._compute_loss(state, action, delta)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

    def _getValue(self, state, action):
        "Returns the value of a state and action"
        action_value = self.nn_model(torch.tensor(np.concatenate([np.array(state).reshape(-1), np.array([action])])))
        return action_value

    def getValue(self, state, action):
        return self._getValue(state, action).item()

    def getGradient(self, state, action, delta):
        loss = self._compute_loss(state, action, delta)
        # TODO: (2024/08/12) This does not return the gradient, need to iterate on all the layers of the network and compute the gradient on each the weights connecting it with the next layer
        # Ref: https://discuss.pytorch.org/t/how-to-print-the-computed-gradient-values-for-a-network/34179/8
        gradient = loss.grad
        return gradient


if __name__ == "__main__":
    import copy
    import pandas as pd
    import torch

    import Python.lib.agents as agents
    from Python.lib.agents.learners import LearningCriterion, LearningTask
    from Python.lib.agents.learners.episodic.discrete import td, fv
    from Python.lib.agents.policies.parameterized import PolNN
    from Python.lib.environments.mountaincars import MountainCarDiscrete
    from Python.lib.simulators.discrete import Simulator
    from Python.lib.utils.computing import compute_set_of_frequent_states_with_zero_reward

    #-- General settings
    seed = 1317
    debug = True

    #-- Environment characteristics
    env_mc = MountainCarDiscrete(nx=20, nv=20, factor_for_force_and_gravity=10, seed_reset=seed, debug=debug)
    nS = env_mc.getNumStates()

    #-- Value function learner characteristics
    nn_hidden_layer_sizes = [4]
    dict_function_approximations = dict({'V': StateValueFunctionApproxNN(nn_input=env_mc.dim, nn_hidden_layer_sizes=nn_hidden_layer_sizes),
                                         'Q': ActionValueFunctionApproxNN(nn_input=env_mc.dim + 1, nn_hidden_layer_sizes=nn_hidden_layer_sizes)})   # +1 for the action

    #-- Simulation characteristics
    N = 5
    T = max_time_steps = 50

    #-- Policy characteristics
    nn_model = NNBackprop(input_size=env_mc.dim, hidden_sizes=nn_hidden_layer_sizes, output_size=3, dict_activation_functions=dict({'hidden': [torch.nn.ReLU] * len(nn_hidden_layer_sizes)}))
    policy_nn = PolNN(env_mc, nn_model, seed=seed)
    print(f"Neural network to model the policy:\n{nn_model}")

    # Initialize the policy to the given initial policy
    policy_nn.reset(initial_values=None)
    print(f"Network parameters initialized as follows:\n{list(policy_nn.getThetaParameter())}")

    # Estimate the Absorption set
    # Perform an initial exploration of the environment in order to define the absorption set based on visit frequency and observed non-zero rewards
    # In this excursion, the start state is defined by the environment's initial state distribution.
    threshold_absorption_set = 0.50
    print(f"\nEstimating the absorption set based on cumulative relative visit frequency (<= {threshold_absorption_set}) from an initial exploration of the environment...")
    learner_for_initial_exploration = td.LeaTDLambda(env_mc,
                                                     dict_function_approximations=dict_function_approximations,
                                                     criterion=LearningCriterion.AVERAGE,
                                                     task=LearningTask.CONTINUING,
                                                     gamma=1.0,
                                                     lmbda=0.0,
                                                     alpha=1.0,
                                                     adjust_alpha=True,
                                                     adjust_alpha_by_episode=False,
                                                     alpha_min=0.1,
                                                     debug=False)
    agent_for_initial_exploration = agents.GenericAgent(policy_nn, learner_for_initial_exploration)
    sim_for_initial_exploration = Simulator(env_mc, agent_for_initial_exploration, debug=debug)

    learner = sim_for_initial_exploration.run_exploration(t_learn=0, max_time_steps=max_time_steps, seed=seed, verbose=debug, verbose_period=1)
    # States visited during the exploration
    pd.DataFrame({'state': learner.getStates(), 'index': [env_mc.getIndexFromState(state) for state in learner.getStates()]}, columns=['state', 'index'])

    # Discretize the states of the environment in order to estimate the absorption set A
    state_indices = [env_mc.getIndexFromState(state) for state in learner.getStates()]
    absorption_set = compute_set_of_frequent_states_with_zero_reward(state_indices, learner.getRewards(), threshold=threshold_absorption_set)
    print(f"Distribution of state frequency on n={learner.getNumSteps()} steps:\n{pd.Series(learner.getStates()).value_counts(normalize=True)}")
    print(f"Distribution of state frequency on n={learner.getNumSteps()} steps:\n{pd.Series(state_indices).value_counts(normalize=True)}")
    print(f"\nSelected absorption set (1D-index, 2D-discrete) ({len(absorption_set)} states):")
    for s in absorption_set:
        print(str(s) + ': ' + str(env_mc.get_state_discrete_from_index(s)))
    ## OK!

    # Now explore and learn the value functions
    learner_after_learning, nsteps = sim_for_initial_exploration.run_exploration_and_learn_value_functions(max_time_steps=10, seed=seed, verbose=debug, verbose_period=1)

    # Now run the single Markov chain under a continuning learning task
    V, Q, A, state_counts_et, _, _, learning_info = sim_for_initial_exploration._run_single_continuing_task(max_time_steps=20, set_cycle=absorption_set, seed=seed, verbose=debug, verbose_period=1)
    print(learning_info['probas_stationary_exit_cycle_set'])

    # FV simulation
    learner_fv = fv.LeaFV(env_mc,
                          N, T, absorption_set, activation_set=None,
                          states_of_interest=env_mc.getTerminalStates(),
                          probas_stationary_start_state_et=None,
                          probas_stationary_start_state_fv=None,
                          dict_function_approximations=dict_function_approximations,
                          criterion=LearningCriterion.AVERAGE,
                          gamma=1.0,
                          lmbda=0.0,
                          alpha=1.0,
                          adjust_alpha=True,
                          adjust_alpha_by_episode=False,
                          alpha_min=0.1,
                          debug=debug)
    agent_nn_fv = agents.GenericAgent(policy_nn.copy(), learner_fv)
    sim_fv = Simulator(env_mc, agent_nn_fv, debug=debug)

    envs = [copy.deepcopy(env_mc) for _ in range(N)]
    n_events_fv, state_values, action_values, advantage_values, state_counts_fv, phi, df_proba_surv, expected_absorption_time, max_survival_time = \
        sim_fv._run_simulation_fv(0, envs,
                  absorption_set,
                  start_set=None,
                  max_time_steps=500,
                  max_time_steps_for_absorbed_particles_check=500,
                  min_prop_absorbed_particles=0.90,
                  stop_if_prop_absorbed_particles_reached_regardless_of_time_steps=True,
                  dist_proba_for_start_state=learning_info['probas_stationary_exit_cycle_set'],
                  expected_absorption_time=10.3,
                  estimated_average_reward=0.8,
                  epsilon_random_action=0.1,
                  seed=131713,
                  verbose=True,
                  verbose_period=1)
