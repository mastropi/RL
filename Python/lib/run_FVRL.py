# -*- coding: utf-8 -*-
"""
Created on Sun Jul 11 08:42:57 2022

@author: Daniel Mastropietro
@description: Runs the FVRL algorithm to learn the optimum parameter of a parameterized policy.
"""

if __name__ == "__main__":
    # Only run this when running the script, o.w. it may give an error when importing functions if setup.py is not found
    import runpy
    runpy.run_path('../../setup.py')

import os
import sys
import shutil
import warnings
import tracemalloc
import optparse

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from matplotlib.ticker import MaxNLocator
from timeit import default_timer as timer

from Python.lib.agents.learners.continuing.fv import LeaFV
from Python.lib.agents.learners.continuing.mc import LeaMC
from Python.lib.agents.learners.policies import LeaPolicyGradient

from Python.lib.agents.policies.parameterized import PolQueueTwoActionsLinearStep

from Python.lib.agents.queues import AgeQueue

from Python.lib.environments.queues import COST_EXP_BUFFER_SIZE_REF, Actions, BufferType, rewardOnJobRejection_ExponentialCost, rewardOnJobRejection_ByClass

from Python.lib.simulators import LearningMethod
from Python.lib.simulators.queues import compute_nparticles_and_narrivals_for_fv_process, \
    compute_rel_errors_for_fv_process, define_queue_environment_and_agent, get_deterministic_blocking_boundaries, \
    LearningMode, SimulatorQueue

from Python.lib.utils.basic import aggregation_bygroups, array_of_objects, convert_str_argument_to_list_of_type, \
    convert_str_to_list_of_type, is_scalar, show_exec_params
from Python.lib.utils.computing import compute_blocking_probability_birth_death_process, compute_expected_cost_knapsack
import Python.lib.utils.plotting as plotting


PLOT_GRADIENT = False                   # Whether to plot the estimated gradient of the average reward at each learning step
# Constants that are used for information purposes only: each section started with checking the value of the following
# constants are currently (11-Jul-2022) written to be executed manually, as the data to plot is read from files specified each time.
PLOT_RESULTS_TOGETHER = False           # Whether to plot the theta-learning trajectories of the FVRL and the MC algorithms on the same graph
PLOT_RESULTS_PAPER = False              # Whether to generate the plots with the theta learning for the paper


#---------------------------- Auxiliary functions ---------------------------#
def run_simulation_policy_learning(simul, replications, dict_params_simul, dict_info,
                                   dict_params_info: dict = {'plot': False, 'log': False},
                                   params_read_from_benchmark_file=False, benchmark=None,
                                   seed=None, verbose=False):
    """

    Arguments:
    simul: simulator object (e.g. of class SimulatorQueue)
        Simulator object used to run the simulation.
        It should have the run() method defined.

    replications: int
        Number of replications to run.

    dict_params_simul: dict
        Set of simulation and estimation parameters.
        This is passed to the `dict_simul` parameter of the simul.run() method.

    dict_info: dict
        Dictionary containing piece of information about the system setup.
        This is passed to the `dict_info` parameter of the simul.run() method.

    dict_params_info: (opt) dict
        Set of parameters related to information handling as it is generated by the simulation process.
        This is passed to the `dict_params_info` parameter of the simul.run() method.
        default: {'plot': False, 'log': False}

    params_read_from_benchmark_file:  bool
        Whether the simulation parameters are read from a benchmark file, normally generated by a prior execution of FVRL.

    benchmark: (opt) pandas DataFrame
        Data frame containing the benchmark information for each replication run for each case of analysis.
        Only used when params_read_from_benchmark_file = True.
        default: None

    seed: (opt) int
        Seed to use at the first learning step for the first replication.
        The seed for the first learning step for subsequent replications is set as `seed + 200*(r - 1)` where r is
        the replication number starting at 1.
        Since the strategy for the seed definition value is only one and used by whether we are running the simulation
        with or without a benchmark file, the seeds used by FVRL and by MC will be the same for each learning step
        of each replication in the analyzed case, so comparison is even fairer than if the seeds between FVRL and MC
        were different.
        default: None

    verbose: (opt) bool
        Whether to be verbose when running the simulation.
        default: False

    Return: Tuple
    Duple with the following elements:
    - theta_opt_values: list containing the optimum theta values found for each replication run on the analyzed case.
    - K_opt_values: list containing the optimum deterministic blocking sizes.
    - cost_opt: optimum expected cost associated to the optimum theta values found (this does NOT need to be the actual optimum expected cost!)
    - df_learning: pandas data frame containing the results of the learning process for each replication run on the analyzed case
    as generated by simul.run().
    """
    is_theta_unidimensional = lambda theta: is_scalar(theta) or len(theta) == 1

    set_required_entries_simul = {'theta_true', 'theta_start', 'buffer_size_activation_factor', 'nparticles', 't_sim',
                                  'burnin_time_steps'}
    set_required_entries_info = {'case', 'ncases', 'learning_method', 'exponent',
                                 'rhos', 'K_true', 'K', 'error_rel_phi', 'error_rel_et',
                                 'alpha_start', 'adjust_alpha', 'min_time_to_update_alpha', 'alpha_min'}
    if not set_required_entries_simul.issubset(dict_params_simul.keys()):
        raise ValueError("Missing entries in the dict_params_simul dictionary: {}" \
                         .format(set_required_entries_simul.difference(dict_params_simul.keys())))
    if not set_required_entries_info.issubset(dict_info.keys()):
        raise ValueError("Missing entries in the dict_info dictionary: {}" \
                         .format(set_required_entries_info.difference(dict_info.keys())))

    if not params_read_from_benchmark_file and is_theta_unidimensional(dict_params_simul['theta_true']):
        # Only compute the *real* expected relative error values for the estimation of Phi and the estimation of E(T_A)
        # in unidimensional problems, which is where we know how to compute them, based on the actual values of
        # the number of particles 'nparticles' and the number of arrival events 't_sim'.
        error_rel_phi_real, error_rel_et_real = compute_rel_errors_for_fv_process(dict_info['rhos'], dict_info['K'],
                                                                                  dict_params_simul['buffer_size_activation_factor'],
                                                                                  dict_params_simul['nparticles'],
                                                                                  dict_params_simul['t_sim'])

        print("\n--> CASE {} of {}: theta_true={} (K_true={}), theta={} (K={}), J/K={:.3f}," \
              " exponent={}: N={} (err_nom={:.1f}%, err={:.1f}%), T={} (err_nom={:.1f}%, err={:.1f}%)" \
              .format(dict_info['case'], dict_info['ncases'], dict_params_simul['theta_true'], dict_info['K_true'],
                      dict_params_simul['theta_start'], dict_info['K'],
                      dict_params_simul['buffer_size_activation_factor'],
                      dict_info['exponent'], dict_params_simul['nparticles'], dict_info['error_rel_phi'] * 100,
                      error_rel_phi_real * 100,
                      dict_params_simul['t_sim'], dict_info['error_rel_et'] * 100, error_rel_et_real * 100))
    else:
        print("\n--> CASE {} of {}: #replications={}, theta_true={} (K_true={}), theta={} (K={}), J/K={}," \
              " exponent={}: N={}, T={})" \
              .format(dict_info['case'], replications, dict_info['ncases'], dict_params_simul['theta_true'], dict_info['K_true'],
                      dict_params_simul['theta_start'], dict_info['K'],
                      dict_params_simul['buffer_size_activation_factor'],
                      dict_info['exponent'], dict_params_simul['nparticles'], dict_params_simul['t_sim']))
        # The values of N and T are read from the benchmark file and the respective actual errors are also reported there.
        # Here we set these values to NaN because they are shown below as part of the parameter settings.
        error_rel_phi_real = np.nan
        error_rel_et_real = np.nan

    # Store the number of particles and number of arrival events so that we can reset them at the start of every replication
    # (only important when running FVRL, because when running the MC learner, their values are read from the benchmark file)
    nparticles0 = dict_params_simul['nparticles']
    t_sim0 = dict_params_simul['t_sim']

    for r in range(1, replications+1):
        print("\n=== Running replication {} of {} ===".format(r, replications))
        # Reset the simulator object so that we start from scratch and set the simulation case number (used to identify the parameter settings used in the simulation)
        simul.reset(reset_learning_history=True, reset_value_functions=True, reset_counts=True)
        simul.setCase(dict_info['case'])
        simul.setReplication(r)

        # Reset the values of the number of particles and number of arrival events
        # (only important when running FVRL, as in the MC learner case, their values are read from the benchmark file)
        dict_params_simul['nparticles'] = nparticles0
        dict_params_simul['t_sim'] = t_sim0

        # Show execution parameters
        params = dict({
            '0(a)-Seed (for first replication)': seed,
            '1(a)-System-#Servers': simul.getEnv().getNumServers(),
            '1(b)-System-JobClassRates': simul.getEnv().getJobClassRates(),
            '1(c)-System-ServiceRates': simul.getEnv().getServiceRates(),
            '1(d)-System-TrueTheta': dict_params_simul['theta_true'],
            '1(e)-System-TrueK': dict_info['K_true'],
            '2(a)-Learning-Method': dict_info['learning_method'],
            '2(b)-Learning-Method#Particles and % Rel Error Phi': (dict_params_simul['nparticles'], error_rel_phi_real * 100),
            '2(c)-Learning-Method#TimeSteps/ArrivalEvents and % Rel Error E(T)': (dict_params_simul['t_sim'], error_rel_et_real * 100),
            '2(d)-Learning-Method#BurnInSteps (BITS)': dict_params_simul['burnin_time_steps'],
            '2(e)-Learning-Method#MinNumCycles': dict_params_simul['min_num_cycles_for_expectations'],
            '2(f)-Learning-LearningMode': simul.dict_params_learning['mode'].name,
            '2(g)-Learning-ThetaStart': dict_params_simul['theta_start'],
            '2(h)-Learning-#Steps': simul.getNumLearningSteps(),
            '2(i)-Learning-AlphaStart': dict_info['alpha_start'],
            '2(j)-Learning-AdjustAlpha?': dict_info['adjust_alpha'],
            '2(k)-Learning-MinEpisodeToAdjustAlpha': dict_info['min_time_to_update_alpha'],
            '2(l)-Learning-AlphaMin': dict_info['alpha_min'],
        })
        show_exec_params(params)

        if params_read_from_benchmark_file:
            # Get the seed for the first learning step and number of events to run the simulation for (from the benchmark file)
            benchmark_this_case_and_replication = benchmark[(benchmark['case'] == dict_info['case']) & (benchmark['replication'] == r)]
            seed = benchmark_this_case_and_replication['seed'].iloc[0]

            # Number of learning steps
            t_learn = benchmark_this_case_and_replication['t_learn'].iloc[-1]
            simul.setNumLearningSteps(t_learn)

            # Number of steps by learning step
            # This is set to the total number of events observed during the FVRL benchmark divided by the number of learning steps,
            # so that the MC learning has the same number of events at each learning step.
            dict_params_simul['t_sim'] = int( np.ceil( np.sum( list(benchmark_this_case_and_replication['n_events_mc'] +
                                                                    benchmark_this_case_and_replication['n_events_fv'])  )
                                                       / t_learn ) )

        _, _, df_learning = simul.run(dict_params_simul,
                                      dict_params_info=dict_params_info,
                                      dict_info=dict_info,
                                      seed=seed + 200*(r - 1), verbose=verbose)    # We multiply r by 200 in order to reduce the chances of overlapping with other seeds (e.g. the seeds used for each learning step)
        # Add a column with the replication number
        df_learning = pd.concat([pd.DataFrame({'replication': [r] * df_learning.shape[0]}, dtype=int), df_learning], axis=1)

        # Concatenate to gather the results for all replications
        if r == 1:
            df_learning_all = df_learning
        else:
            # We concatenate the new results to all the previous results using ignore_index=False
            # so that each replication has the same index number for each learning step, and this
            # allows us to easily extract the optimum theta found at each replication when returning them below.
            df_learning_all = pd.concat([df_learning_all, df_learning], axis=0, ignore_index=False)

    # Return the optimum theta found for each replication and the data frame containing the results for all replications
    theta_opt_values = df_learning_all['theta_next'].loc[simul.getNumLearningSteps()-1]
    if is_scalar(theta_opt_values):
        # This happens when only one replication was run
        theta_opt_values = [theta_opt_values]
    # Compute the optimum blocking sizes (using the ROUND function instead of the CEIL function so that we take into account to what integer value is theta closest
    # (e.g. theta = 3.1 => K = 4 instead of K = 5 (which is too large for theta very close to 3, as when theta = 3, K is 4)
    K_opt_values =  [get_deterministic_blocking_boundaries(simul.agent, t, exact=False) for t in theta_opt_values]
    # Optimum expected cost.
    # NOTE: This is NOT the expected cost at 'theta_next' just retrieved above but the expected cost at 'theta' i.e. before the update of theta
    # (but still the two values are expected to be similar)
    # I don't compute the expected cost at 'theta_next' because I don't have yet in place a function that I can easily
    # call to compute it (we would need first to compute the stationary probabilities in order to call the function that
    # is already in place for its computation, namely `estimate_expected_reward(env, agent, probas_stationary_true)`)
    cost_opt = -df_learning_all['expected_reward_true'].loc[simul.getNumLearningSteps()-1]
    return theta_opt_values,\
           K_opt_values, \
           cost_opt, \
           df_learning_all


def compute_optimum_blocking_sizes_and_expected_cost(simul: SimulatorQueue, dict_params_environment: dict, func_optimum: callable=min):
    """
    Computes the optimum blocking sizes and the corresponding optimum expected cost of an acceptance policy

    Arguments:
    simul: SimulatorQueue
        Simulator object that should have a queue environment (`getEnv()` method retrieves it) and an `agent` attribute
        defined which should have a parameterized policy or set of parameterized policies.

    dict_params_environment: dict
        Dictionary containing the environment parameters used when the simulator object was instantiated
        (by e.g. function define_queue_environment_and_agent()).
        It should contain the following entries:
        a) for a single-buffer queue systems:
            - 'reward_func': callable that computes the blocking cost.
            - 'reward_func_params': dictionary containing the parameters received by the callable given in 'reward_func'.
        b) for a no-buffer/loss-network system:
            - 'capacity': capacity of the system.
            - 'job_class_rates': arriving job rates by class.
            - 'service_rates': service rates by each arriving job class.
            - 'reward_func_params': dictionary of reward function parameters, which should have the following entries:
                - 'reward_at_rejection': list containing the reward (negative) for rejecting an incoming job of class i, i = 0, 1, ..., len(job_class_rates)-1.

    func_optimum: callable
        Callable stating the function on which the optimum is computed. This is only used in no-buffer/loss-network systems.

    Return: tuple
    Tuple with the following two elements:
    - optimum blocking sizes: list of integers or list of lists containing the optimum blocking sizes based on the dict_params_environment['reward_func']
    (for single-buffer queue systems, in which we have a list of integers --e.g. [8]) or on the parameters defined in `dict_params_environment`
    (for systems with no buffer, i.e. loss networks, in which we have a list of lists -- e.g. [[4, 7, 10]]).
    - optimum_expected cost: the expected cost at the optimum blocking sizes.
    - average expected cost: (for loss networks) the average expected cost.
    - maximum expected cost: (for loss networks) the maximum expected cost.
    """
    rhos = [l / m for l, m in zip(dict_params_environment['job_class_rates'], dict_params_environment['service_rates'])]
    if simul.getEnv().getBufferType() == BufferType.SINGLE:
        print(f"Computing the optimum expected cost and set of optimum capacities (normaly one value) for a single-buffer queue system with "
              f"lambdas = {dict_params_environment.get('job_class_rates')}, mus = {dict_params_environment.get('service_rates')}, rhos = {rhos} and exponential reward function...")
        assert dict_params_environment['reward_func'].__name__ == "rewardOnJobRejection_ExponentialCost", \
            "The reward function is an exponential function of the blocking size: {}".format(dict_params_environment['reward_func'].__name__)

        # IMPORTANT: The following is only valid for the SINGLE-SERVER system.
        # In the single-buffer / single-server queue system we assume that:
        # - the reward function is the exponential function of the buffer size whose reference buffer size parameter is stored
        # in the 'buffer_size_ref' entry of dict_params_environment['reward_func_params'].
        # - the optimum theta parameter for the acceptance policy is close to the reference buffer size parameter of such exponential reward function.
        # Strictly speaking we should minimize the exponential function but it is not so evident how this should be implemented... (i.e. what functions I would use in Python to accomplish that)
        # TODO: (2023/03/01) Find the optimum theta by minimizing the expected cost function based on the exponential cost for blocking. I could probably use an optimization package.
        optimum_theta = dict_params_environment['reward_func_params']['buffer_size_ref']
        optimum_K = get_deterministic_blocking_boundaries(simul.agent, optimum_theta)
        assert is_scalar(optimum_K), "The optimum blocking sizes must be just one value and must be stored as a scalar: {}".format(optimum_K)
        # Convert the optimum blocking sizes to a list because in general there may be more than one optimum
        # (for instance, in a no-buffer/loss-network system, this could be the case)
        # In addition, each element list is in turn a list which contains the blocking sizes for the different job classes,
        optimum_blocking_sizes = [[optimum_K]]
        cost_at_optimum_K = -dict_params_environment['reward_func'](simul.getEnv(), (optimum_K, None), Actions.REJECT, (optimum_K, None))
        # Stationary probability of x=K which is the only blocking state and thus, whne multiplied with the blocking cost at K, gives the expected cost
        pK = compute_blocking_probability_birth_death_process(rhos, optimum_K)
        optimum_expected_cost =  pK * cost_at_optimum_K
        average_expected_cost = np.nan
        maximum_expected_cost = np.Inf
    elif simul.getEnv().getBufferType() == BufferType.NOBUFFER:
        # Loss Network case
        # => In a Loss Network (with limited capacity) it makes sense to talk about expected blocking costs because we have a value for the maximum number of jobs that the system can take
        # In a system with queue this is not the case because its capacity is not fixed and it's what it's optimized by the optimization algorithm, so we don't compute any expected costs.
        blocking_costs = [-r for r in dict_params_environment['reward_func_params']['reward_at_rejection']]
        print(f"Computing the optimum expected cost and set of optimum blocking sizes for a knapsack with capacity = {dict_params_environment['capacity']}, "
              f"blocking costs = {blocking_costs}, lambdas = {dict_params_environment['job_class_rates']}, mus = {dict_params_environment['service_rates']}, rhos = {rhos}...")
        expected_costs = compute_expected_cost_knapsack(blocking_costs, dict_params_environment['capacity'], rhos, dict_params_environment['job_class_rates'])

        # Find the minimum of the expected costs just computed
        expected_values = [v for v in expected_costs.values()]
        optimum_expected_cost = func_optimum(expected_values)
        optimum_blocking_sizes = [list(k) for k, v in expected_costs.items() if v == optimum_expected_cost]
        average_expected_cost = np.mean(expected_values)
        maximum_expected_cost = np.max(expected_values)
        print(f"Minimum expected cost = {optimum_expected_cost} happening at state = {optimum_blocking_sizes}")
        print("Lowest 10 expected cost values:")
        for k in sorted(expected_costs, key=lambda x: expected_costs[x])[:11]:
            print(k, expected_costs[k])
        print("Highest 10 expected cost values:")
        for k in sorted(expected_costs, key=lambda x: expected_costs[x])[-11:]:
            print(k, expected_costs[k])
    else:
        optimum_blocking_sizes = []
        optimum_expected_cost = np.nan
        average_expected_cost = np.nan
        maximum_expected_cost = np.nan

    return optimum_blocking_sizes, optimum_expected_cost, average_expected_cost, maximum_expected_cost
#---------------------------- Auxiliary functions ---------------------------#


#------------------- Functions to parse input arguments ---------------------#
def parse_input_parameters(argv):
    # Written for uugot.it project in Apr-2021
    # Parse input parameters
    # Ref: https://docs.python.org/3.7/library/optparse.html
    # Main steps:
    # 1) The option parser is initialized with optparse.OptionParser(), where we can specify the usage= and version=,
    # as e.g. `optparse.OptionParser(usage="%prog [-v] [-p]", version="%prog 1.0")`
    # 2) New options to parse are added with parser.add_option(), where the metavar= argument (e.g. `metavar="input file"`)
    # is used to indicate that the option expects a value and gives a short description of its content
    # (e.g. `--filename="file.txt"` as opposed to `--verbose`, which expects no value).
    # We can also define:
    #    a) the default value of the option (although this is more clearly done with parser.set_defaults().
    #    b) the action to take with the option value read with the action= argument, e.g. "store_true", "store_false",
    #       which are actually needed for FLAG options that do NOT require any option value (e.g. -v for verbose, etc.),
    #       and ***whose default value (i.e. when the flag is not given) is specified by the default= parameter***.
    #       The default action is "store" which is used for options accepting a value as in `--file="file.txt".
    #       --> NOTE that the action can be "callback" meaning that a callback function with the signature callback(option, opt, value, parser)
    #       is called to parse the argument value.
    #       If the value of the argument needs to be processed by the callback (most likely) we need to:
    #       - specify its type via the `type=` option of the parser.add_option() function. Otherwise, the argument value will be set to None.
    #       If the value of the argument needs to be updated (e.g. a string converted to a list) we need to:
    #       - define the name of the argument to set with the `dest=` option of the parser.add_option() method.
    #       - set the value of the argument in the callback by calling `setattr(parser.values, option.dest, <value>)`.
    #       Ref: https://docs.python.org/3.7/library/optparse.html#option-callbacks
    #    b) the type of the option value expected with the type= argument (e.g. type="int"), which defaults to "string".
    #    c) the store destination with the dest= argument defining the attribute name of the `options` object
    #       created when running parser.parse_args() (see next item) where the option value is stored.
    #       See more details about the default value of dest= below.
    # 3) Options are parsed with parser.parse_args() into a tuple (options, args), where `options` is an object
    # that contains all the name-value pair options and `args` is an object containing the positional parameters
    # that come after all other options have been passed (e.g. `-v --file="file.txt" arg1 arg2`).
    # 4) Every option read is stored as an attribute of the `options` object created by parser.parse_args()
    # whose name is the value specified by the dest= parameter of the parser.add_option() method, or its
    # (intelligent) default if none is specified (e.g. the option '--model_pos' is stored as options.model_pos by default)
    usage = "usage: %prog [options]"
    parser = optparse.OptionParser(usage="%prog "
                                         "[--queue_system] "
                                         "[--method] "
                                         "[--t_learn] "
                                         "[--replications] "
                                         "[--benchmark_filename] "
                                         "[--benchmark_datetime] "
                                         "[--clipping] "
                                         "[--clipping_value] "
                                         "[--theta_ref] "
                                         "[--theta_true] "
                                         "[--theta_start] "
                                         "[--J_factor] "
                                         "[--use_stationary_probability_for_start_states "
                                         "[-N] "
                                         "[-T] "
                                         "[--error_rel_et] "
                                         "[--error_rel_phi] "
                                         "[--seed] "
                                         "[--create_log] "
                                         "[--save_results] "
                                         "[--plot] "
                                         )
    parser.add_option("--queue_system",
                      type="str",
                      metavar="Queue system",
                      help="Queue system defining the environment on which learning takes place [default: %default]")
    parser.add_option("--method",
                      type="str",
                      metavar="Learning method",
                      help="Learning method [default: %default]")
    parser.add_option("--t_learn",
                      type="int",
                      metavar="# Learning steps",
                      help="Number of learning steps [default: %default]")
    parser.add_option("--replications",
                      type="int",
                      metavar="# Replications",
                      help="Number of replications to run [default: %default]")
    parser.add_option("--benchmark_filename",
                      type="str",
                      metavar="Benchmark file",
                      help="File from where the value of execution parameters (e.g. theta_start, T, etc.) that need to be defined in order to perform a fair comparison between the MC and the FV methods [default: %default]")
    parser.add_option("--benchmark_datetime",
                      type="str",
                      metavar="Benchmark datetime",
                      help="Datetime in format <yymmdd>_<hhmmss> that appears in the name of the file to use as benchmark from where the value of execution parameters is read [default: %default]")
    parser.add_option("--clipping",
                      action="store_true",
                      help="Whether to clip the changes of the theta parameter during learning [default: %default]")
    parser.add_option("--clipping_value",
                      type="float",
                      metavar="Clipping value",
                      help="Value to which the changes of the theta parameter during learning are clipped to [default: %default]")
    parser.add_option("--theta_ref",
                      type="float",
                      metavar="Reference theta value",
                      help="Reference theta value used in the blocking cost function of the buffer size in single-buffer systems [default: %default]")
    parser.add_option("--theta_true",
                      action="callback",
                      callback=convert_str_argument_to_list_of_type,
                      metavar="True theta value(s) (only used to define the benchmark file when parameter `benchmark_datetime` is given)",
                      help="True theta values of the problem to optimize that is used to identify the benchmark file where the value of execution parameters is read [default: %default]")
    parser.add_option("--theta_start", dest="theta_start",
                      type="str",
                      action="callback",
                      callback=convert_str_argument_to_list_of_type,
                      metavar="Initial theta",
                      help="Initial theta parameter for the learning process [default: %default]")
    parser.add_option("--J_factor", dest="J_factor",
                      type="str",
                      action="callback",
                      callback=convert_str_argument_to_list_of_type,
                      metavar="J/K factors", default="0.3, 0.3, 0.3",
                      help="Factors that define the absorption set size J [default: %default]")
    parser.add_option("--use_stationary_probability_for_start_states",
                      action="store_true",
                      help="Whether to use the stationary probability distribution for the start states in Fleming-Viot simulation [default: %default]")
    parser.add_option("-N",
                      type="int",
                      metavar="# particles",
                      help="Number of Fleming-Viot particles [default: %default]")
    parser.add_option("-T",
                      type="int",
                      metavar="# arrival events",
                      help="Number of arrival events to observe before ending the simulation [default: %default]")
    parser.add_option("--error_rel_et",
                      type="float",
                      metavar="Expected error for E(T_A)",
                      help="Expected error for the estimation of E(T_A) [default: %default]")
    parser.add_option("--error_rel_phi",
                      type="float",
                      metavar="Expected error for Phi(t)",
                      help="Expected error for the estimation of Phi(t) [default: %default]")
    parser.add_option("--burnin_time_steps",
                      type="int",
                      metavar="Burn-in time steps",
                      help="Number of time steps to leave as burn-in period until the Markov process is assumed to be in stationary regime and proceed to the estimation of stationary measures [default: %default]")
                            ## Note that blocking probability would be OVERESTIMATED when the number of burn-in steps is too small
                            ## because the return time T (in MC) and the re-absorption time T_A (in FV) will be underestimated,
                            ## as the simulated queues start close to the state defining the cycles of interest (i.e. J-1),
                            ## so returning to those states at the beginning of the simulation will take shorter time
                            ## than the respective stationary return times.
    parser.add_option("--min_num_cycles_for_expectations",
                      type="int",
                      metavar="Min. number of cycles for expectation estimations",
                      help="Minimum number of observed cycles needed for what is considered a reliable estimation of expectations (such as the estimation of expected cycle times or the MC-based estimation of the stationary probability) [default: %default]")
    parser.add_option("--seed",
                      type="int",
                      metavar="Seed",
                      help="Seed value to use for the simulations [default: %default]")
    parser.add_option("--create_log",
                      action="store_true",
                      help="Whether to create a log file [default: %default]")
    parser.add_option("--save_results",
                      action="store_true",
                      help="Whether to save the results into a CSV file [default: %default]")
    parser.add_option("--save_with_dt",
                      action="store_true",
                      help="Whether to use the execution datetime as suffix of output file names [default: %default]")
    parser.add_option("--plot",
                      action="store_true",
                      help="Whether to plot the learning process (e.g. theta estimates and objective function) [default: %default]")
    if False:
        parser.add_option("-d", "--debug", dest="debug", default=False,
                          action="store_true",
                          help="debug mode")
        parser.add_option("-v", "--verbose", dest="verbose", default=False,
                          action="store_true",
                          help="verbose: show relevant messages in the log")

    # NOTE: The default values of parameters processed by callbacks should define the value in the type of the `dest` option of the callback!
    default_queue_system = "loss-network" #"single-server" #"loss-network"
    default_create_output_files = True
    parser.set_defaults(queue_system=default_queue_system,
                        method="MC",
                        t_learn=30,
                        replications=20,
                        benchmark_filename="benchmark_fv.csv", # Not used when benchmark_datetime is given (i.e. not empty or None)
                        benchmark_datetime=None, #"20230410_102003", #"20230409_163723",  # Format: "<yymmdd>_<hhmmss>". Use this parameter ONLY when method = "MC" and we want to automatically generate the benchmark filename to read the benchmark data from
                        clipping=False,
                        clipping_value=1.0,
                        theta_ref=18, #COST_EXP_BUFFER_SIZE_REF (this value should coincide with the optimum K, meaning that the optimum theta (theta_true) is theta_ref - 1)
                        theta_true=[3, 5], #17,          # Use this parameter ONLY when method = "MC" and we want to automatically generate the benchmark filename to read the benchmark data from
                        # TODO: (2023/03/19) Make the process work on a SINGLE-CLASS loss network
                        theta_start=[0.1, 0.1], #28.1, #[0.1, 0.1],  # For loss-network: [4.1, 4.1] #[7.1, 7.1] #[3.1, 3.1] #[0.1, 0.1]
                        J_factor=[0.3, 0.5], #0.3, #[0.5, 0.5], #[0.3, 0.5]
                        use_stationary_probability_for_start_states=True,
                        N=np.nan if default_queue_system == "single-server" else 200,   #100 #500
                        T=np.nan if default_queue_system == "single-server" else 400,  #500 #1000
                        error_rel_et=0.5 if default_queue_system == "single-server" else np.nan,
                        error_rel_phi=0.5 if default_queue_system == "single-server" else np.nan,
                        burnin_time_steps=10,   #10 #20
                        min_num_cycles_for_expectations=5,
                        seed=1313,  # 1317 #1717 #1313
                        create_log=default_create_output_files,
                        save_results=default_create_output_files,
                        save_with_dt=True,
                        plot=True)

    (options, args) = parser.parse_args(argv)

    print("Parsed command line options: " + repr(options))

    # options: `Values` object whose parameters are referred with the dot notation (e.g. options.x)
    # args: argument values (which do not require an argument name
    return options, args

def generate_parameter_string_for_filename(queue_system,
                                           capacity,
                                           blocking_costs,
                                           rhos,
                                           theta_ref,
                                           theta_true,
                                           theta_start,
                                           J_factor,
                                           NT_values,
                                           error_rel_phi,
                                           error_rel_et,
                                           use_stationary_probability_for_start_states):
    if queue_system == "loss-network":
        params_str = learning_method.name + \
                     "-K={}-costs={}-rhos={},theta0={}-theta={}-J={}-NT={}-ProbStart={}" \
                         .format(capacity, [int(c) for c in blocking_costs], rhos, theta_true, theta_start, J_factor, NT_values, use_stationary_probability_for_start_states)
    else:
        params_str = learning_method.name + \
                     "-theta_ref={}-theta={}-J={}-E={},{}".format(theta_ref, theta_start, J_factor, error_rel_phi, error_rel_et)

    return params_str

def show_execution_parameters(options):
    print("Execution parameters:")
    print("learning_method={}".format(learning_method.name))
    if learning_method.name == "MC":
        print("Benchmark file: {}".format(benchmark_file))
    print("t_learn={}".format(options.t_learn))
    print("#replications={}".format(options.replications))
    print("clipping={}".format(options.clipping))
    print("clipping_value={}".format(options.clipping_value))
    print("theta_ref={}".format(theta_ref))
    print("theta_start={}".format(options.theta_start))
    print("J_factor={}".format(options.J_factor))
    print("use_stationary_probability_for_start_states={}".format(use_stationary_probability_for_start_states))
    print("#particles(N)= {}".format(N))
    print("#learning_steps(T)={}".format(T))
    print("error_rel_phi={} (only used when N and T are not defined)".format(error_rel_phi))
    print("error_rel_et={} (only used when N and T are not defined)".format(error_rel_et))
    print("burnin_time_steps={}".format(options.burnin_time_steps))
    print("min_num_cycles_for_expectations={}".format(options.min_num_cycles_for_expectations))
    print("create_log={}".format(options.create_log))
    print("save_results={}".format(options.save_results))
    print("seed={}".format(options.seed))

    print("\nSystem's characteristics:")
    print("Queue system: {}".format(options.queue_system))
    print("Capacity: {}".format(capacity))
    print("Blocking costs: {}".format(blocking_costs))
    print("Job arrival rates by class: {}".format(job_class_rates))
    print("Service rates by class: {}".format(service_rates))
    print("Loads by class: {}".format(rhos))
    print("Number of servers: {}".format(nservers))
    print("Policy assignment probabilities (from job class to server): {}".format(policy_assignment_probabilities))
    print("Reward function for blocking: {}".format(reward_func.__name__ if reward_func is not None else None))
    print("Reward function for accepting: {}".format(rewards_accept_by_job_class))
#------------------- Functions to parse input arguments ---------------------#

if __name__ == "__main__":
    #------ Parse input arguments
    options, args = parse_input_parameters(sys.argv[1:])

    print("Parsed user arguments:")
    print(f"Options: {options}")
    print(f"Arguments: {args}")
    print("")

    # System's characteristics
    if options.queue_system not in ["single-server", "loss-network"]:
        raise ValueError(f"Invalid queue system: {options.queue_system}")
    if options.queue_system == "single-server":
        capacity = +np.Inf
        assert np.isinf(capacity), "The system's capacity must be infinite for single-buffer systems"
        job_class_rates = [0.7]  # For multi-server: [0.8, 0.7]
        service_rates = [1.0]
        blocking_costs = "exponential"
    elif options.queue_system == "loss-network":
        capacity = 6 #9 #6 #10
        job_class_rates = [1, 5] #[1, 5, 8] # When lambda = rho: #[0.8, 0.8, 0.8] #[0.1, 0.5, 0.8] #[2, 8, 15] #[20, 80, 150]
        rhos = [0.5, 0.3] #[0.8, 0.6] #[0.5, 0.3] #[0.6, 0.4]#[0.5]*len(job_class_rates) #[0.8]*len(job_class_rates)
        service_rates = [l/r for l, r in zip(job_class_rates, rhos)] #[1.0, 1.0, 1.0]
        #************ BLOCKING COSTS ************
        # We should consider the following two conditions to define the blocking cost values:
        # 1) For a fast-enough learning (i.e. < 30 learning steps) WITHOUT THINKING OF PROBABILITY *ESTIMATION*, i.e. assuming we KNOW the stationary probabilities:
        #   --> Blocking costs should be such that alpha * gradient ~ "reasonable delta(theta) for learning" ~ 1
        #   If we think that a natural choice of alpha is 1, then we have gradient ~ 1.
        #   Since the contribution to the gradient for each job class i (i.e. for each theta dimension i) is of the order of
        #       sum_{blocking states x for class i} p(x) * Qdiff(x)
        #   we can think that:
        #       Qdiff(x) ~ "average cost of blocking ANY of the job classes" = sum_{job class i} c(i) * lambda(i) / Lambda
        #       (note that the average blocking cost depends on both the (stationary) probability of blocking states for each job class AND the job arrival probability, i.e. the arrival job-class rate is ALSO important)
        #   We can design the system so that:
        #       c(i) * lambda(i) / Lambda is about the same for all i.
        #   We can also choose:
        #       all rhos to be the same, making p(x) ~ same order regardless of the blocking class we are considering.
        #   Thus the "sum of the blocking states..." mentioned above would be of the order:
        #       #blocking-states * rho^(K-1) * c * lambda/Lambda
        #   which should be ~ 1.
        #   If we suppose that #blocking-states ~ 10, we can choose c(i) * lambda(i)
        #
        # 2) For a realistic problem scenario
        #   --> The cost should be large enough so as to make the revenue of operating the system negative when blocking occurs.
        #   In fact, in the problem we want to solve we state that we want to discover rare events (prob < 1E-5) which,
        #   although rare, if they occur they are "very costly".
        #   This "very costly" condition should be defined on the basis of the revenue (before costs deduction) under normal operation of the system (i.e. without blocking)
        #   So, if we say that the expected revenue under normal conditions is 1 (or 1000 dollars, whatever) and we want to operate the system
        #   in a realm where the expected cost < 50% of those expected revenues (revenues before costs deduction).
        adjustment_constant_for_large_enough_cost = 20 # This constant is normally > 1 and is adjusted by eye-ball as a function of rhos (e.g. 20 for rhos = [0.5, 0.3], 2 for rhos = [0.2, 0.1]), based on NOT obtaining costs that are too large (but the final decision comes from observing the magnitude of the obtained gradient, which should not be too large nor too small)
        blocking_costs = [adjustment_constant_for_large_enough_cost * sum(job_class_rates) * 1/r**(capacity - 1) / l for l, r in zip(job_class_rates, rhos)] #[5000, 1000, 100] (for rho_average = 0.8) #[2E5, 1.5E5, 1E5] #[2E5, 1E2, 1E0] #[1E1, 1E2, 1E4] #[20.0, 10.0, 5.0]
        # Perturb the costs so that we don't get the trivial optimum at e.g. [6, 6, 6]
        np.random.seed(1)
        multiplicative_noise_values = [0.5, 2]
        blocking_costs = [int(round(c*n)) if c*n > 1 else c*n for c, n in zip(blocking_costs, multiplicative_noise_values)]
        blocking_costs = [2000, 20000] #[2000, 20000] ([0.5, 0.3]) #[180, 600] ([0.8. 0.6]) # Costs computed in run_FVRL.py from p(x), lambdas and rhos
        #************ BLOCKING COSTS ************
    rhos = [l / m for l, m in zip(job_class_rates, service_rates)]
    nservers = len(service_rates)

    learning_method = LearningMethod.FV if options.method == "FV" else LearningMethod.MC
    # NOTE: In unidimensional theta parameters and in blocking by buffer size systems, we the value of theta_ref from the input arguments of the script.
    # In those cases the theta_ref value defines the reference value of the reward function that is an exponential function of the buffer size.
    # In multidimensional theta parameters with blocking by job class (as opposed to blocking by buffer size)
    # the theta_ref has no meaning, therefore we set it to a list of NaN, where the list has length equal to the dimension of the theta parameter to estimate.
    if options.queue_system == "single-server":
        theta_ref = options.theta_ref
        # The values of N (#particles) and T (#arrival events) are computed from the expected relative errors wished for the estimation of E(T_A) and Phi(t)
        N = np.nan; T = np.nan
        error_rel_phi = options.error_rel_phi
        error_rel_et = options.error_rel_et
        use_stationary_probability_for_start_states = None
    elif options.queue_system == "loss-network":
        theta_ref = [np.nan]*len(job_class_rates)
        # Values of N (#particles) and T (#arrival events) to use for the simulation used to estimate stationary probabilities
        if learning_method.name == "MC":
            # This is ONLY used when there is no benchmark file or if it does not exist
            # (in order to run an ad-hoc MC learning that is not linked to a previously run FV learning)
            N = 1; T = options.T #200 * 1000 #50 * 100 #30 * 50 #500 * 100 #1000 * 100
            use_stationary_probability_for_start_states = None
        else:
            N = options.N; T = options.T #N = 100; T = 100, N = 500; T = 100
            use_stationary_probability_for_start_states = options.use_stationary_probability_for_start_states
        error_rel_phi = np.nan
        error_rel_et = np.nan

    # Reward functions
    rewards_accept_by_job_class = [0.0] * len(job_class_rates)
    if options.queue_system == "single-server":
        reward_func = rewardOnJobRejection_ExponentialCost
        reward_func_params = dict({'buffer_size_ref': np.nan})   # The value of this parameter will be set at each simulation iteration run in the below LOOP
        policy_assignment_probabilities = [[1.0]] # For multi-server: [[0.5, 0.5, 0.0], [0.0, 0.5, 0.5]] )
    elif options.queue_system == "loss-network":
        reward_func = rewardOnJobRejection_ByClass
        reward_func_params = dict({'reward_at_rejection': [-c for c in blocking_costs]})   # None
        policy_assignment_probabilities = None
    #------ Parse input arguments

    # Look for memory leaks
    # Ref: https://pythonspeed.com/fil/docs/fil/other-tools.html (from the Fil profiler which also seems interesting)
    # Doc: https://docs.python.org/3/library/tracemalloc.html
    # tracemalloc.start()

    start_time_all = timer()

    # ---------------------- OUTPUT FILES --------------------#
    # create_log = False;    # In case we need to override the parameter received as argument to the script
    logsdir = "../../RL-002-QueueBlocking/logs/RL/" + options.queue_system
    # save_results = True;   # In case we need to override the parameter received as argument to the script
    resultsdir = "../../RL-002-QueueBlocking/results/RL/" + options.queue_system
    # ---------------------- OUTPUT FILES --------------------#

    # -- Parameters defining the environment, policies, learners and agent
    # Learning parameters for the value function V
    gamma = 1.0

    # Learning parameters for the policy P
    if learning_method == LearningMethod.FV:
        learnerV = LeaFV
        benchmark_file = None
        plot_trajectories = False
        symbol = 'g-'
    else:
        # Monte-Carlo learner is the default
        learnerV = LeaMC
        if options.benchmark_datetime is not None and options.benchmark_datetime != "" or options.benchmark_filename == "":
            # Build the benchmark filename from the datetime given and the input parameters
            # Notes:
            # - In the call below, we enclose some parameters in brackets because when the benchmark file is saved
            # those parameters are stored as a LIST of different values tried in the filename
            # (see below when calling this function using e.g. theta_ref_values, etc.).
            # - NOT all parameters are used to generate the string of parameters. The parameters that are used depend
            # on the queue system (e.g. "single-server" or "loss-network").
            params_str = generate_parameter_string_for_filename(# System parameters
                                                                options.queue_system, capacity, blocking_costs, rhos,
                                                                # Policy parameters
                                                                [theta_ref], options.theta_true, [options.theta_start],
                                                                # Learning parameters
                                                                [options.J_factor], [[options.N, options.T]], error_rel_phi, error_rel_et, options.use_stationary_probability_for_start_states)
            if options.benchmark_datetime != "":
                sep = "_" + options.benchmark_datetime + "_"
            else:
                sep = "_"
            benchmark_file = os.path.join(resultsdir, "SimulatorQueue" + sep + params_str.replace("MC", "FV") + ".csv")
        elif options.benchmark_filename is not None:
            benchmark_file = os.path.join(os.path.abspath(resultsdir), options.benchmark_filename) # "SimulatorQueue_20221018_161552-K0=24-K=35-J=0.3-E1.0,0.2-AlphaConst(B=5)_seed1313.csv") #"benchmark_fv.csv")
        if options.benchmark_filename is not None and not os.path.exists(benchmark_file):
            warnings.warn("Benchmark file '{}' does not exist. The Monte-Carlo simulation will run without any reference to a previously run Fleming-Viot simulation".format(benchmark_file))
            benchmark_file = None
        plot_trajectories = False
        symbol = 'r-'
    fixed_window = False
    alpha_start = 1.0  # / t_sim  # Use `/ t_sim` when using update of theta at each simulation step (i.e. LeaPolicyGradient.learn_update_theta_at_each_time_step() is called instead of LeaPolicyGradient.learn_update_theta_at_end_of_episode())
    adjust_alpha = True  # True
    func_adjust_alpha = np.float # np.sqrt
    min_time_to_update_alpha = 0  # int(t_learn / 3)
    alpha_min = 0.01  # 0.1

    show_execution_parameters(options)
    #raise KeyboardInterrupt

    # Create the environment and learning agent
    dict_params = dict({'environment': {'queue_system': options.queue_system,
                                        'capacity': capacity,
                                        'nservers': nservers,
                                        'job_class_rates': job_class_rates,
                                        'service_rates': service_rates,
                                        'policy_assignment_probabilities': policy_assignment_probabilities,
                                        'reward_func': reward_func,
                                        'reward_func_params': reward_func_params,
                                        'rewards_accept_by_job_class': rewards_accept_by_job_class,
                                        },
                        'policy': {'parameterized_policy': PolQueueTwoActionsLinearStep if is_scalar(options.theta_start) else [PolQueueTwoActionsLinearStep for _ in options.theta_start],
                                   'theta': 1.0 if is_scalar(options.theta_start) else [1.0 for _ in options.theta_start]  # This value is dummy in the sense that it will be updated below
                                   },
                        'learners': {'V': {'learner': learnerV,
                                           'params': {'gamma': 1}
                                           },
                                     'Q': {'learner': None,
                                           'params': {}},
                                     'P': {'learner': LeaPolicyGradient,
                                           'params': {'alpha_start': alpha_start,
                                                      'adjust_alpha': adjust_alpha,
                                                      'func_adjust_alpha': func_adjust_alpha,
                                                      'min_time_to_update_alpha': min_time_to_update_alpha,
                                                      'alpha_min': alpha_min,
                                                      'fixed_window': fixed_window,
                                                      'clipping': options.clipping,
                                                      'clipping_value': options.clipping_value,
                                                      }
                                           }
                                     },
                        'agent': {'agent': AgeQueue}
                        })
    env_queue, rhos, agent = define_queue_environment_and_agent(dict_params)

    # -- Simulation parameters that are common for ALL parameter settings
    # t_learn is now defined as input parameter passed to the script
    # 2022/01/14: t_learn = 10 times the optimum true theta so that we are supposed to reach that optimum under the REINFORCE_TRUE learning mode with decreasing alpha
    # t_learn = 800 #100 #800 #100 #198 - 91 #198 #250 #50
    verbose = False
    dict_params_learning = dict({'mode': LearningMode.REINFORCE_TRUE, 't_learn': options.t_learn})
    dict_params_info = dict({'plot': False, 'log': False})

    # Simulator object
    simul = SimulatorQueue(env_queue, agent, dict_params_learning,
                           log=options.create_log, save=options.save_results, logsdir=logsdir, resultsdir=resultsdir, debug=False)

    if options.save_results:
        # Initialize the file to store the results (e.g. add the column names of the file)
        # NOTE: We initialize the results file at the very beginning and close it at the very end of the simulation run
        # so that all cases and replications run will be output to the same file! This is convenient as then we will have
        # all results for all cases and replications gathered together.
        simul.initialize_results_file()

    #-------- Define all the values on which simulations will be run
    # The simulations is run, either from parameters defined by a benchmark file or from parameters defined below.
    # Define the parameters on which the non-benchmark simulation will be run
    # These are defined here (even if we are running the simulation based on a benchmark file)
    # because these parameter values are used when naming the results file for both situations.

    # In the non-benchmark case (i.e. FVRL) we run the learning method on each set of parameters defined here
    # for as many replications defined as input argument.
    # When defining the theta values we specify the blocking size K and we subtract 1. Recall that K = ceil(theta+1)
    # So, if we want K = 35, we can set theta somewhere between 33+ and 34, so we define e.g. theta = 34.9 - 1
    # Note that the list of theta values can contain more than one value, in which case a simulation will be run for each of them
    theta_ref_values = [theta_ref]  # [24.0 - 1] #[20.0 - 1]  # [32.0-1, 34.0-1, 36.0-1] #[10.0-1, 15.0-1, 20.0-1, 25.0-1, 30.0-1]  # 39.0
    theta_start_values = [options.theta_start]  # [34.9 - 1] #[30.0 - 1] #[20.0 - 1, 25.0 - 1]
    # theta_ref_values = np.linspace(start=1.0, stop=20.0, num=20)
    J_factor_values = [options.J_factor]  # [0.2, 0.3, 0.5]  # [0.2, 0.3, 0.5, 0.7]
    NT_exponents = [0]  # [-2, -1, 0, 1]  # Exponents to consider for different N and T values as in exp(exponent)*N0, where N0 is the reference value to achieve a pre-specified relative error
    NT_values = [[N, T]] if not np.isnan(N) and not np.isnan(T) else None
    # Accepted relative errors for the estimation of Phi and of E(T_A)
    # They define respectively the number of particles N and the number of arrival events T to observe in each learning step.
    error_rel_phi = [error_rel_phi]  # [1.0] #0.5
    error_rel_et = [error_rel_et]  # [1.0] #0.5
    #-------- Define all the values on which simulations will be run

    # Run the simulations
    if benchmark_file is None:
        # Output variables of the simulation
        case = 0
        ncases = len(theta_ref_values) * len(theta_start_values) * len(J_factor_values) * len(NT_exponents)
        theta_opt_values = [[np.nan] * options.replications] * ncases   # List of optimum theta values achieved by the learning algorithm for each replication in each parameter setting
        K_opt_values = [[np.nan] * options.replications] * ncases       # List of optimum K values where deterministic blocking occurs
        cost_opt_values = [[np.nan] * options.replications] * ncases    # List of optimum costs found by the learning algorithm
        for i, theta_ref in enumerate(theta_ref_values):
            # For single-buffer systems, store the value of theta_ref as parameter of the reward function because,
            # in those systems, we use it in the next line to compute the TRUE optimum blocking sizes and the optimum expected cost.
            # In no-buffer systems (e.g. loss networks), the TRUE optimum blocking sizes and optimum expected cost are computed by brute force,
            # i.e. by trying all possible blocking sizes that can be tried on the system.
            if simul.getEnv().getBufferType() == BufferType.SINGLE:
                dict_params['environment']['reward_func_params']['buffer_size_ref'] = theta_ref

            # Compute the optimum blocking sizes and its respective expected cost, i.e. the one this optimization process optimizes
            K_true, cost_true, cost_mean, cost_max = compute_optimum_blocking_sizes_and_expected_cost(simul, dict_params['environment'])
            # Update the theta_true value based on the K_true just computed.
            # We pick ONE of the K-values as in general, although NOT commonly, the optimum K-values (i.e. the array of optimum K)
            # may be more than one array, which is the case when the optimum expected cost is achieved at several different values.
            # NOTE: This calculation of theta_true ASSUMES that the parameterized policy is a linear step function of the buffer size or job class occupancy
            theta_true = [[k-1 for k in K] for K in K_true][0]   # e.g. K_true = [[0, 2, 6]] => theta_true = [-1, 1, 5]
                ## NOTE THAT A THETA VALUE CAN BE NEGATIVE UP TO -1 (so that we can have a deterministic blocking size equal to K = 0)!
                ## HOWEVER: what happens in FVRL, when we have to choose J < K... and K = 0...?
                ## TODO: (2023/03/01) Check if theta can be 0 when using FVRL to learn theta (because of what I just wrote in the line above)

            print("\nSimulating with {} learning on a queue environment with reference theta {} and optimum theta (one less the deterministic blocking size) = {}..." \
                .format(learning_method.name, theta_ref, theta_true))

            # Set the number of learning steps to double the true theta value
            # Use this ONLY when looking at the MC method and running the learning process on several true theta values
            # to see when the MC method breaks... i.e. when it can no longer learn the optimum theta.
            # The logic behind this choice is that we start at theta = 1.0 and we expect to have a +1 change in
            # theta at every learning step, so we would expect to reach the optimum value after about a number of
            # learning steps equal to the true theta value... so in the end, to give some margin, we allow for as
            # many learning steps as twice the value of true theta parameter.
            #simul.dict_params_learning['t_learn'] = int(theta_true*2)

            # Iterate on the different start theta parameters tried
            for theta_start in theta_start_values:
                # Initial deterministic blocking sizes
                K = get_deterministic_blocking_boundaries(simul.agent, theta_start)
                for j, J_factor in enumerate(J_factor_values):
                    if NT_values is None:
                        # We compute the N, T values from the expected relative errors for the estimation of Phi and E(T_A) given as parameters
                        N_min = 50; N_max = 500
                        T_min = 100; T_max = 5000
                        NT_values = [compute_nparticles_and_narrivals_for_fv_process(rhos, K, J_factor, error_rel_phi=err1, error_rel_et=err2)
                                     for err1, err2 in zip(error_rel_phi, error_rel_et)]
                        if learning_method.name == "MC":
                            # If we are running Monte-Carlo with no benchmark file, set the number of particles to 1 and the # arrival events T to N*T
                            # where N and T come from the FV-equivalent simulation.
                            NT_values = [[1, N*T] for N, T in NT_values]
                            N_min = 1
                    else:
                        N_min = 1; N_max = np.Inf
                        T_min = 1; T_max = np.Inf
                    for idx_case, (exponent, (N, T)) in enumerate(zip(NT_exponents, NT_values)):
                        print("NT values obtained for the requested expected relative errors (BEFORE BOUNDING):")
                        print("err(phi) = {:.3f}% => N = {}".format(error_rel_phi[idx_case] * 100, NT_values[idx_case][0]))
                        print("err(E(T)) = {:.3f}% => T = {}".format(error_rel_et[idx_case] * 100, NT_values[idx_case][1]))
                        # Lower bound for N and T so that we don't have too little particles!
                        N = min( max(N_min, N), N_max )
                        T = min( max(T_min, T), T_max )
                        # Set the parameters for this run
                        case += 1
                        t_sim = T  # This is used just for the title of plots done below (after the loop)
                        dict_params_simul = {
                            'theta_true': theta_true,
                            'theta_start': theta_start,
                            'buffer_size_activation_factor': J_factor,
                            'nparticles': N,
                            't_sim': T,     # This is the number of arrival events, NOT the number of time steps to use in the estimation of E(T_A) in FV
                                            # In fact, the calculation of T on the basis of the expected relative error in the estimation of E(T_A) gives
                                            # us the number of arrival events (see details in my small dark green notebook in entry dated 06-Nov-2022).
                            'use_stationary_probability_for_start_states': use_stationary_probability_for_start_states,
                            'burnin_time_steps': options.burnin_time_steps,
                            'min_num_cycles_for_expectations': options.min_num_cycles_for_expectations,
                        }
                        dict_info = {'case': case,
                                     'ncases': ncases,
                                     'learning_method': learning_method.name,
                                     'exponent': exponent,
                                     'rhos': rhos,
                                     'K_true': K_true,
                                     'cost_true': cost_true,
                                     'K': K,
                                     'error_rel_phi': error_rel_phi[idx_case],
                                     'error_rel_et': error_rel_et[idx_case],
                                     'N_min': N_min,
                                     'N_max': N_max,
                                     'T_min': T_min,
                                     'T_max': T_max,
                                     'alpha_start': alpha_start,
                                     'adjust_alpha': adjust_alpha,
                                     'min_time_to_update_alpha': min_time_to_update_alpha,
                                     'alpha_min': alpha_min
                                     }

                        # Run the simulation process
                        theta_opt_values[case-1], K_opt_values[case-1], cost_opt_values[case-1], \
                            df_learning = run_simulation_policy_learning(simul,
                                                                         options.replications,
                                                                         dict_params_simul,
                                                                         dict_info,
                                                                         dict_params_info=dict_params_info,
                                                                         seed=options.seed,
                                                                         verbose=verbose)
    else:
        # Read the execution parameters from the benchmark file
        print("Reading benchmark data containing the parameter settings from file\n{}".format(benchmark_file))
        benchmark = pd.read_csv(benchmark_file, sep="|")
        # Filter the benchmark data so that we end up with one record per analyzed group
        # (so that we can extract the parameter settings for each analyzed group -e.g. N, T, J values, etc.)
        benchmark_groups = benchmark[(benchmark['t_learn'] == 1) & (benchmark['replication'] == 1)]
        ncases = benchmark_groups.shape[0]
        theta_true_values = array_of_objects(ncases, value=np.nan) #np.nan * np.ones(ncases) ((2023/03/12) use np.ones() if the array_of_objects() doesn't work when dealing with scalar values of theta_true (e.g. single-server case)
        theta_opt_values = [[np.nan] * options.replications] * ncases   # List of optimum theta values achieved by the learning algorithm for each replication in each parameter setting
        K_opt_values = [[np.nan] * options.replications] * ncases       # List of optimum K values where deterministic blocking occurs
        cost_opt_values = [[np.nan] * options.replications] * ncases    # List of optimum costs found by the learning algorithm
        idx_case = -1
        NT_values = []
        for i in range(ncases):
            idx_case += 1
            case = benchmark_groups['case'].iloc[i]
            replications = len(np.unique(benchmark[benchmark['case'] == case]['replication']))

            # Simulation and estimation parameters that are common for all replications of the current case (group) analyzed
            theta_true = benchmark_groups['theta_true'].iloc[i]
            theta_true = convert_str_to_list_of_type(theta_true, type=int)  # Recall that the true theta values are always integer
            if simul.getEnv().getBufferType() == BufferType.SINGLE:
                # Store the value of theta_true as parameter of the reward function because we use it below to compute the optimum blocking sizes and optimum expected cost
                dict_params['environment']['reward_func_params']['buffer_size_ref'] = theta_true
            theta_true_values[idx_case] = theta_true
            theta_start = benchmark_groups['theta'].iloc[i]
            theta_start = convert_str_to_list_of_type(theta_start)
            J_factor = benchmark_groups['J/K'].iloc[i]
            J_factor = convert_str_to_list_of_type(J_factor)
            exponent = benchmark_groups['exponent'].iloc[i]
            N = benchmark_groups['N'].iloc[i] if learning_method.name == "FV" else 1
            T = benchmark_groups['T'].iloc[i]   # See NOTE below for entry 't_sim' of dict_params_simul dictionary about the meaning of T which is NOT the simulation time that will be used for the MC learning when a benchmark is available!
            NT_values += [[N, T]]
            burnin_time_steps = benchmark_groups['burnin_time_steps'].iloc[i]
            min_num_cycles_for_expectations = benchmark_groups['min_n_cycles'].iloc[i]

            K_true, cost_true, cost_mean, cost_max = compute_optimum_blocking_sizes_and_expected_cost(simul, dict_params['environment'])
            K = get_deterministic_blocking_boundaries(simul.agent, theta_start)

            dict_params_simul = {
                'theta_true': theta_true,
                'theta_start': theta_start,
                'buffer_size_activation_factor': J_factor,
                'nparticles': 1,
                't_sim': T,         # This is the 'T' parameter used at the first learning step when running FVRL, and is used only as informational purposes inside
                                    # run_simulation_policy_learning() and in the output filename in order for the user to know the characteristics of the case we are currently comparing with MC learning.
                                    # It is NOT the simulation time that will be used for MC learning, as this is defined by the actual #events observed during FVRL.
                'burnin_time_steps': burnin_time_steps,
                'min_num_cycles_for_expectations': min_num_cycles_for_expectations,
            }
            dict_info = {'case': case,
                         'ncases': ncases,
                         'learning_method': learning_method.name,
                         'exponent': exponent,
                         'rhos': rhos,
                         'K_true': K_true,
                         'cost_true': cost_true,
                         'K': K,
                         'error_rel_phi': 0.0,
                         'error_rel_et': 0.0,
                         'alpha_start': alpha_start,
                         'adjust_alpha': adjust_alpha,
                         'min_time_to_update_alpha': min_time_to_update_alpha,
                         'alpha_min': alpha_min
                         }

            # Run the simulation process
            theta_opt_values[idx_case], K_opt_values[idx_case], cost_opt_values[idx_case], \
                df_learning = run_simulation_policy_learning(simul,
                                                             replications,
                                                             dict_params_simul,
                                                             dict_info,
                                                             dict_params_info=dict_params_info,
                                                             params_read_from_benchmark_file=True,
                                                             benchmark=benchmark,
                                                             seed=None,
                                                             verbose=verbose)

        # Update the value of t_sim so that it stores the number of events observed at each learning step
        # in the *last* MC learning run (i.e. the last replication of the last analyzed case)
        # which is shown in the plots below that e.g. show the evolution of the theta parameter as is being learned.
        t_sim = df_learning['n_events_mc'].iloc[-1] + df_learning['n_events_fv'].iloc[-1]

    print("Optimum theta, K's and expected costs found by the learning algorithm for each replication and each parameter setting:\n{}" \
          .format(pd.DataFrame({'theta_opt': theta_opt_values, 'K_opt': K_opt_values, 'expected cost': cost_opt_values}, index=range(1, ncases+1))))
    print(f"True optima:\nK_opt = {K_true[0]}, expected cost = {cost_true}")

    # Closes the object (e.g. any log and result files are closed)
    simul.close()

    if options.save_results:
        if learning_method == LearningMethod.FV:
            # Make a copy of the results benchmark file to be used for a Monte-Carlo learner
            # so that we can easily refer to it in case we run the MC learning right after the FVRL learner
            # (to this end, the name used for the filename here should be the same as the default filename defined
            # as input argument for the variable benchmark_filename).
            shutil.copyfile(simul.results_file, os.path.join(os.path.dirname(simul.results_file), "benchmark_fv.csv"))
        # Build the string containing the parameter values that should be included in the output filename
        params_str = generate_parameter_string_for_filename(options.queue_system,
                                                            capacity,
                                                            blocking_costs,
                                                            rhos,
                                                            theta_ref_values,
                                                            theta_true,
                                                            theta_start_values,
                                                            J_factor_values,
                                                            NT_values,
                                                            error_rel_phi,
                                                            error_rel_et,
                                                            use_stationary_probability_for_start_states)
        # Now rename the results file to include the parameter settings so that it's easier to identify when analyzing results.
        results_dir = os.path.dirname(simul.results_file)
        results_filename = os.path.basename(simul.results_file)
        if options.save_with_dt:
            # This allows keeping the datetime string in the filename
            lookfor = ".csv"
        else:
            # This allows removing the datetime string from the filename
            lookfor = "_"
        # Add the parameter settings to the filename, either keeping or removing the execution datetime string
        results_filename_with_parameters = results_filename[:str.index(results_filename, lookfor)] + "_" + params_str + ".csv"
        # Rename and if the file with the new file already exists, delete it first
        results_file_with_parameters = os.path.join(results_dir, results_filename_with_parameters)
        if os.path.exists(results_file_with_parameters):
           os.remove(results_file_with_parameters)
        os.rename(simul.results_file, results_file_with_parameters)

        # Add the parameter settings to the log file as well
        log_dir = os.path.dirname(simul.logfile)
        log_filename = os.path.basename(simul.logfile)
        log_filename_with_parameters = results_filename[:str.index(log_filename, ".log")] + "_" + params_str + ".log"
        log_file_with_parameters = os.path.join(log_dir, log_filename_with_parameters)
        os.rename(simul.logfile, log_file_with_parameters)
    else:
        params_str = "-- No output file --"

    if len(theta_ref_values) == 1:
        if PLOT_GRADIENT and env_queue.getBufferType == BufferType.SINGLE:
            # Save the estimation of G(t) for the last learning step to a file
            # file_results_G = "G.csv"
            # pd.DataFrame({'G': simul.G}).to_csv(file_results_G)

            # -- Plot theta and the gradient of the value function
            SET_YLIM = False

            # Estimated value function
            ax, line_est = plotting.plot_colormap(df_learning['theta'], -df_learning['V'], cmap_name="Blues")

            # True value function
            # Block size for each theta, defined by the fact that K-1 is between theta and theta+1 => K = ceil(theta+1)
            Ks = [np.int(np.ceil(np.squeeze(t) + 1)) for t in df_learning['theta']]
            # Blocking probability = Pr(K)
            p_stationary = [compute_blocking_probability_birth_death_process(rhos, K) for K in Ks]
            pblock_K = np.array([p[-1] for p in p_stationary])
            pblock_Km1 = np.array([p[-2] for p in p_stationary])
            # Blocking probability adjusted for different jump rates between K-1 and K (affected by the non-deterministic probability of blocking at K-1)
            pblock_K_adj = np.squeeze([pK * (1 - (K-1-theta)) for K, theta, pK in zip(Ks, df_learning['theta'], pblock_K)])
            pblock_Km1_adj = pblock_Km1  # np.squeeze([pKm1 + pK - pK_adj for pKm1, pK, pK_adj in zip(pblock_Km1, pblock_K, pblock_K_adj)])
            # assert np.allclose(pblock_K + pblock_Km1, pblock_K_adj + pblock_Km1_adj)
            # True value function: expected cost at K which is the buffer size where blocking most likely occurs...
            # (in fact, if theta is say 3.1, the probability of blocking at 4 (= K-1) is small and most blocking
            # will occur at K; if theta is 3.9, the probability of blocking at 4 (= K-1)
            # i.e. we compute at K-1 and NOT at K because we want to compare the true value function
            # with the *estimated* value function when the policy starts blocking at buffer size = theta
            # Vtrue = np.array([rewardOnJobRejection_ExponentialCost(env_queue, (K, None), Actions.REJECT, (K, None)) * pK for K, pK in zip(Ks, pblock_K)])

            # ACTUAL true value function, which takes into account the probability of blocking at K-1 as well, where the policy is non-deterministic (for non-integer theta)
            # The problem with this approach is that the stationary distribution of the chain is NOT the same as with chain
            # where rejection ONLY occurs at s=K... in fact, the transition probabilities to s=K and to s=K-1 when the
            # initial state is s=K-1 are affected by the non-deterministic probability of blocking when s=K-1...
            # Qualitatively, the stationary probability of K would be reduced and the stationary probability of K-1 would be
            # increased by the same amount.
            Vtrue = np.array(
                [rewardOnJobRejection_ExponentialCost(env_queue, (K, None), Actions.REJECT, (K, None)) * pK +
                 rewardOnJobRejection_ExponentialCost(env_queue, (K-1, None), Actions.REJECT, (K-1, None)) * (K-1-theta) * pKm1
                 for K, theta, pK, pKm1 in zip(Ks, df_learning['theta'], pblock_K_adj, pblock_Km1_adj)])

            # True grad(V)
            # Ref: my hand-written notes in Letter-size block of paper with my notes on the general environment - agent setup
            gradVtrue = [-rewardOnJobRejection_ExponentialCost(env_queue, (K-1, None), Actions.REJECT, (K-1, None)) * pKm1
                            for K, pKm1 in zip(Ks, pblock_Km1)]

            ord = np.argsort(Ks)
            # NOTE that we plot the true value function at K-1 (not at K) because K-1 is the value that is closest to theta
            # and we are plotting the *estimated* value function vs. theta (NOT vs. K).
            # line_true, = ax.plot([Ks[o]-1 for o in ord], [-Vtrue[o] for o in ord], 'g.-', linewidth=5, markersize=20)
            line_true, = ax.plot(df_learning['theta'], -Vtrue, 'gx-')  # Use when computing the ACTUAL true Value function V, which also depends on theta!
            ax.set_xlim((0, ax.get_xlim()[1]))
            ax.set_yscale('log')
            # ax.set_ylim((0, 10))
            ax.set_xlabel('theta (for estimated functions) / K-1 for true value function')
            ax.set_ylabel('Value function V (cost)')
            ax.legend([line_est, line_true], ['Estimated V', 'True V'], loc='upper left')
            ax2 = ax.twinx()
            ax2, line_grad = plotting.plot_colormap(df_learning['theta'], -df_learning['gradV'], cmap_name="Reds", ax=ax2)
            line_gradtrue, = ax2.plot([Ks[o] - 1 for o in ord], [-gradVtrue[o] for o in ord], 'k.-', linewidth=3, markersize=12)
            ax2.axhline(0, color="lightgray")
            ax2.set_ylabel('grad(V)')
            if SET_YLIM:
                ax2.set_ylim((-5, 5))  # Note: grad(V) is expected to be -1 or +1...
            ax2.legend([line_grad, line_gradtrue], ['grad(V)', 'True grad(V)'], loc='upper right')
            # Note that the optimum true K is computed as round(theta+1) and NOT as ceil(theta+1) because we want that e.g. K = 4 when theta = 3.1
            # (instead of having K = 5, which is too large, considering the blocking probability is *almost* 1 at 4).
            if is_scalar(t_sim):
                title = "Value function and its gradient as a function of theta and K. " + \
                        "Optimum K = {}, Theta start = {}, t_sim = {:.0f}".format(np.round(theta_true+1), theta_start, t_sim)
            else:
                title = "Value function and its gradient as a function of theta and K. " + \
                        "Optimum K = {}, Theta start = {}".format(np.round(theta_true+1), theta_start)
            plt.title(title)

            # grad(V) vs. V
            plt.figure()
            plt.plot(-df_learning['V'], -df_learning['gradV'], 'k.')
            ax = plt.gca()
            ax.axhline(0, color="lightgray")
            ax.axvline(0, color="lightgray")
            ax.set_xscale('log')
            # ax.set_xlim((-1, 1))
            if SET_YLIM:
                ax.set_ylim((-1, 1))
            ax.set_xlabel('Value function V (cost)')
            ax.set_ylabel('grad(V)')

        # Plot evolution of theta (for the last analyzed case)
        title = "{}".format(params_str) + "\nMethod: {}, Optimum Theta = {}, Theta start = {}, Theta final = {}, K_final = {}, Expected cost = {}" \
                                          "\nN = {}, T = {:.0f} J = {} Stat.Prob.StartStates = {}" \
                                          "\nK = {}, Blocking Costs = {}, lambdas = {}, rhos = {}" \
                    .format(learning_method.name, theta_true, theta_start, np.mean(theta_opt_values[-1]), np.mean(K_opt_values[-1]), np.mean(cost_opt_values[-1]),
                            N, t_sim, J_factor_values, use_stationary_probability_for_start_states,
                            capacity, [int(c) for c in blocking_costs], job_class_rates, rhos)
        if options.plot and plot_trajectories:
            "In the case of the MC learner, plot the theta-learning trajectory as well as rewards received during learning"
            assert N == 1, "The simulated system has only one particle (N={})".format(N)
            # NOTE: (2021/11/27) I verified that the values of learnerP.getRewards() for the last learning step
            # are the same to those for learnerV.getRewards() (which only stores the values for the LAST learning step)
            plt.figure()
            # Times at which the trajectory of states is recorded
            times = simul.getLearnerP().getTimes()
            times_unique = np.unique(times)
            assert len(times_unique) == len(simul.getLearnerP().getThetas()) - 1, \
                    "The number of unique learning times ({}) is equal to the number of theta updates ({})".format(
                    len(times_unique), len(simul.getLearnerP().getThetas()) - 1)
            ## Note that we subtract 1 to the number of learning thetas because the first theta stored in the policy object is the initial theta before any update
            plt.plot(np.r_[0.0, times_unique], simul.getLearnerP().getThetas(), 'b.-')
            ## We add 0.0 as the first time to plot because the first theta value stored in the policy is the initial theta with which the simulation started
            ax = plt.gca()
            # Add vertical lines signalling the BEGINNING of each queue simulation
            times_sim_starts = range(0, options.t_learn * (t_sim + 1), t_sim + 1)
            for t in times_sim_starts:
                ax.axvline(t, color='lightgray', linestyle='dashed')

            # Buffer sizes
            buffer_sizes = [env_queue.getBufferSizeFromState(s) for s in simul.getLearnerP().getSetBoundaries()]
            ax.plot(times, buffer_sizes, 'g.', markersize=3)
            # Mark the start of each queue simulation
            # DM-2021/11/28: No longer feasible (or easy to do) because the states are recorded twice for the same time step (namely at the first DEATH after a BIRTH event)
            ax.plot(times_sim_starts, [buffer_sizes[t] for t in times_sim_starts], 'gx')
            ax.set_xlabel("time step")
            ax.set_ylabel("theta")
            ax.yaxis.set_major_locator(MaxNLocator(integer=True))

            # Secondary plot showing the rewards received
            ax2 = ax.twinx()
            ax2.plot(times, -np.array(simul.getLearnerP().getRewards()), 'r-', alpha=0.3)
            # Highlight with points the non-zero rewards
            ax2.plot(times, [-r if r != 0.0 else None for r in simul.getLearnerP().getRewards()], 'r.')
            ax2.set_ylabel("Reward")
            ax2.set_yscale('log')
            plt.title(title)
        elif options.plot:
            # Plot the evolutoin of theta and of the expected cost
            axes = plt.figure().subplots(1, 2)
            ax_params, ax_objective = axes

            # Plot of parameters evolution
            lines = []
            reflines = []
            legend_lines = []
            legend_reflines = []
            expected_lines = []
            expected_reflines = []
            for r in df_learning['replication'].value_counts().index:
                mask = df_learning['replication'] == r
                thetas = df_learning['theta'][mask]
                expected_rewards = df_learning['expected_reward_true'][mask]
                # Linestyles are sorted so that more continuous line means larger rho
                linestyles_ref = ['-', '--', '-.', ':']
                # Rank of rhos in reversed order (largest value has highest rank so that the largest value gets the most solid line in the plot)
                rank_rhos = list(reversed(np.array(rhos).argsort().argsort()))
                linestyles = [linestyles_ref[r] for r in rank_rhos]
                for i in range(len(theta_true)):
                    linestyle_i = linestyles[i % len(linestyles)]
                    lines_i = ax_params.plot([t[i] for t in thetas], symbol, linestyle=linestyle_i)
                    if r == 1:
                        refline_i = ax_params.axhline(theta_true[i], color='black', linestyle=linestyle_i)
                        lines += lines_i
                        reflines += [refline_i]
                        legend_lines += ["Theta_" + str(i+1)]
                        legend_reflines += ["True Optimum Theta_" + str(i + 1)]

                # Plot of objective function evolution (i.e. the expected cost)
                expected_line = ax_objective.plot(-np.array(expected_rewards), symbol, linestyle='-')
                if r == 1:
                    refline = ax_objective.axhline(cost_true, color="black", linestyle="-")
                    expected_lines += expected_line
                    expected_reflines += [refline]

            # Finalize plots
            ax_params.set_xlabel("Learning step")
            ax_params.set_ylabel("theta(s)")
            ylim = ax_params.get_ylim()
            ax_params.set_ylim((-1.1, np.max([ylim[1], np.max(np.squeeze(K_true))])))    # lower limit = -1.1 because -1 is the minimum possible theta value that is allowed for the linear step parameterized policy
            ax_params.xaxis.set_major_locator(MaxNLocator(integer=True))
            #ax_params.set_aspect(1 / ax_params.get_data_ratio())
            ax_params.legend(lines + reflines, legend_lines + legend_reflines)
            #ax_params.set_title("Replications for delta(Q) = 100")

            ax_objective.set_xlabel("Learning step")
            ax_objective.set_ylabel("Expected cost")
            ax_objective.set_title("Expected cost range: [{:.1f}, {:.1f}], average = {:.1f}".format(cost_true, cost_max, cost_mean))
            # ax_objective.set_yscale('log')
            ax_objective.xaxis.set_major_locator(MaxNLocator(integer=True))
            # ax_objective.set_aspect(1 / ax_objective.get_data_ratio())
            ax_objective.legend(expected_lines + expected_reflines, ["Expected cost for the estimated optimum theta(s)", "Optimum expected cost"])

            plt.suptitle(title)

    end_time_all = timer()
    elapsed_time_all = end_time_all - start_time_all
    print("\n+++ OVERALL execution time: {:.1f} min, {:.1f} hours".format(elapsed_time_all / 60, elapsed_time_all / 3600))

    tracemalloc.stop()

    if PLOT_RESULTS_TOGETHER:
        """
        The plots of the FVRL and the respective MC execution are plotted on the same graph
        HOWEVER, THIS IS EXPECTED TO BE RUN MANUALLY AND BY PIECES, AS THE INPUT FILES CONTAINING THE RESULTS TO PLOT
        NEED TO BE CHANGED EVERY TIME WE WANT TO PLOT THE RESULTS.
        """
        # Read the results from files and plot the MC and FV results on the same graph
        resultsdir = "E:/Daniel/Projects/PhD-RL-Toulouse/projects/RL-002-QueueBlocking/results/RL/" + options.queue_system

        theta_true = 19
        # IGA results starting at a larger theta value: theta_start = 39, N = 800, t_sim = 800
        N = 800
        t_sim = N
        results_file_fv = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20211230_001050.csv")
        results_file_mc = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220101_145647.csv")

        # IGA results starting at a small theta value: theta_start = 1, N = 400, t_sim = 400
        N = 400
        t_sim = N
        results_file_fv = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220102_093954.csv")
        results_file_mc = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220102_173144.csv")

        results_fv = pd.read_csv(results_file_fv)
        results_mc = pd.read_csv(results_file_mc)

        t_learn = results_fv.shape[0]
        n_events_mean = np.mean(results_fv['nevents_mc'] + results_fv['nevents_proba'])
        assert n_events_mean == np.mean(results_mc['nevents_mc'])

        plt.figure()
        plt.plot(results_fv['theta'], 'g.-')
        plt.plot(results_mc['theta'], 'r.-')
        ax = plt.gca()
        ax.set_xlabel('Learning step')
        ax.set_ylabel('theta')
        ax.set_ylim((0, 40))
        ax.axhline(theta_true, color='black', linestyle='dashed')
        ax.yaxis.set_major_locator(MaxNLocator(integer=True))
        ax.set_aspect(1 / ax.get_data_ratio())
        ax.legend(["Fleming-Viot", "Monte-Carlo", "Optimum theta"])
        plt.title("# particles N = {}, Simulation time for P(T>t) and E(T_A) = {}, # learning steps = {}, Average number of events per learning step = {:.0f}" \
                    .format(N, t_sim, t_learn, n_events_mean))

    if PLOT_RESULTS_PAPER:
        """
        The plots to show in the paper are generated.
        HOWEVER, THIS IS EXPECTED TO BE RUN MANUALLY AND BY PIECES, AS THE INPUT FILES CONTAINING THE RESULTS TO PLOT
        NEED TO BE CHANGED EVERY TIME WE WANT TO PLOT THE RESULTS.
        """
        # Read the results from files and plot the MC and FV results on the same graph
        resultsdir = "E:/Daniel/Projects/PhD-RL-Toulouse/projects/RL-002-QueueBlocking/results/RL/" + options.queue_system

        # -- Alpha adaptive
        # results_file_fv = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220121_031815_FV-J=0.5K-K=20.csv")
        # results_file_mc = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220121_150307_MC-J=0.5K-K=20.csv")
        results_file_fv = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220125_025611_FV-K=20-J=0.5K.csv")
        results_file_mc = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220125_131828_MC-K=20-J=0.5K.csv")
        K_true = 19  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

        results_file_fv = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220124_040745_FV-K=30-J=0.5K.csv")
        results_file_mc = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220125_161121_MC-K=30-J=0.5K.csv")
        K_true = 19  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

        # All exponents, K0 = 9
        # 2022/02/01 --> But this is wrong because of the error about the true theta, which was not updated to the one we set!
        results_file_fv = os.path.join(os.path.abspath(resultsdir),
                                       "SimulatorQueue_20220125_025523_FV-K0=40-K=10-AlphaAdaptive.csv")
        results_file_mc = os.path.join(os.path.abspath(resultsdir), ".csv")
        K_true = 9  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

        # -- Alpha constant
        # J/K = 0.5, K0 = 19
        results_file_fv = os.path.join(os.path.abspath(resultsdir),
                                       "SimulatorQueue_20220125_121657_FV-K0=20-K=30-J=0.5-AlphaConst.csv")
        results_file_mc = os.path.join(os.path.abspath(resultsdir),
                                       "SimulatorQueue_20220125_123300_MC-K0=20-K=30-J=0.5-AlphaConst.csv")
        K_true = 19  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

        # results_file_fv = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220125_122700_FV-K0=10-K=30-J=0.5-AlphaConst.csv")
        # results_file_mc = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220125_124237_MC-K0=10-K=30-J=0.5-AlphaConst.csv")

        results_file_fv = os.path.join(os.path.abspath(resultsdir),
                                       "SimulatorQueue_20220125_180636_FV-K0=30-K=5-J=0.5-AlphaConst.csv")
        results_file_mc = os.path.join(os.path.abspath(resultsdir),
                                       "SimulatorQueue_20220125_181511_MC-K0=30-K=5-J=0.5-AlphaConst.csv")
        K_true = 24  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

        # J/K = 0.5, K0 = 9
        results_file_fv1 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220130_105312_FV-K0=30-K=10-J=0.5-E1.5-AlphaConst(B=5).csv")
        results_file_mc1 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220130_112523_MC-K0=30-K=10-J=0.5-E1.5-AlphaConst(B=5).csv")
        K_true = 9  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

        # J/K = 0.5, K0 = 19
        results_file_fv1 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220125_190830_FV-K0=20-K=30-J=0.5-E1.5-AlphaConst(B=5).csv")
        results_file_mc1 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220125_192242_MC-K0=20-K=30-J=0.5-E1.5-AlphaConst(B=5).csv")
        results_file_fv2 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220125_193204_FV-K0=20-K=30-J=0.5-E1.0-AlphaConst(B=5).csv")
        results_file_mc2 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220125_200859_MC-K0=20-K=30-J=0.5-E1.0-AlphaConst(B=5).csv")
        results_file_fv3 = os.path.join(os.path.abspath(resultsdir), ".csv")
        results_file_mc3 = os.path.join(os.path.abspath(resultsdir), ".csv")
        K_true = 19  # Optimum blocking size, the integer-valued K at which the expected cost is minimum.
        # NOTE: K_true is NOT exactly theta_true + 1 because theta_true defines xref (if I recall correctly)
        # and the minimum of the expected cost function of K is not always xref + 1, although it is close to it.

        # J/K = 0.3, K0 = 24
        # 2022/10/14: A little after the submission to AISTATS-2023 where we were not able to add these plots
        results_file_fv1 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20221013_190126-K0=24-K=35-J=0.3-E1.0-AlphaConst(B=5)_seed1317.csv")
        results_file_mc1 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20221014_082933_MC-K0=24-K=35-J=0.3-E1.0-AlphaConst(B=5)_seed1317.csv")
        results_file_fv2 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20221013_190211-K0=24-K=35-J=0.3-E1.0-AlphaConst(B=5)_seed1717.csv")
        results_file_mc2 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20221014_083614_MC-K0=24-K=35-J=0.3-E1.0-AlphaConst(B=5)_seed1717.csv")
        K_true = 24  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

        # 2022/10/18: After adding a BURN-IN time (essential for a correct estimation of E(T_A)), with estimation even when the burn-in period CANNOT be satisfied
        # Errors 20% for N and 20% for T
        results_file_fv1 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20221017_224403-K0=24-K=35-J=0.3-E0.2-AlphaConst(B=5)_seed1717.csv")
        results_file_mc1 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20221018_114036_MC-K0=24-K=35-J=0.3-E0.2-AlphaConst(B=5)_seed1717.csv")
        results_file_fv2 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20221017_224434-K0=24-K=35-J=0.3-E0.2-AlphaConst(B=5)_seed1317.csv")
        results_file_mc2 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20221018_114509_MC-K0=24-K=35-J=0.3-E0.2-AlphaConst(B=5)_seed1317.csv")
        results_file_fv3 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20221017_224514-K0=24-K=35-J=0.3-E0.2-AlphaConst(B=5)_seed1313.csv")
        results_file_mc3 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20221018_114531_MC-K0=24-K=35-J=0.3-E0.2-AlphaConst(B=5)_seed1313.csv")
        K_true = 24  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

        # 2022/10/18: After adding a BURN-IN time (essential for a correct estimation of E(T_A)), with estimation ONLY when the burn-in period can be satisfied
        # Errors 100% for N and 100% for T
        results_file_fv1 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20221018_124948-K0=24-K=35-J=0.3-E1.0-AlphaConst(B=5)_seed1717.csv")
        results_file_mc1 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20221018_145520_MC-K0=24-K=35-J=0.3-E1.0-AlphaConst(B=5)_seed1717.csv")
        results_file_fv2 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20221018_125247-K0=24-K=35-J=0.3-E1.0-AlphaConst(B=5)_seed1317.csv")
        results_file_mc2 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20221018_145552_MC-K0=24-K=35-J=0.3-E1.0-AlphaConst(B=5)_seed1317.csv")
        results_file_fv3 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20221018_125307-K0=24-K=35-J=0.3-E1.0-AlphaConst(B=5)_seed1313.csv")
        results_file_mc3 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20221018_145614_MC-K0=24-K=35-J=0.3-E1.0-AlphaConst(B=5)_seed1313.csv")
        K_true = 24  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

        # 2022/10/18: After adding a BURN-IN time (essential for a correct estimation of E(T_A)), with estimation ONLY when the burn-in period can be satisfied
        # Errors 100% for N and 20% for T
        results_file_fv1 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20221018_161511-K0=24-K=35-J=0.3-E1.0,0.2-AlphaConst(B=5)_seed1717.csv")
        results_file_mc1 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20221019_175002_MC-K0=24-K=35-J=0.3-E1.0,0.2-AlphaConst(B=5)_seed1717.csv")
        results_file_fv2 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20221018_161526-K0=24-K=35-J=0.3-E1.0,0.2-AlphaConst(B=5)_seed1317.csv")
        results_file_mc2 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20221019_175022_MC-K0=24-K=35-J=0.3-E1.0,0.2-AlphaConst(B=5)_seed1317.csv")
        results_file_fv3 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20221018_161552-K0=24-K=35-J=0.3-E1.0,0.2-AlphaConst(B=5)_seed1313.csv")
        results_file_mc3 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20221019_175034_MC-K0=24-K=35-J=0.3-E1.0,0.2-AlphaConst(B=5)_seed1313.csv")
        K_true = 24  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

        # J/K = 0.5, K0 = 24
        # These only simulates for 300 learning steps
        # results_file_fv1 = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220125_233513_FV-K0=25-K=35-J=0.5-E1.5-AlphaConst(B=5).csv")
        # results_file_mc1 = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220126_004040_MC-K0=25-K=35-J=0.5-E1.5-AlphaConst(B=5).csv")
        # results_file_fv2 = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220125_235038_FV-K0=25-K=35-J=0.5-E1.0-AlphaConst(B=5).csv")
        # results_file_mc2 = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220126_032802_MC-K0=25-K=35-J=0.5-E1.0-AlphaConst(B=5).csv")

        results_file_fv1 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220126_033710_FV-K0=25-K=35-J=0.5-E1.5-AlphaConst(B=5).csv")
        results_file_mc1 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220126_133352_MC-K0=25-K=35-K=0.5-E1.5-AlphaConst(B=5).csv")
        results_file_fv2 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220126_033755_FV-K0=25-K=35-J=0.5-E1.0-AlphaConst(B=5).csv")
        results_file_mc2 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220126_133409_MC-K0=25-K=35-K=0.5-E1.0-AlphaConst(B=5).csv")
        results_file_fv3 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220125_235230_FV-K0=25-K=35-J=0.5-E0.5-AlphaConst(B=5).csv")
        results_file_mc3 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220127_022406_MC-K0=25-K=35-K=0.5-E0.5-AlphaConst(B=5).csv")
        K_true = 24  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

        # J/K = 0.3, K0 = 19
        # These only simulates for 300 learning steps
        # results_file_fv1 = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220125_235638_FV-K0=20-K=30-J=0.3-E1.5-AlphaConst(B=5).csv")
        # results_file_mc1 = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220126_004118_MC-K0=20-K=30-J=0.3-E1.5-AlphaConst(B=5).csv")
        # results_file_fv2 = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220125_235608_FV-K0=20-K=30-J=0.3-E1.0-AlphaConst(B=5).csv")
        # results_file_mc2 = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220126_023359_MC-K0=20-K=30-J=0.3-E1.0-AlphaConst(B=5).csv")

        results_file_fv1 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220126_135652_FV-K0=20-K=30-J=0.3-E1.5-AlphaConst(B=5).csv")
        results_file_mc1 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220126_193306_MC-K0=20-K=30-J=0.3-E1.5-AlphaConst(B=5).csv")
        results_file_fv2 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220126_135719_FV-K0=20-K=30-J=0.3-E1.0-AlphaConst(B=5).csv")
        results_file_fv2 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220519_165242_FV-K0=20-K=30-J=0.3-E0.5-AlphaConst(B=5)_seed1313.csv")
        results_file_mc2 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220126_193252_MC-K0=20-K=30-J=0.3-E1.0-AlphaConst(B=5).csv")
        results_file_mc2 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220519_203152_MC-K0=20-K=30-J=0.3-E0.5-AlphaConst(B=5)_seed1313.csv")
        K_true = 19  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

        theta_update_strategy = "normal"

        # -- Alpha constant + clipping
        # results_file_fv = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220125_125902_FV-K0=20-K=30-J=0.5-AlphaConst-Clipping.csv")
        # results_file_mc = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220125_132227_MC-K0=20-K=30-J=0.5-AlphaConst-Clipping.csv")

        # J/K = 0.5, K0 = 19
        results_file_fv1 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220125_212158_FV-K0=20-K=30-J=0.5-E1.5-Clipping.csv")
        results_file_mc1 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220125_214819_MC-K0=20-K=30-J=0.5-E1.5-Clipping.csv")
        results_file_fv2 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220125_212353_FV-K0=20-K=30-J=0.5-E1.0-Clipping.csv")
        results_file_mc2 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220125_220710_MC-K0=20-K=30-J=0.5-E1.0-Clipping.csv")
        K_true = 19  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

        # J/K = 0.3, K0 = 19
        results_file_fv1 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220126_035241_FV-K0=20-K=30-J=0.3-E1.5-Clipping.csv")
        results_file_mc1 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220126_131153_MC-K0=20-K=30-J=0.3-E1.5-Clipping.csv")
        results_file_fv2 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220126_035406_FV-K0=20-K=30-J=0.3-E1.0-Clipping.csv")
        results_file_mc2 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220126_131215_MC-K0=20-K=30-J=0.3-E1.0-Clipping.csv")
        K_true = 19  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

        # J/K = 0.5, K0 = 24
        results_file_fv1 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220126_034444_FV-K0=25-K=35-J=0.5-E1.5-Clipping.csv")
        results_file_mc1 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220126_131125_MC-K0=25-K=35-J=0.5-E1.5-Clipping.csv")
        results_file_fv2 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220126_034327_FV-K0=25-K=35-J=0.5-E1.0-Clipping.csv")
        results_file_mc2 = os.path.join(os.path.abspath(resultsdir),
                                        "SimulatorQueue_20220126_131057_MC-K0=25-K=35-J=0.5-E1.0-Clipping.csv")
        K_true = 24  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

        theta_update_strategy = "clipping"

        # Read the data
        results_fv = pd.read_csv(results_file_fv)

        results_fv1 = pd.read_csv(results_file_fv1);
        results_fv1['case'] = 1
        results_mc1 = pd.read_csv(results_file_mc1);
        results_mc1['case'] = 1
        results_fv2 = pd.read_csv(results_file_fv2);
        results_fv2['case'] = 2
        results_mc2 = pd.read_csv(results_file_mc2);
        results_mc2['case'] = 2
        results_fv3 = pd.read_csv(results_file_fv3);
        results_fv3['case'] = 3
        results_mc3 = pd.read_csv(results_file_mc3);
        results_mc3['case'] = 3

        results_fv = pd.concat([results_fv1, results_fv2])
        results_mc = pd.concat([results_mc1, results_mc2])

        results_fv = pd.concat([results_fv1, results_fv2, results_fv3])
        results_mc = pd.concat([results_mc1, results_mc2, results_mc3])

        results_fv = results_fv1
        results_mc = results_mc1

        results_fv = results_fv2
        results_mc = results_mc2

        #-- Parameters used in the plots
        theta_update_strategy = "normal"
        error_nominal_phi = 1.0
        error_nominal_et = 0.2

        # Whether to set specific tick marks for the Y-axis in order to align visually two contiguous plots in the paper
        set_special_axis_ticks = True
        # set_special_axis_ticks = False

        # Whether to show the text box with simulation parameters IN THE PLOT
        show_textbox = True

        t_learn_max = 25
        if t_learn_max is not None:
            results_fv = results_fv[results_fv['t_learn'] <= t_learn_max]
            results_mc = results_mc[results_mc['t_learn'] <= t_learn_max]


        # Plotting process starts
        # (2022/10/18) Note that grouping by N and T have been commented out because N and T could be updated at every learning step
        # (in order to make learning faster)
        all_cases = aggregation_bygroups(results_fv, ['case', 'theta_true', 'J/K', 'exponent'], #, 'N', 'T'],
                                         ['K', 'n_events_mc', 'n_events_fv'],
                                         stats=['count', 'mean', 'std', 'min', 'max'])
        n_events_by_case = aggregation_bygroups(results_mc, ['case'], ['n_events_mc'])

        J_factor_values = all_cases.index.get_level_values('J/K')
        case_values = all_cases.index.get_level_values('case')
        theta_true_values = all_cases.index.get_level_values('theta_true')
        exponent_values = all_cases.index.get_level_values('exponent')
        N_values = [np.max(results_fv.loc[ results_fv['case'] == case, 'N' ]) for case in np.unique(results_fv['case'])]
        T_values = [np.max(results_fv.loc[ results_fv['case'] == case, 'T' ]) for case in np.unique(results_fv['case'])]
        # Cases are sorted from larger error to smaller error
        #case_descs = ['Larger error', 'Mid error', 'Smaller error']
        colors = ['red', 'blue', 'green']
        colors = ['black', 'black', 'black']
        # Linestyles to use for each 'case' stored in results_* data frames
        #linestyles = ['dotted', 'dashed', 'solid']
        # linestyles = ['dashed', 'solid', 'solid']
        linestyles = ['solid', 'dashed', 'solid']
        # Linestyles to use for each method (FV and MC, in this order)
        linestyles_method = ['solid', 'dashed']
        linewidth = 2
        fontsize = 15
        # Number of subplots in the figure as follows:
        # - When subplotting by method, n_subplots = 1 or 2
        # - When subplotting by case, n_subplots = # cases (i.e. len(np.unique(results_fv['case'])))
        n_subplots = len(np.unique(results_fv['case']))
        # Criterion to subplot by: "method" (FV or MC) or case (e.g. one subplot for each replication or for each different parameter setting (e.g. error = 100% and error = 150%)
        subplotby = "case"
        # Shift of the optimum theta when we define the cost as an increasing function of the blocking size
        rho = 0.7
        b = 3.0
        shift_optimum = np.log(-np.log(rho) / (np.log(b) + np.log(rho))) / np.log(b)  # ~ -0.66667 when rho = 0.7 and b = 3.0
        for J_factor in np.unique(J_factor_values):
            cases = case_values[J_factor_values == J_factor]
            ncases = len(cases)

            # IMPORTANT: We assume that the true theta value and the start theta values are ALL the same for all cases
            theta_true = theta_true_values[0] + shift_optimum
            # K_true = int(np.round(theta_true + 1))
            theta_start = results_fv['theta'].iloc[0]
            K_start = int(np.ceil(theta_start + 1))

            axes = plt.figure(figsize=(24, 18)).subplots(1, n_subplots, squeeze=False)
            legend = [[], []]
            figfile = os.path.join(os.path.abspath(resultsdir),
                                   "RL-single-FVMC-K0={}-start={}-J={}-E{:.1f},{:.1f}-{}.jpg" \
                                   .format(K_true, K_start, J_factor, error_nominal_phi, error_nominal_et, theta_update_strategy))
            for idx_case, case in enumerate(cases):
                print("Plotting case {} with idx_case = {}, K_true={}, K_start={}, J={}K".format(case, idx_case, K_true,
                                                                                                 K_start, J_factor))
                # The metadata for the title and legend
                #case_desc = case_descs[idx_case]
                N = N_values[idx_case]
                T = T_values[idx_case]
                n_events_et = all_cases['n_events_mc']['mean'].iloc[idx_case]
                n_events_fv = all_cases['n_events_fv']['mean'].iloc[idx_case]
                n_events_mean = n_events_by_case.iloc[idx_case]['n_events_mc']['mean']

                # The data to plot
                ind_fv = results_fv['case'] == case
                ind_mc = results_mc['case'] == case
                K_start = int(np.ceil(theta_start + 1))
                K_opt_fv = int(np.round(results_fv['theta'][ind_fv].iloc[-1])) + 1  # Optimum K found by the algorithm = closest integer to last theta + 1
                K_opt_mc = int(np.round(results_mc['theta'][ind_mc].iloc[-1])) + 1  # Optimum K found by the algorithm = closest integer to last theta + 1
                x_fv = results_fv['t_learn'][ind_fv]
                x_mc = results_mc['t_learn'][ind_mc]
                y_fv = results_fv['theta'][ind_fv]
                y_mc = results_mc['theta'][ind_mc]
                if subplotby == "method":
                    axes[0][0].plot(x_fv, y_fv, color='green', linestyle=linestyles[idx_case], linewidth=linewidth)
                    axes[0][n_subplots - 1].plot(x_mc, y_mc, color='red', linestyle='dashed', linewidth=linewidth)
                else:
                    assert subplotby == "case"
                    # When setting linestyles by method because each case possibly goes in a different subplot (when n_subplots > 1)
                    axes[0][idx_case].plot(x_fv, y_fv, color='green', linestyle=linestyles_method[0], linewidth=linewidth)
                    axes[0][idx_case].plot(x_mc, y_mc, color='red', linestyle=linestyles_method[1], linewidth=linewidth)
                    axes[0][idx_case].set_title("Replication {}".format(idx_case+1))

                # Errors
                err_phi = results_fv['err_phi'][ind_fv].iloc[0]
                err_et = results_fv['err_et'][ind_fv].iloc[0]

                # legend[0] += ["{}) {}: FVRL (N={}, T={}, error(Phi)={:.0f}%, error(ET)={:.0f}%)" #, avg #events per learning step={})" \
                #                .format(idx_case+1, case_desc, N, T, err_phi*100, err_et*100)] #, n_events_et + n_events_fv)]
                # legend[n_subplots-1] += ["{}) {}: MC (comparable to FVRL case ({})" #, avg #events per learning step={})" \
                #                .format(idx_case+1, case_desc, idx_case+1)] #, n_events_et + n_events_fv)]
                # legend[0] += ["FVRL (N={}, T={}, expected error = {:.0f}%)\n(avg #events per learning step={:.0f})" \
                #                  .format(N, T, error*100, n_events_mean)]
                # legend[n_subplots-1] += ["MC (avg #events per learning step={:.0f})".format(n_events_mean)]
                legend[0] += ["N={}, T={}: expected error Phi = {:.0f}%, expected error E(T_A) = {:.0f}%)\n(avg #events per learning step={:.0f})" \
                                  .format(N, T, error_nominal_phi * 100, error_nominal_et*100, n_events_mean)]

            for idx in range(n_subplots):
                axes[0][idx].set_xlabel('Learning step', fontsize=fontsize)
                axes[0][idx].set_ylabel('theta', fontsize=fontsize)
                for tick in axes[0][idx].xaxis.get_major_ticks():
                    tick.label.set_fontsize(fontsize)
                for tick in axes[0][idx].yaxis.get_major_ticks():
                    tick.label.set_fontsize(fontsize)
                axes[0][idx].set_ylim((0, np.max([axes[0][idx].get_ylim()[1], K_start, K_true])))
                axes[0][idx].axhline(theta_true, color='gray', linestyle='dashed')
                axes[0][idx].yaxis.set_major_locator(MaxNLocator(integer=True))
                axes[0][idx].set_aspect(1 / axes[0][idx].get_data_ratio())
                if set_special_axis_ticks:
                    axes[0][idx].set_yticks(range(0, 36, 5))
                # axes[0][idx].legend(legend[idx] + ["Optimum theta"], fontsize='xx-large', loc='lower right')
                if show_textbox:
                    axes[0][idx].text(axes[0][idx].get_xlim()[1] / 1.7, 5,
                                      "J/K={}, N={}, T={}\nAvg #events per step = {:.0f}\n\nK* = {}" \
                                      #\nEstimated K* (FV) = {}\nEstimated K* (MC) = {}" \
                                      .format(J_factor, N, T, n_events_mean, K_true),
                                      #.format(J_factor, N, T, n_events_mean, K_true, K_opt_fv, K_opt_mc),
                                      horizontalalignment='center',
                                      fontsize=fontsize, bbox={'facecolor': 'none', 'edgecolor': 'black'})
                # plt.title("# particles N = {}, Simulation time for P(T>t) and E(T_A) = {}, # learning steps = {}, Average number of events per learning step = {:.0f}" \
                #          .format(N, 0, 0, 0))

            # To avoid cut off of vertical axis label!!
            # Ref: https://stackoverflow.com/questions/6774086/why-is-my-xlabel-cut-off-in-my-matplotlib-plot
            plt.gcf().subplots_adjust(left=0.15)
            # Save leaving just a little margin (pad_inches=0.1), otherwise we would need to crop the generated figure
            # before including it in a paper.
            # Ref: https://stackoverflow.com/questions/36203597/remove-margins-from-a-matplotlib-figure
            plt.savefig(figfile, bbox_inches="tight", pad_inches=0.1)
            print("Figure saved to file {}".format(figfile))
