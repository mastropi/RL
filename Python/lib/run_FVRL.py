# -*- coding: utf-8 -*-
"""
Created on Sun Jul 11 08:42:57 2022

@author: Daniel Mastropietro
@description: Runs the FVRL algorithm to learn the optimum parameter of a parameterized policy.
"""

import runpy
runpy.run_path('../../setup.py')

import os
import sys
import shutil
import warnings
import tracemalloc

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from matplotlib.ticker import MaxNLocator
from timeit import default_timer as timer

from Python.lib.agents.learners.continuing.fv import LeaFV
from Python.lib.agents.learners.continuing.mc import LeaMC
from Python.lib.agents.learners.policies import LeaPolicyGradient

from Python.lib.agents.policies.parameterized import PolQueueTwoActionsLinearStep

from Python.lib.agents.queues import AgeQueue

from Python.lib.environments.queues import Actions, rewardOnJobRejection_ExponentialCost

from Python.lib.simulators import LearningMethod, define_queue_environment_and_agent
from Python.lib.simulators.queues import compute_nparticles_and_narrivals_for_fv_process, \
    compute_rel_errors_for_fv_process, get_deterministic_blocking_boundaries, LearningMode, SimulatorQueue

from Python.lib.utils.basic import aggregation_bygroups, is_scalar, show_exec_params
from Python.lib.utils.computing import stationary_distribution_birth_death_process
import Python.lib.utils.plotting as plotting


PLOT_GRADIENT = False                   # Whether to plot the estimated gradient of the average reward at each learning step
# Constants that are used for information purposes only: each section started with checking the value of the following
# constants are currently (11-Jul-2022) written to be executed manually, as the data to plot is read from files specified each time.
PLOT_RESULTS_TOGETHER = False           # Whether to plot the theta-learning trajectories of the FVRL and the MC algorithms on the same graph
PLOT_RESULTS_PAPER = False              # Whether to generate the plots with the theta learning for the paper


# ---------------------------- Auxiliary functions ---------------------------#
def run_simulation_policy_learning(simul, replications, dict_params_simul, dict_info,
                                   dict_params_info: dict = {'plot': False, 'log': False},
                                   params_read_from_benchmark_file=False, benchmark=None,
                                   seed=None, verbose=False):
    """

    Arguments:
    simul: simulator object (e.g. of class SimulatorQueue)
        Simulator object used to run the simulation.
        It should have the run() method defined.

    replications: int
        Number of replications to run.

    dict_params_simul: dict
        Set of simulation and estimation parameters.
        This is passed to the `dict_simul` parameter of the simul.run() method.

    dict_info: dict
        Dictionary containing piece of information about the system setup.
        This is passed to the `dict_info` parameter of the simul.run() method.

    dict_params_info: (opt) dict
        Set of parameters related to information handling as it is generated by the simulation process.
        This is passed to the `dict_params_info` parameter of the simul.run() method.
        default: {'plot': False, 'log': False}

    params_read_from_benchmark_file:  bool
        Whether the simulation parameters are read from a benchmark file, normally generated by a prior execution of FVRL.

    benchmark: (opt) pandas DataFrame
        Data frame containing the benchmark information for each replication run for each case of analysis.
        Only used when params_read_from_benchmark_file = True.
        default: None

    seed: (opt) int
        Seed to use at the first learning step for the first replication.
        The seed for the first learning step for subsequent replications is set as `seed + 200*(r - 1)` where r is
        the replication number starting at 1.
        Since the strategy for the seed definition value is only one and used by whether we are running the simulation
        with or without a benchmark file, the seeds used by FVRL and by MC will be the same for each learning step
        of each replication in the analyzed case, so comparison is even fairer than if the seeds between FVRL and MC
        were different.
        default: None

    verbose: (opt) bool
        Whether to be verbose when running the simulation.
        default: False

    Return: Tuple
    Duple with the following elements:
    - theta_opt_values: list containing the optimum theta values found for each replication run on the analyzed case.
    - df_learning: pandas data frame containing the results of the learning process for each replication run on the analyzed case
    as generated by simul.run().
    """
    set_required_entries_simul = {'theta_true', 'theta_start', 'buffer_size_activation_factor', 'nparticles', 't_sim',
                                  'burnin_time_steps'}
    set_required_entries_info = {'case', 'ncases', 'learning_method', 'exponent',
                                 'rhos', 'K_true', 'K', 'error_rel_phi', 'error_rel_et',
                                 'alpha_start', 'adjust_alpha', 'min_time_to_update_alpha', 'alpha_min'}
    if not set_required_entries_simul.issubset(dict_params_simul.keys()):
        raise ValueError("Missing entries in the dict_params_simul dictionary: {}" \
                         .format(set_required_entries_simul.difference(dict_params_simul.keys())))
    if not set_required_entries_info.issubset(dict_info.keys()):
        raise ValueError("Missing entries in the dict_info dictionary: {}" \
                         .format(set_required_entries_info.difference(dict_info.keys())))

    if not params_read_from_benchmark_file:
        # Compute the *real* expected relative error values for the estimation of Phi and the estimation of E(T_A)
        # based on the actual values of the number of particles 'nparticles' and the number of arrival events 't_sim'.
        error_rel_phi_real, error_rel_et_real = compute_rel_errors_for_fv_process(dict_info['rhos'], dict_info['K'],
                                                                                  dict_params_simul['buffer_size_activation_factor'],
                                                                                  dict_params_simul['nparticles'],
                                                                                  dict_params_simul['t_sim'])

        print("\n--> CASE {} of {}: theta_true={} (K_true={}), theta={} (K={}), J/K={:.3f}," \
              " exponent={}: N={} (err_nom={:.1f}%, err={:.1f}%), T={} (err_nom={:.1f}%, err={:.1f}%)" \
              .format(dict_info['case'], dict_info['ncases'], dict_params_simul['theta_true'], dict_info['K_true'],
                      dict_params_simul['theta_start'], dict_info['K'],
                      dict_params_simul['buffer_size_activation_factor'],
                      dict_info['exponent'], dict_params_simul['nparticles'], dict_info['error_rel_phi'] * 100,
                      error_rel_phi_real * 100,
                      dict_params_simul['t_sim'], dict_info['error_rel_et'] * 100, error_rel_et_real * 100))
    else:
        print("\n--> CASE {} of {}: #replications={}, theta_true={} (K_true={}), theta={} (K={}), J/K={:.3f}," \
              " exponent={}: N={}, T={})" \
              .format(dict_info['case'], replications, dict_info['ncases'], dict_params_simul['theta_true'], dict_info['K_true'],
                      dict_params_simul['theta_start'], dict_info['K'],
                      dict_params_simul['buffer_size_activation_factor'],
                      dict_info['exponent'], dict_params_simul['nparticles'], dict_params_simul['t_sim']))
        # The values of N and T are read from the benchmark file and the respective actual errors are also reported there.
        # Here we set these values to NaN because they are shown below as part of the parameter settings.
        error_rel_phi_real = np.nan
        error_rel_et_real = np.nan

    # Store the number of particles and number of arrival events so that we can reset them at the start of every replication
    # (only important when running FVRL, because when running the MC learner, their values are read from the benchmark file)
    nparticles0 = dict_params_simul['nparticles']
    t_sim0 = dict_params_simul['t_sim']

    for r in range(1, replications+1):
        print("\n=== Running replication {} of {} ===".format(r, replications))
        # Reset the simulator object so that we start from scratch and set the simulation case number (used to identify the parameter settings used in the simulation)
        simul.reset(reset_learning_history=True, reset_value_functions=True, reset_counts=True)
        simul.setCase(dict_info['case'])
        simul.setReplication(r)

        # Reset the values of the number of particles and number of arrival events
        # (only important when running FVRL, as in the MC learner case, their values are read from the benchmark file)
        dict_params_simul['nparticles'] = nparticles0
        dict_params_simul['t_sim'] = t_sim0

        # Show execution parameters
        params = dict({
            '0(a)-Seed (for first replication)': seed,
            '1(a)-System-#Servers': simul.getEnv().getNumServers(),
            '1(b)-System-JobClassRates': simul.getEnv().getJobClassRates(),
            '1(c)-System-ServiceRates': simul.getEnv().getServiceRates(),
            '1(d)-System-TrueTheta': dict_params_simul['theta_true'],
            '1(e)-System-TrueK': dict_info['K_true'],
            '2(a)-Learning-Method': dict_info['learning_method'],
            '2(b)-Learning-Method#Particles and % Rel Error Phi': (dict_params_simul['nparticles'], error_rel_phi_real * 100),
            '2(c)-Learning-Method#TimeSteps/ArrivalEvents and % Rel Error E(T)': (dict_params_simul['t_sim'], error_rel_et_real * 100),
            '2(d)-Learning-Method#BurnInSteps (BITS)': dict_params_simul['burnin_time_steps'],
            '2(e)-Learning-Method#MinNumCycles': dict_params_simul['min_num_cycles_for_expectations'],
            '2(f)-Learning-LearningMode': simul.dict_learning_params['mode'].name,
            '2(g)-Learning-ThetaStart': dict_params_simul['theta_start'],
            '2(h)-Learning-#Steps': simul.getNumLearningSteps(),
            '2(i)-Learning-AlphaStart': dict_info['alpha_start'],
            '2(j)-Learning-AdjustAlpha?': dict_info['adjust_alpha'],
            '2(k)-Learning-MinEpisodeToAdjustAlpha': dict_info['min_time_to_update_alpha'],
            '2(l)-Learning-AlphaMin': dict_info['alpha_min'],
        })
        show_exec_params(params)

        if params_read_from_benchmark_file:
            # Get the seed for the first learning step and number of events to run the simulation for (from the benchmark file)
            benchmark_this_case_and_replication = benchmark[(benchmark['case'] == dict_info['case']) & (benchmark['replication'] == r)]
            seed = benchmark_this_case_and_replication['seed'].iloc[0]

            # Number of learning steps
            t_learn = benchmark_this_case_and_replication['t_learn'].iloc[-1]
            simul.setNumLearningSteps(t_learn)

            # Number of steps by learning step
            # This is set to the total number of events observed during the FVRL benchmark divided by the number of learning steps,
            # so that the MC learning has the same number of events at each learning step.
            dict_params_simul['t_sim'] = int( np.ceil( np.sum( list(benchmark_this_case_and_replication['n_events_mc'] +
                                                                    benchmark_this_case_and_replication['n_events_fv'])  )
                                                       / t_learn ) )

        _, _, df_learning = simul.run(dict_params_simul,
                                      dict_params_info=dict_params_info,
                                      dict_info=dict_info,
                                      seed=seed + 200*(r - 1), verbose=verbose)    # We multiply r by 200 in order to reduce the chances of overlapping with other seeds (e.g. the seeds used for each learning step)
        # Add a column with the replication number
        df_learning = pd.concat([pd.DataFrame({'replication': [r] * df_learning.shape[0]}, dtype=int), df_learning], axis=1)

        # Concatenate to gather the results for all replications
        if r == 1:
            df_learning_all = df_learning
        else:
            # We concatenate the new results to all the previous results using ignore_index=False
            # so that each replication has the same index number for each learning step, and this
            # allows us to easily extract the optimum theta found at each replication when returning them below.
            df_learning_all = pd.concat([df_learning_all, df_learning], axis=0, ignore_index=False)

    # Return the optimum theta found for each replication and the data frame containing the results for all replications
    return list( df_learning_all['theta_next'].loc[simul.getNumLearningSteps()-1] ),\
           df_learning_all
# ---------------------------- Auxiliary functions ---------------------------#


# Default execution parameters when no arguments are given in the command line
# Example of execution from the command line:
# python simulators.py 50 FV False 1.0 23.0 33.9 0.5 1.0 1.0
print("User arguments: {}".format(sys.argv))
nargs_required = 3
counter_opt_args = 0
if len(sys.argv) == 1:  # Only the execution file name is contained in sys.argv
    sys.argv += [15]    # t_learn: Number of learning steps
    sys.argv += ["FV"]  # learning_method: estimation method: "FV" or "MC";
                        # when learning_method = "MC", we expect there exists a benchmark file called benchmark_fv.csv
                        # that defines how many events were observed during the equivalent FV learning method
                        # (for fair comparison). If one does not exist, the Monte-Carlo simulation wil be run
                        # no its own and no comparison is expected to be carried out with a previously FV simulation.
    sys.argv += [3]     # Number of replications to run
if len(sys.argv) == nargs_required + counter_opt_args + 1:
    sys.argv += ["benchmark_fv.csv"] # Benchmark filename relative to the resultsdir directory defined below when the learning method is MC (instead of FVRL)
counter_opt_args += 1
if len(sys.argv) == nargs_required + counter_opt_args + 1:
    sys.argv += [False] # clipping: whether to use clipping: False or True
counter_opt_args += 1
if len(sys.argv) == nargs_required + counter_opt_args + 1:
    sys.argv += [1.0]   # clipping_value: clipping value when clipping = True
counter_opt_args += 1
if len(sys.argv) == nargs_required + counter_opt_args + 1:
    sys.argv += [5.0]   # 18.0, 23.0]  # theta_true: true theta (only one value is allowed). NOTE: The optimum theta is NOT precisely `theta_true` as `theta_true` defines the sref value on which the exponential cost is centered. For more details see the notes in the costBlockingExponential() function defined in environments/queues.py.
counter_opt_args += 1
if len(sys.argv) == nargs_required + counter_opt_args + 1:
    sys.argv += [22.1]   # 29.1, 34.1]   # theta_start: non-integral initial theta value for the learning process (only one value is allowed)
counter_opt_args += 1
if len(sys.argv) == nargs_required + counter_opt_args + 1:
    sys.argv += [0.5]   # J_factor: fraction J/K to use in the FV learning method
counter_opt_args += 1
if len(sys.argv) == nargs_required + counter_opt_args + 1:
    sys.argv += [0.5]   # error_rel_phi: expected relative error for the estimation of Phi(t,K) in the FV learning method (1.0 means 100%) --> it defines the number of particles to use
counter_opt_args += 1
if len(sys.argv) == nargs_required + counter_opt_args + 1:
    sys.argv += [0.5]   # error_rel_et: expected relative error for the estimation of E(T) in the FV learning method (1.0 means 100%) --> it defines the number of arrival events to observe in the MC-based simulation to estimate E(T)
counter_opt_args += 1
if len(sys.argv) == nargs_required + counter_opt_args + 1:
    sys.argv += ["nosave"]  # Either "nosave" or anything else for saving the results and log
counter_opt_args += 1
if len(sys.argv) == nargs_required + counter_opt_args + 1:
    sys.argv += [True]  # Whether to save with the datetime in the file name
counter_opt_args += 1
if len(sys.argv) == nargs_required + counter_opt_args + 1:
    sys.argv += [True]  # Whether to plot the learning trajectory (for theta)
counter_opt_args += 1
print("Parsed user arguments: {}".format(sys.argv))
print("")

# -- Parse user arguments
# This function parses a boolean input parameter which, depending on where this script is called from, may be a boolean
# value already (if run from e.g. PyCharm) or may be a string value (if run from the command line).
parse_boolean_parameter = lambda x: isinstance(x, bool) and x or isinstance(x, str) and x == "True"

t_learn = int(sys.argv[1])
learning_method = LearningMethod.FV if sys.argv[2] == "FV" else LearningMethod.MC
replications = int(sys.argv[3])
benchmark_filename = sys.argv[4]
clipping = parse_boolean_parameter(sys.argv[5])
clipping_value = float(sys.argv[6])
theta_true = float(sys.argv[7])
theta_start = float(sys.argv[8])
J_factor = float(sys.argv[9])
error_rel_phi = float(sys.argv[10])
error_rel_et = float(sys.argv[11])
create_log = sys.argv[12] != "nosave"
save_results = sys.argv[12] != "nosave"
save_with_dt = parse_boolean_parameter(sys.argv[13])
plot = parse_boolean_parameter(sys.argv[14])

seed = 1313 # 1317 #1717 #1313  #1859 (for learning step 53+91=144) #1769 (for learning step 53, NOT 52 because it took too long) #1717

print("Execution parameters:")
print("t_learn={}".format(t_learn))
print("#replications={}".format(replications))
print("learning_method={}".format(learning_method.name))
print("clipping={}".format(clipping))
print("clipping_value={}".format(clipping_value))
print("theta_true={}".format(theta_true))
print("theta_start={}".format(theta_start))
print("J_factor={}".format(J_factor))
print("error_rel_phi={}".format(error_rel_phi))
print("error_rel_et={}".format(error_rel_et))
print("benchmark file: {}".format(benchmark_filename))
print("create_log={}".format(create_log))
print("save_results={}".format(save_results))
print("seed={}".format(seed))

# Look for memory leaks
# Ref: https://pythonspeed.com/fil/docs/fil/other-tools.html (from the Fil profiler which also seems interesting)
# Doc: https://docs.python.org/3/library/tracemalloc.html
# tracemalloc.start()

start_time_all = timer()

# ---------------------- OUTPUT FILES --------------------#
# create_log = False;    # In case we need to override the parameter received as argument to the script
logsdir = "../../RL-002-QueueBlocking/logs/RL/single-server"
# save_results = True;   # In case we need to override the parameter received as argument to the script
resultsdir = "../../RL-002-QueueBlocking/results/RL/single-server"
# ---------------------- OUTPUT FILES --------------------#

# -- Parameters defining the environment, policies, learners and agent
# Learning parameters for the value function V
gamma = 1.0

# Learning parameters for the policy P
if learning_method == LearningMethod.FV:
    learnerV = LeaFV
    benchmark_file = None
    plot_trajectories = False
    symbol = 'g-'
else:
    # Monte-Carlo learner is the default
    learnerV = LeaMC
    benchmark_file = os.path.join(os.path.abspath(resultsdir), benchmark_filename) # "SimulatorQueue_20221018_161552-K0=24-K=35-J=0.3-E1.0,0.2-AlphaConst(B=5)_seed1313.csv") #"benchmark_fv.csv")
    if not os.path.exists(benchmark_file):
        warnings.warn("Benchmark file {} does not exist. The Monte-Carlo simulation will run without any reference to a previously run Fleming-Viot simulation".format(benchmark_filename))
        benchmark_file = None
    plot_trajectories = False
    symbol = 'g-'
burnin_time_steps = 20  # Number of burn-in time steps until the Markov process is assumed to be in stationarity regime
                        # (this has an impact in the estimation of expectations --e.g. E(T) in Monte-Carlo simulation
                        # and E(T_A) in Fleming-Viot. In particular, blocking probability would be OVERESTIMATED when
                        # the number of burn-in steps is too small, because the return times T and T_A will be
                        # underestimated, as particles (simulated queues) start close to the state defining the cycles
                        # of interest (i.e. J-1), so returning to those states will take shorter time than the stationary
                        # return time at the beginning of the simulation.
                ### NOTE THAT THIS VALUE burnin_time_steps IS OPTIONAL AS IT IS SET TO A DEFAULT VALUE IN SimulatorQueue.run(). ###
min_num_cycles_for_expectations = 5 # Minimum number of observed cycles to consider that the estimation of expectations
                                    # (such as the expected cycle time or the stationary probability) is reliable.
fixed_window = False
alpha_start = 10.0  # / t_sim  # Use `/ t_sim` when using update of theta at each simulation step (i.e. LeaPolicyGradient.learn_TR() is called instead of LeaPolicyGradient.learn())
adjust_alpha = True  # True
func_adjust_alpha = np.float # np.sqrt
min_time_to_update_alpha = 0  # int(t_learn / 3)
alpha_min = 0.01  # 0.1

dict_params = dict({'environment': {'capacity': np.Inf,
                                    'nservers': 1,  # 3
                                    'job_class_rates': [0.7],  # [0.8, 0.7]
                                    'service_rates': [1.0],  # [1.0, 1.0, 1.0]
                                    'policy_assignment_probabilities': [[1.0]],  # [[0.5, 0.5, 0.0], [0.0, 0.5, 0.5]] )
                                    'reward_func': rewardOnJobRejection_ExponentialCost,
                                    'rewards_accept_by_job_class': None
                                    },
                    'policy': {'parameterized_policy': PolQueueTwoActionsLinearStep if is_scalar(theta_start) else [PolQueueTwoActionsLinearStep for _ in theta_start],
                               'theta': 1.0 if is_scalar(theta_start) else [1.0 for _ in theta_start]  # This value is dummy in the sense that it will be updated below
                               },
                    'learners': {'V': {'learner': learnerV,
                                       'params': {'gamma': 1}
                                       },
                                 'Q': {'learner': None,
                                       'params': {}},
                                 'P': {'learner': LeaPolicyGradient,
                                       'params': {'alpha_start': alpha_start,
                                                  'adjust_alpha': adjust_alpha,
                                                  'func_adjust_alpha': func_adjust_alpha,
                                                  'min_time_to_update_alpha': min_time_to_update_alpha,
                                                  'alpha_min': alpha_min,
                                                  'fixed_window': fixed_window,
                                                  'clipping': clipping,
                                                  'clipping_value': clipping_value,
                                                  }
                                       }
                                 },
                    'agent': {'agent': AgeQueue}
                    })
env_queue, rhos, agent = define_queue_environment_and_agent(dict_params)

# -- Simulation parameters that are common for ALL parameter settings
# t_learn is now defined as input parameter passed to the script
# 2022/01/14: t_learn = 10 times the optimum true theta so that we are supposed to reach that optimum under the REINFORCE_TRUE learning mode with decreasing alpha
# t_learn = 800 #100 #800 #100 #198 - 91 #198 #250 #50
verbose = False
dict_learning_params = dict({'mode': LearningMode.REINFORCE_TRUE, 't_learn': t_learn})
dict_params_info = dict({'plot': False, 'log': False})

# Simulator object
simul = SimulatorQueue(env_queue, agent, dict_learning_params,
                       log=create_log, save=save_results, logsdir=logsdir, resultsdir=resultsdir, debug=False)

if save_results:
    # Initialize the file to store the results (e.g. add the column names of the file)
    # NOTE: We initialize the results file at the very beginning and close it at the very end of the simulation run
    # so that all cases and replications run will be output to the same file! This is convenient as then we will have
    # all results for all cases and replications gathered together.
    simul.initialize_results_file()

# Run the simulations, either from parameters defined by a benchmark file or from parameters defined below
# Define the parameters on which the non-benchmark simulation will be run
# These are defined here (even if we are running the simulation based on a benchmark file)
# because these parameter values are used when naming the results file for both situations.

# In the non-benchmark case (i.e. FVRL) we run the learning method on each set of parameters defined here
# for as many replications defined as input argument.
theta_true_values = [theta_true]  # [24.0 - 1] #[20.0 - 1]  # [32.0-1, 34.0-1, 36.0-1] #[10.0-1, 15.0-1, 20.0-1, 25.0-1, 30.0-1]  # 39.0
theta_start_values = [theta_start]  # [34.9 - 1] #[30.0 - 1] #[20.0 - 1, 25.0 - 1]
# theta_true_values = np.linspace(start=1.0, stop=20.0, num=20)
# When defining the theta values we specify the blocking size K and we substract 1. Recall that K = ceiling(theta+1)
# So, if we want K = 35, we can set theta somewhere between 33+ and 34, so we define e.g. theta = 34.9 - 1
# Note that the list of theta values can contain more than one value, in which case a simulation will be run for each of them
assert len(theta_true_values) == len(theta_start_values), \
    "The number of true theta values ({}) and start theta values ({}) should be the same" \
        .format(len(theta_true_values), len(theta_start_values))
J_factor_values = [J_factor]  # [0.2, 0.3, 0.5]  # [0.2, 0.3, 0.5, 0.7]
NT_exponents = [0]  # [-2, -1, 0, 1]  # Exponents to consider for different N and T values as in exp(exponent)*N0, where N0 is the reference value to achieve a pre-specified relative error
# Accepted relative errors for the estimation of Phi and of E(T_A)
# They define respectively the number of particles N and the number of arrival events T to observe in each learning step.
error_rel_phi = [error_rel_phi]  # [1.0] #0.5
error_rel_et = [error_rel_et]  # [1.0] #0.5

if benchmark_file is None:
    # Output variables of the simulation
    case = 0
    ncases = len(theta_true_values) * len(theta_start_values) * len(J_factor_values) * len(NT_exponents)
    theta_opt_values = [[np.nan] * replications] * ncases  # List of optimum theta values achieved by the learning algorithm for each replication in each parameter setting
    for i, theta_true in enumerate(theta_true_values):
        print("\nSimulating with {} learning on a queue environment with optimum theta (one less the deterministic blocking size) = {}" \
            .format(learning_method.name, theta_true))

        # Set the number of learning steps to double the true theta value
        # Use this ONLY when looking at the MC method and running the learning process on several true theta values
        # to see when the MC method breaks... i.e. when it can no longer learn the optimum theta.
        # The logic behind this choice is that we start at theta = 1.0 and we expect to have a +1 change in
        # theta at every learning step, so we would expect to reach the optimum value after about a number of
        # learning steps equal to the true theta value... so in the end, to give some margin, we allow for as
        # many learning steps as twice the value of true theta parameter.
        # simul.dict_learning_params['t_learn'] = int(theta_true*2)

        for k, theta_start in enumerate(theta_start_values):
            K_true = get_deterministic_blocking_boundaries(simul.agent, theta_true)
            K = get_deterministic_blocking_boundaries(simul.agent, theta_start)
            for j, J_factor in enumerate(J_factor_values):
                N_min = 50; N_max = 500
                T_min = 100; T_max = 5000
                NT_values = [compute_nparticles_and_narrivals_for_fv_process(rhos, K, J_factor, error_rel_phi=err1, error_rel_et=err2)
                             for err1, err2 in zip(error_rel_phi, error_rel_et)]
                for idx_case, (exponent, (N, T)) in enumerate(zip(NT_exponents, NT_values)):
                    print("NT values obtained for the requested expected relative errors (BEFORE BOUNDING):")
                    print("err(phi) = {:.3f}% => N = {}".format(error_rel_phi[idx_case] * 100, NT_values[idx_case][0]))
                    print("err(E(T)) = {:.3f}% => T = {}".format(error_rel_et[idx_case] * 100, NT_values[idx_case][1]))
                    # Lower bound for N and T so that we don't have too little particles!
                    N = min( max(N_min, N), N_max )
                    T = min( max(T_min, T), T_max )
                    # Set the parameters for this run
                    case += 1
                    t_sim = T  # This is used just for the title of plots done below (after the loop)
                    dict_params_simul = {
                        'theta_true': theta_true,
                        'theta_start': theta_start,
                        'buffer_size_activation_factor': J_factor,
                        'nparticles': N,
                        't_sim': T,     # This is the number of arrival events, NOT the number of time steps to use in the estimation of E(T_A) in FV
                                        # In fact, the calculation of T on the basis of the expected relative error in the estimation of E(T_A) gives
                                        # us the number of arrival events (see details in my small dark green notebook in entry dated 06-Nov-2022).
                        'burnin_time_steps': burnin_time_steps,
                        'min_num_cycles_for_expectations': min_num_cycles_for_expectations,
                    }
                    dict_info = {'case': case,
                                 'ncases': ncases,
                                 'learning_method': learning_method.name,
                                 'exponent': exponent,
                                 'rhos': rhos,
                                 'K_true': K_true,
                                 'K': K,
                                 'error_rel_phi': error_rel_phi[idx_case],
                                 'error_rel_et': error_rel_et[idx_case],
                                 'N_min': N_min,
                                 'N_max': N_max,
                                 'T_min': T_min,
                                 'T_max': T_max,
                                 'alpha_start': alpha_start,
                                 'adjust_alpha': adjust_alpha,
                                 'min_time_to_update_alpha': min_time_to_update_alpha,
                                 'alpha_min': alpha_min
                                 }

                    # Run the simulation process
                    theta_opt_values[case - 1], df_learning = run_simulation_policy_learning(simul,
                                                                                             replications,
                                                                                             dict_params_simul,
                                                                                             dict_info,
                                                                                             dict_params_info=dict_params_info,
                                                                                             seed=seed,
                                                                                             verbose=verbose)
else:
    # Read the execution parameters from the benchmark file
    print("Reading benchmark data containing the parameter settings from file\n{}".format(benchmark_file))
    benchmark = pd.read_csv(benchmark_file)
    benchmark_groups = benchmark[(benchmark['t_learn'] == 1) & (benchmark['replication'] == 1)]
    ncases = benchmark_groups.shape[0]
    theta_true_values = np.nan * np.ones(ncases)
    theta_opt_values = [[np.nan] * replications] * ncases  # List of optimum theta values achieved by the learning algorithm for each replication in each parameter setting
    idx_case = -1
    for i in range(ncases):
        idx_case += 1
        case = benchmark_groups['case'].iloc[i]
        replications = len(np.unique(benchmark[benchmark['case'] == case]['replication']))

        # Simulation and estimation parameters that are common for all replications of the current case (group) analyzed
        theta_true = benchmark_groups['theta_true'].iloc[i]
        theta_true_values[idx_case] = theta_true
        theta_start = benchmark_groups['theta'].iloc[i]
        J_factor = benchmark_groups['J/K'].iloc[i]
        exponent = benchmark_groups['exponent'].iloc[i]
        N = benchmark_groups['N'].iloc[i]
        T = benchmark_groups['T'].iloc[i]
        burnin_time_steps = benchmark_groups['burnin_time_steps'].iloc[i]

        K_true = get_deterministic_blocking_boundaries(simul.agent, theta_true)
        K = get_deterministic_blocking_boundaries(simul.agent, theta_start)

        dict_params_simul = {
            'theta_true': theta_true,
            'theta_start': theta_start,
            'buffer_size_activation_factor': J_factor,
            'nparticles': 1,
            't_sim': T,         # This is the 'T' parameter used at the first learning step when running FVRL, and is used only as informational purposes inside
                                # run_simulation_policy_learning() in order for the user to know the characteristics of the case we are currently comparing with MC learning.
                                # It is NOT the simulation time that will be used for MC learning, as this is defined by the actual #events observed during FVRL.
            'burnin_time_steps': burnin_time_steps,
            'min_num_cycles_for_expectations': min_num_cycles_for_expectations,
        }
        dict_info = {'case': case,
                     'ncases': ncases,
                     'learning_method': learning_method.name,
                     'exponent': exponent,
                     'rhos': rhos,
                     'K_true': K_true,
                     'K': K,
                     'error_rel_phi': 0.0,
                     'error_rel_et': 0.0,
                     'alpha_start': alpha_start,
                     'adjust_alpha': adjust_alpha,
                     'min_time_to_update_alpha': min_time_to_update_alpha,
                     'alpha_min': alpha_min
                     }

        # Run the simulation process
        theta_opt_values[idx_case], df_learning = run_simulation_policy_learning(simul,
                                                                                replications,
                                                                                dict_params_simul,
                                                                                dict_info,
                                                                                dict_params_info=dict_params_info,
                                                                                params_read_from_benchmark_file=True,
                                                                                benchmark=benchmark,
                                                                                seed=None,
                                                                                verbose=verbose)
    # Update the value of t_sim so that it stores the number of events observed at each learning step
    # in the *last* MC learning run (i.e. the last replication of the last analyzed case)
    # which is shown in the plots below that e.g. show the evolution of the theta parameter as is being learned.
    t_sim = df_learning['n_events_mc'].iloc[-1] + df_learning['n_events_fv'].iloc[-1]

print("Optimum theta found by the learning algorithm for each replication and each parameter setting:\n{}" \
      .format(pd.DataFrame({'theta_opt': theta_opt_values}, index=range(1, ncases+1))))

# Closes the object (e.g. any log and result files are closed)
simul.close()

if save_results:
    if learning_method == LearningMethod.FV:
        # Make a copy of the results benchmark file to be used for a Monte-Carlo learner
        # so that we can easily refer to it in case we run the MC learning right after the FVRL learner
        # (to this end, the name used for the filename here should be the same as the default filename defined
        # as input argument for the variable benchmark_filename).
        shutil.copyfile(simul.results_file, os.path.join(os.path.dirname(simul.results_file), "benchmark_fv.csv"))

    params_str = learning_method.name + \
        "-theta0={}-theta={}-J={}-E={},{}".format(theta_true_values, theta_start_values, J_factor_values, error_rel_phi, error_rel_et)

    # Now rename the results file to include the parameter settings so that it's easier to identify when analyzing results.
    results_dir = os.path.dirname(simul.results_file)
    results_filename = os.path.basename(simul.results_file)
    if save_with_dt:
        # This allows keeping the datetime string in the filename
        lookfor = ".csv"
    else:
        # This allows removing the datetime string from the filename
        lookfor = "_"
    # Add the parameter settings to the filename, either keeping or removing the execution datetime string
    results_filename_with_parameters = results_filename[:str.index(results_filename, lookfor)] + "_" + params_str + ".csv"
    # Rename and if the file with the new file already exists, delete it first
    results_file_with_parameters = os.path.join(results_dir, results_filename_with_parameters)
    if os.path.exists(results_file_with_parameters):
       os.remove(results_file_with_parameters)
    os.rename(simul.results_file, results_file_with_parameters)

    # Add the parameter settings to the log file as well
    log_dir = os.path.dirname(simul.logfile)
    log_filename = os.path.basename(simul.logfile)
    log_filename_with_parameters = results_filename[:str.index(log_filename, ".log")] + "_" + params_str + ".log"
    log_file_with_parameters = os.path.join(log_dir, log_filename_with_parameters)
    os.rename(simul.logfile, log_file_with_parameters)

if len(theta_true_values) == 1:
    if PLOT_GRADIENT:
        # Save the estimation of G(t) for the last learning step to a file
        # file_results_G = "G.csv"
        # pd.DataFrame({'G': simul.G}).to_csv(file_results_G)

        # -- Plot theta and the gradient of the value function
        SET_YLIM = False

        # Estimated value function
        ax, line_est = plotting.plot_colormap(df_learning['theta'], -df_learning['V'], cmap_name="Blues")

        # True value function
        # Block size for each theta, defined by the fact that K-1 is between theta and theta+1 => K = ceiling(theta+1)
        Ks = [np.int(np.ceil(np.squeeze(t) + 1)) for t in df_learning['theta']]
        # Blocking probability = Pr(K)
        p_stationary = [stationary_distribution_birth_death_process(simul.getEnv().getNumServers(), K, rhos)[1] for K in
                        Ks]
        pblock_K = np.array([p[-1] for p in p_stationary])
        pblock_Km1 = np.array([p[-2] for p in p_stationary])
        # Blocking probability adjusted for different jump rates between K-1 and K (affected by the non-deterministic probability of blocking at K-1)
        pblock_K_adj = np.squeeze(
            [pK * (1 - (K-1-theta)) for K, theta, pK in zip(Ks, df_learning['theta'], pblock_K)])
        pblock_Km1_adj = pblock_Km1  # np.squeeze([pKm1 + pK - pK_adj for pKm1, pK, pK_adj in zip(pblock_Km1, pblock_K, pblock_K_adj)])
        # assert np.allclose(pblock_K + pblock_Km1, pblock_K_adj + pblock_Km1_adj)
        # True value function: expected cost at K which is the buffer size where blocking most likely occurs...
        # (in fact, if theta is say 3.1, the probability of blocking at 4 (= K-1) is small and most blocking
        # will occur at K; if theta is 3.9, the probability of blocking at 4 (= K-1)
        # i.e. we compute at K-1 and NOT at K because we want to compare the true value function
        # with the *estimated* value function when the policy starts blocking at buffer size = theta
        # Vtrue = np.array([rewardOnJobRejection_ExponentialCost(env_queue, (K, None), Actions.REJECT, (K, None)) * pK for K, pK in zip(Ks, pblock_K)])

        # ACTUAL true value function, which takes into account the probability of blocking at K-1 as well, where the policy is non-deterministic (for non-integer theta)
        # The problem with this approach is that the stationary distribution of the chain is NOT the same as with chain
        # where rejection ONLY occurs at s=K... in fact, the transition probabilities to s=K and to s=K-1 when the
        # initial state is s=K-1 are affected by the non-deterministic probability of blocking when s=K-1...
        # Qualitatively, the stationary probability of K would be reduced and the stationary probability of K-1 would be
        # increased by the same amount.
        Vtrue = np.array(
            [rewardOnJobRejection_ExponentialCost(env_queue, (K, None), Actions.REJECT, (K, None)) * pK +
             rewardOnJobRejection_ExponentialCost(env_queue, (K-1, None), Actions.REJECT, (K-1, None)) * (K-1-theta) * pKm1
             for K, theta, pK, pKm1 in zip(Ks, df_learning['theta'], pblock_K_adj, pblock_Km1_adj)])

        # True grad(V)
        # Ref: my hand-written notes in Letter-size block of paper with my notes on the general environment - agent setup
        gradVtrue = [
            -rewardOnJobRejection_ExponentialCost(env_queue, (K-1, None), Actions.REJECT, (K-1, None)) * pKm1 for
            K, pKm1 in zip(Ks, pblock_Km1)]

        ord = np.argsort(Ks)
        # NOTE that we plot the true value function at K-1 (not at K) because K-1 is the value that is closest to theta
        # and we are plotting the *estimated* value function vs. theta (NOT vs. K).
        # line_true, = ax.plot([Ks[o]-1 for o in ord], [-Vtrue[o] for o in ord], 'g.-', linewidth=5, markersize=20)
        line_true, = ax.plot(df_learning['theta'], -Vtrue, 'gx-')  # Use when computing the ACTUAL true Value function V, which also depends on theta!
        ax.set_xlim((0, ax.get_xlim()[1]))
        ax.set_yscale('log')
        # ax.set_ylim((0, 10))
        ax.set_xlabel('theta (for estimated functions) / K-1 for true value function')
        ax.set_ylabel('Value function V (cost)')
        ax.legend([line_est, line_true], ['Estimated V', 'True V'], loc='upper left')
        ax2 = ax.twinx()
        ax2, line_grad = plotting.plot_colormap(df_learning['theta'], -df_learning['gradV'], cmap_name="Reds", ax=ax2)
        line_gradtrue, = ax2.plot([Ks[o] - 1 for o in ord], [-gradVtrue[o] for o in ord], 'k.-', linewidth=3, markersize=12)
        ax2.axhline(0, color="lightgray")
        ax2.set_ylabel('grad(V)')
        if SET_YLIM:
            ax2.set_ylim((-5, 5))  # Note: grad(V) is expected to be -1 or +1...
        ax2.legend([line_grad, line_gradtrue], ['grad(V)', 'True grad(V)'], loc='upper right')
        if is_scalar(t_sim):
            title = "Value function and its gradient as a function of theta and K. " + \
                    "Optimum K = {}, Theta start = {}, t_sim = {:.0f}".format(np.ceil(theta_true+1), theta_start, t_sim)
        else:
            title = "Value function and its gradient as a function of theta and K. " + \
                    "Optimum K = {}, Theta start = {}".format(np.ceil(theta_true+1), theta_start)
        plt.title(title)

        # grad(V) vs. V
        plt.figure()
        plt.plot(-df_learning['V'], -df_learning['gradV'], 'k.')
        ax = plt.gca()
        ax.axhline(0, color="lightgray")
        ax.axvline(0, color="lightgray")
        ax.set_xscale('log')
        # ax.set_xlim((-1, 1))
        if SET_YLIM:
            ax.set_ylim((-1, 1))
        ax.set_xlabel('Value function V (cost)')
        ax.set_ylabel('grad(V)')

    # Plot evolution of theta
    title = "Method: {}, Optimum Theta = {}, Theta start = {}, t_sim = {:.0f}, t_learn = {:.0f}, fixed_window={}, clipping={}" \
                .format(learning_method.name, theta_true, theta_start, t_sim, t_learn, fixed_window, clipping) + \
            (clipping and ", clipping_value={}".format(clipping_value) or "")
    if plot and plot_trajectories:
        "In the case of the MC learner, plot the theta-learning trajectory as well as rewards received during learning"
        assert N == 1, "The simulated system has only one particle (N={})".format(N)
        # NOTE: (2021/11/27) I verified that the values of learnerP.getRewards() for the last learning step
        # are the same to those for learnerV.getRewards() (which only stores the values for the LAST learning step)
        plt.figure()
        # Times at which the trajectory of states is recorded
        times = simul.getLearnerP().getTimes()
        times_unique = np.unique(times)
        assert len(times_unique) == len(simul.getLearnerP().getPolicy().getThetas()) - 1, \
                "The number of unique learning times ({}) is equal to the number of theta updates ({})".format(
                len(times_unique), len(simul.getLearnerP().getPolicy().getThetas()) - 1)
        ## Note that we subtract 1 to the number of learning thetas because the first theta stored in the policy object is the initial theta before any update
        plt.plot(np.r_[0.0, times_unique], simul.getLearnerP().getPolicy().getThetas(), 'b.-')
        ## We add 0.0 as the first time to plot because the first theta value stored in the policy is the initial theta with which the simulation started
        ax = plt.gca()
        # Add vertical lines signalling the BEGINNING of each queue simulation
        times_sim_starts = range(0, t_learn * (t_sim + 1), t_sim + 1)
        for t in times_sim_starts:
            ax.axvline(t, color='lightgray', linestyle='dashed')

        # Buffer sizes
        buffer_sizes = [env_queue.getBufferSizeFromState(s) for s in simul.getLearnerP().getStates()]
        ax.plot(times, buffer_sizes, 'g.', markersize=3)
        # Mark the start of each queue simulation
        # DM-2021/11/28: No longer feasible (or easy to do) because the states are recorded twice for the same time step (namely at the first DEATH after a BIRTH event)
        ax.plot(times_sim_starts, [buffer_sizes[t] for t in times_sim_starts], 'gx')
        ax.set_xlabel("time step")
        ax.set_ylabel("theta")
        ax.yaxis.set_major_locator(MaxNLocator(integer=True))

        # Secondary plot showing the rewards received
        ax2 = ax.twinx()
        ax2.plot(times, -np.array(simul.getLearnerP().getRewards()), 'r-', alpha=0.3)
        # Highlight with points the non-zero rewards
        ax2.plot(times, [-r if r != 0.0 else None for r in simul.getLearnerP().getRewards()], 'r.')
        ax2.set_ylabel("Reward")
        ax2.set_yscale('log')
        plt.title(title)
    elif plot:
        # The evolution of theta is plotted for the last replication run on the last case considered
        # (as the simul object and the value of K_true used below contain the information about precisely such execution)
        plt.figure()
        plt.plot(simul.getLearnerP().getPolicy().getThetas(), symbol)
        plt.title(title)
        ax = plt.gca()
        ax.set_xlabel('Learning step')
        ax.set_ylabel('theta')
        ylim = ax.get_ylim()
        ax.set_ylim((0, np.max([ylim[1], K_true])))
        ax.axhline(theta_true, color='black', linestyle='dashed')  # This is the last true theta value considered for the simulations
        ax.yaxis.set_major_locator(MaxNLocator(integer=True))
        ax.set_aspect(1 / ax.get_data_ratio())

end_time_all = timer()
elapsed_time_all = end_time_all - start_time_all
print("\n+++ OVERALL execution time: {:.1f} min, {:.1f} hours".format(elapsed_time_all / 60, elapsed_time_all / 3600))

tracemalloc.stop()

if PLOT_RESULTS_TOGETHER:
    """
    The plots of the FVRL and the respective MC execution are plotted on the same graph
    HOWEVER, THIS IS EXPECTED TO BE RUN MANUALLY AND BY PIECES, AS THE INPUT FILES CONTAINING THE RESULTS TO PLOT
    NEED TO BE CHANGED EVERY TIME WE WANT TO PLOT THE RESULTS.
    """
    # Read the results from files and plot the MC and FV results on the same graph
    resultsdir = "E:/Daniel/Projects/PhD-RL-Toulouse/projects/RL-002-QueueBlocking/results/RL/single-server"

    theta_true = 19
    # IGA results starting at a larger theta value: theta_start = 39, N = 800, t_sim = 800
    N = 800
    t_sim = N
    results_file_fv = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20211230_001050.csv")
    results_file_mc = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220101_145647.csv")

    # IGA results starting at a small theta value: theta_start = 1, N = 400, t_sim = 400
    N = 400
    t_sim = N
    results_file_fv = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220102_093954.csv")
    results_file_mc = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220102_173144.csv")

    results_fv = pd.read_csv(results_file_fv)
    results_mc = pd.read_csv(results_file_mc)

    t_learn = results_fv.shape[0]
    n_events_mean = np.mean(results_fv['nevents_mc'] + results_fv['nevents_proba'])
    assert n_events_mean == np.mean(results_mc['nevents_mc'])

    plt.figure()
    plt.plot(results_fv['theta'], 'g.-')
    plt.plot(results_mc['theta'], 'r.-')
    ax = plt.gca()
    ax.set_xlabel('Learning step')
    ax.set_ylabel('theta')
    ax.set_ylim((0, 40))
    ax.axhline(theta_true, color='black', linestyle='dashed')
    ax.yaxis.set_major_locator(MaxNLocator(integer=True))
    ax.set_aspect(1 / ax.get_data_ratio())
    ax.legend(["Fleming-Viot", "Monte-Carlo", "Optimum theta"])
    plt.title("# particles N = {}, Simulation time for P(T>t) and E(T_A) = {}, # learning steps = {}, Average number of events per learning step = {:.0f}" \
                .format(N, t_sim, t_learn, n_events_mean))

if PLOT_RESULTS_PAPER:
    """
    The plots to show in the paper are generated.
    HOWEVER, THIS IS EXPECTED TO BE RUN MANUALLY AND BY PIECES, AS THE INPUT FILES CONTAINING THE RESULTS TO PLOT
    NEED TO BE CHANGED EVERY TIME WE WANT TO PLOT THE RESULTS.
    """
    # Read the results from files and plot the MC and FV results on the same graph
    resultsdir = "E:/Daniel/Projects/PhD-RL-Toulouse/projects/RL-002-QueueBlocking/results/RL/single-server"

    # -- Alpha adaptive
    # results_file_fv = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220121_031815_FV-J=0.5K-K=20.csv")
    # results_file_mc = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220121_150307_MC-J=0.5K-K=20.csv")
    results_file_fv = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220125_025611_FV-K=20-J=0.5K.csv")
    results_file_mc = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220125_131828_MC-K=20-J=0.5K.csv")
    K_true = 19  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

    results_file_fv = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220124_040745_FV-K=30-J=0.5K.csv")
    results_file_mc = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220125_161121_MC-K=30-J=0.5K.csv")
    K_true = 19  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

    # All exponents, K0 = 9
    # 2022/02/01 --> But this is wrong because of the error about the true theta, which was not updated to the one we set!
    results_file_fv = os.path.join(os.path.abspath(resultsdir),
                                   "SimulatorQueue_20220125_025523_FV-K0=40-K=10-AlphaAdaptive.csv")
    results_file_mc = os.path.join(os.path.abspath(resultsdir), ".csv")
    K_true = 9  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

    # -- Alpha constant
    # J/K = 0.5, K0 = 19
    results_file_fv = os.path.join(os.path.abspath(resultsdir),
                                   "SimulatorQueue_20220125_121657_FV-K0=20-K=30-J=0.5-AlphaConst.csv")
    results_file_mc = os.path.join(os.path.abspath(resultsdir),
                                   "SimulatorQueue_20220125_123300_MC-K0=20-K=30-J=0.5-AlphaConst.csv")
    K_true = 19  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

    # results_file_fv = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220125_122700_FV-K0=10-K=30-J=0.5-AlphaConst.csv")
    # results_file_mc = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220125_124237_MC-K0=10-K=30-J=0.5-AlphaConst.csv")

    results_file_fv = os.path.join(os.path.abspath(resultsdir),
                                   "SimulatorQueue_20220125_180636_FV-K0=30-K=5-J=0.5-AlphaConst.csv")
    results_file_mc = os.path.join(os.path.abspath(resultsdir),
                                   "SimulatorQueue_20220125_181511_MC-K0=30-K=5-J=0.5-AlphaConst.csv")
    K_true = 24  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

    # J/K = 0.5, K0 = 9
    results_file_fv1 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220130_105312_FV-K0=30-K=10-J=0.5-E1.5-AlphaConst(B=5).csv")
    results_file_mc1 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220130_112523_MC-K0=30-K=10-J=0.5-E1.5-AlphaConst(B=5).csv")
    K_true = 9  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

    # J/K = 0.5, K0 = 19
    results_file_fv1 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220125_190830_FV-K0=20-K=30-J=0.5-E1.5-AlphaConst(B=5).csv")
    results_file_mc1 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220125_192242_MC-K0=20-K=30-J=0.5-E1.5-AlphaConst(B=5).csv")
    results_file_fv2 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220125_193204_FV-K0=20-K=30-J=0.5-E1.0-AlphaConst(B=5).csv")
    results_file_mc2 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220125_200859_MC-K0=20-K=30-J=0.5-E1.0-AlphaConst(B=5).csv")
    results_file_fv3 = os.path.join(os.path.abspath(resultsdir), ".csv")
    results_file_mc3 = os.path.join(os.path.abspath(resultsdir), ".csv")
    K_true = 19  # Optimum blocking size, the integer-valued K at which the expected cost is minimum.
    # NOTE: K_true is NOT exactly theta_true + 1 because theta_true defines xref (if I recall correctly)
    # and the minimum of the expected cost function of K is not always xref + 1, although it is close to it.

    # J/K = 0.3, K0 = 24
    # 2022/10/14: A little after the submission to AISTATS-2023 where we were not able to add these plots
    results_file_fv1 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20221013_190126-K0=24-K=35-J=0.3-E1.0-AlphaConst(B=5)_seed1317.csv")
    results_file_mc1 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20221014_082933_MC-K0=24-K=35-J=0.3-E1.0-AlphaConst(B=5)_seed1317.csv")
    results_file_fv2 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20221013_190211-K0=24-K=35-J=0.3-E1.0-AlphaConst(B=5)_seed1717.csv")
    results_file_mc2 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20221014_083614_MC-K0=24-K=35-J=0.3-E1.0-AlphaConst(B=5)_seed1717.csv")
    K_true = 24  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

    # 2022/10/18: After adding a BURN-IN time (essential for a correct estimation of E(T_A)), with estimation even when the burn-in period CANNOT be satisfied
    # Errors 20% for N and 20% for T
    results_file_fv1 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20221017_224403-K0=24-K=35-J=0.3-E0.2-AlphaConst(B=5)_seed1717.csv")
    results_file_mc1 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20221018_114036_MC-K0=24-K=35-J=0.3-E0.2-AlphaConst(B=5)_seed1717.csv")
    results_file_fv2 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20221017_224434-K0=24-K=35-J=0.3-E0.2-AlphaConst(B=5)_seed1317.csv")
    results_file_mc2 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20221018_114509_MC-K0=24-K=35-J=0.3-E0.2-AlphaConst(B=5)_seed1317.csv")
    results_file_fv3 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20221017_224514-K0=24-K=35-J=0.3-E0.2-AlphaConst(B=5)_seed1313.csv")
    results_file_mc3 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20221018_114531_MC-K0=24-K=35-J=0.3-E0.2-AlphaConst(B=5)_seed1313.csv")
    K_true = 24  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

    # 2022/10/18: After adding a BURN-IN time (essential for a correct estimation of E(T_A)), with estimation ONLY when the burn-in period can be satisfied
    # Errors 100% for N and 100% for T
    results_file_fv1 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20221018_124948-K0=24-K=35-J=0.3-E1.0-AlphaConst(B=5)_seed1717.csv")
    results_file_mc1 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20221018_145520_MC-K0=24-K=35-J=0.3-E1.0-AlphaConst(B=5)_seed1717.csv")
    results_file_fv2 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20221018_125247-K0=24-K=35-J=0.3-E1.0-AlphaConst(B=5)_seed1317.csv")
    results_file_mc2 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20221018_145552_MC-K0=24-K=35-J=0.3-E1.0-AlphaConst(B=5)_seed1317.csv")
    results_file_fv3 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20221018_125307-K0=24-K=35-J=0.3-E1.0-AlphaConst(B=5)_seed1313.csv")
    results_file_mc3 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20221018_145614_MC-K0=24-K=35-J=0.3-E1.0-AlphaConst(B=5)_seed1313.csv")
    K_true = 24  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

    # 2022/10/18: After adding a BURN-IN time (essential for a correct estimation of E(T_A)), with estimation ONLY when the burn-in period can be satisfied
    # Errors 100% for N and 20% for T
    results_file_fv1 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20221018_161511-K0=24-K=35-J=0.3-E1.0,0.2-AlphaConst(B=5)_seed1717.csv")
    results_file_mc1 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20221019_175002_MC-K0=24-K=35-J=0.3-E1.0,0.2-AlphaConst(B=5)_seed1717.csv")
    results_file_fv2 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20221018_161526-K0=24-K=35-J=0.3-E1.0,0.2-AlphaConst(B=5)_seed1317.csv")
    results_file_mc2 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20221019_175022_MC-K0=24-K=35-J=0.3-E1.0,0.2-AlphaConst(B=5)_seed1317.csv")
    results_file_fv3 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20221018_161552-K0=24-K=35-J=0.3-E1.0,0.2-AlphaConst(B=5)_seed1313.csv")
    results_file_mc3 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20221019_175034_MC-K0=24-K=35-J=0.3-E1.0,0.2-AlphaConst(B=5)_seed1313.csv")
    K_true = 24  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

    # J/K = 0.5, K0 = 24
    # These only simulates for 300 learning steps
    # results_file_fv1 = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220125_233513_FV-K0=25-K=35-J=0.5-E1.5-AlphaConst(B=5).csv")
    # results_file_mc1 = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220126_004040_MC-K0=25-K=35-J=0.5-E1.5-AlphaConst(B=5).csv")
    # results_file_fv2 = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220125_235038_FV-K0=25-K=35-J=0.5-E1.0-AlphaConst(B=5).csv")
    # results_file_mc2 = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220126_032802_MC-K0=25-K=35-J=0.5-E1.0-AlphaConst(B=5).csv")

    results_file_fv1 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220126_033710_FV-K0=25-K=35-J=0.5-E1.5-AlphaConst(B=5).csv")
    results_file_mc1 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220126_133352_MC-K0=25-K=35-K=0.5-E1.5-AlphaConst(B=5).csv")
    results_file_fv2 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220126_033755_FV-K0=25-K=35-J=0.5-E1.0-AlphaConst(B=5).csv")
    results_file_mc2 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220126_133409_MC-K0=25-K=35-K=0.5-E1.0-AlphaConst(B=5).csv")
    results_file_fv3 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220125_235230_FV-K0=25-K=35-J=0.5-E0.5-AlphaConst(B=5).csv")
    results_file_mc3 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220127_022406_MC-K0=25-K=35-K=0.5-E0.5-AlphaConst(B=5).csv")
    K_true = 24  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

    # J/K = 0.3, K0 = 19
    # These only simulates for 300 learning steps
    # results_file_fv1 = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220125_235638_FV-K0=20-K=30-J=0.3-E1.5-AlphaConst(B=5).csv")
    # results_file_mc1 = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220126_004118_MC-K0=20-K=30-J=0.3-E1.5-AlphaConst(B=5).csv")
    # results_file_fv2 = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220125_235608_FV-K0=20-K=30-J=0.3-E1.0-AlphaConst(B=5).csv")
    # results_file_mc2 = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220126_023359_MC-K0=20-K=30-J=0.3-E1.0-AlphaConst(B=5).csv")

    results_file_fv1 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220126_135652_FV-K0=20-K=30-J=0.3-E1.5-AlphaConst(B=5).csv")
    results_file_mc1 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220126_193306_MC-K0=20-K=30-J=0.3-E1.5-AlphaConst(B=5).csv")
    results_file_fv2 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220126_135719_FV-K0=20-K=30-J=0.3-E1.0-AlphaConst(B=5).csv")
    results_file_fv2 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220519_165242_FV-K0=20-K=30-J=0.3-E0.5-AlphaConst(B=5)_seed1313.csv")
    results_file_mc2 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220126_193252_MC-K0=20-K=30-J=0.3-E1.0-AlphaConst(B=5).csv")
    results_file_mc2 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220519_203152_MC-K0=20-K=30-J=0.3-E0.5-AlphaConst(B=5)_seed1313.csv")
    K_true = 19  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

    theta_update_strategy = "normal"

    # -- Alpha constant + clipping
    # results_file_fv = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220125_125902_FV-K0=20-K=30-J=0.5-AlphaConst-Clipping.csv")
    # results_file_mc = os.path.join(os.path.abspath(resultsdir), "SimulatorQueue_20220125_132227_MC-K0=20-K=30-J=0.5-AlphaConst-Clipping.csv")

    # J/K = 0.5, K0 = 19
    results_file_fv1 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220125_212158_FV-K0=20-K=30-J=0.5-E1.5-Clipping.csv")
    results_file_mc1 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220125_214819_MC-K0=20-K=30-J=0.5-E1.5-Clipping.csv")
    results_file_fv2 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220125_212353_FV-K0=20-K=30-J=0.5-E1.0-Clipping.csv")
    results_file_mc2 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220125_220710_MC-K0=20-K=30-J=0.5-E1.0-Clipping.csv")
    K_true = 19  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

    # J/K = 0.3, K0 = 19
    results_file_fv1 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220126_035241_FV-K0=20-K=30-J=0.3-E1.5-Clipping.csv")
    results_file_mc1 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220126_131153_MC-K0=20-K=30-J=0.3-E1.5-Clipping.csv")
    results_file_fv2 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220126_035406_FV-K0=20-K=30-J=0.3-E1.0-Clipping.csv")
    results_file_mc2 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220126_131215_MC-K0=20-K=30-J=0.3-E1.0-Clipping.csv")
    K_true = 19  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

    # J/K = 0.5, K0 = 24
    results_file_fv1 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220126_034444_FV-K0=25-K=35-J=0.5-E1.5-Clipping.csv")
    results_file_mc1 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220126_131125_MC-K0=25-K=35-J=0.5-E1.5-Clipping.csv")
    results_file_fv2 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220126_034327_FV-K0=25-K=35-J=0.5-E1.0-Clipping.csv")
    results_file_mc2 = os.path.join(os.path.abspath(resultsdir),
                                    "SimulatorQueue_20220126_131057_MC-K0=25-K=35-J=0.5-E1.0-Clipping.csv")
    K_true = 24  # Optimum blocking size, the integer-valued K at which the expected cost is minimum

    theta_update_strategy = "clipping"

    # Read the data
    results_fv = pd.read_csv(results_file_fv)

    results_fv1 = pd.read_csv(results_file_fv1);
    results_fv1['case'] = 1
    results_mc1 = pd.read_csv(results_file_mc1);
    results_mc1['case'] = 1
    results_fv2 = pd.read_csv(results_file_fv2);
    results_fv2['case'] = 2
    results_mc2 = pd.read_csv(results_file_mc2);
    results_mc2['case'] = 2
    results_fv3 = pd.read_csv(results_file_fv3);
    results_fv3['case'] = 3
    results_mc3 = pd.read_csv(results_file_mc3);
    results_mc3['case'] = 3

    results_fv = pd.concat([results_fv1, results_fv2])
    results_mc = pd.concat([results_mc1, results_mc2])

    results_fv = pd.concat([results_fv1, results_fv2, results_fv3])
    results_mc = pd.concat([results_mc1, results_mc2, results_mc3])

    results_fv = results_fv1
    results_mc = results_mc1

    results_fv = results_fv2
    results_mc = results_mc2

    #-- Parameters used in the plots
    theta_update_strategy = "normal"
    error_nominal_phi = 1.0
    error_nominal_et = 0.2

    # Whether to set specific tick marks for the Y-axis in order to align visually two contiguous plots in the paper
    set_special_axis_ticks = True
    # set_special_axis_ticks = False

    # Whether to show the text box with simulation parameters IN THE PLOT
    show_textbox = True

    t_learn_max = 25
    if t_learn_max is not None:
        results_fv = results_fv[results_fv['t_learn'] <= t_learn_max]
        results_mc = results_mc[results_mc['t_learn'] <= t_learn_max]


    # Plotting process starts
    # (2022/10/18) Note that grouping by N and T have been commented out because N and T could be updated at every learning step
    # (in order to make learning faster)
    all_cases = aggregation_bygroups(results_fv, ['case', 'theta_true', 'J/K', 'exponent'], #, 'N', 'T'],
                                     ['K', 'n_events_mc', 'n_events_fv'],
                                     stats=['count', 'mean', 'std', 'min', 'max'])
    n_events_by_case = aggregation_bygroups(results_mc, ['case'], ['n_events_mc'])

    J_factor_values = all_cases.index.get_level_values('J/K')
    case_values = all_cases.index.get_level_values('case')
    theta_true_values = all_cases.index.get_level_values('theta_true')
    exponent_values = all_cases.index.get_level_values('exponent')
    N_values = [np.max(results_fv.loc[ results_fv['case'] == case, 'N' ]) for case in np.unique(results_fv['case'])]
    T_values = [np.max(results_fv.loc[ results_fv['case'] == case, 'T' ]) for case in np.unique(results_fv['case'])]
    # Cases are sorted from larger error to smaller error
    #case_descs = ['Larger error', 'Mid error', 'Smaller error']
    colors = ['red', 'blue', 'green']
    colors = ['black', 'black', 'black']
    # Linestyles to use for each 'case' stored in results_* data frames
    #linestyles = ['dotted', 'dashed', 'solid']
    # linestyles = ['dashed', 'solid', 'solid']
    linestyles = ['solid', 'dashed', 'solid']
    # Linestyles to use for each method (FV and MC, in this order)
    linestyles_method = ['solid', 'dashed']
    linewidth = 2
    fontsize = 15
    # Number of subplots in the figure as follows:
    # - When subplotting by method, n_subplots = 1 or 2
    # - When subplotting by case, n_subplots = # cases (i.e. len(np.unique(results_fv['case'])))
    n_subplots = len(np.unique(results_fv['case']))
    # Criterion to subplot by: "method" (FV or MC) or case (e.g. one subplot for each replication or for each different parameter setting (e.g. error = 100% and error = 150%)
    subplotby = "case"
    # Shift of the optimum theta when we define the cost as an increasing function of the blocking size
    rho = 0.7
    b = 3.0
    shift_optimum = np.log(-np.log(rho) / (np.log(b) + np.log(rho))) / np.log(b)  # ~ -0.66667 when rho = 0.7 and b = 3.0
    for J_factor in np.unique(J_factor_values):
        cases = case_values[J_factor_values == J_factor]
        ncases = len(cases)

        # IMPORTANT: We assume that the true theta value and the start theta values are ALL the same for all cases
        theta_true = theta_true_values[0] + shift_optimum
        # K_true = int(np.ceil(theta_true + 1))
        theta_start = results_fv['theta'].iloc[0]
        K_start = int(np.ceil(theta_start + 1))

        axes = plt.figure(figsize=(24, 18)).subplots(1, n_subplots, squeeze=False)
        legend = [[], []]
        figfile = os.path.join(os.path.abspath(resultsdir),
                               "RL-single-FVMC-K0={}-start={}-J={}-E{:.1f},{:.1f}-{}.jpg" \
                               .format(K_true, K_start, J_factor, error_nominal_phi, error_nominal_et, theta_update_strategy))
        for idx_case, case in enumerate(cases):
            print("Plotting case {} with idx_case = {}, K_true={}, K_start={}, J={}K".format(case, idx_case, K_true,
                                                                                             K_start, J_factor))
            # The metadata for the title and legend
            #case_desc = case_descs[idx_case]
            N = N_values[idx_case]
            T = T_values[idx_case]
            n_events_et = all_cases['n_events_mc']['mean'].iloc[idx_case]
            n_events_fv = all_cases['n_events_fv']['mean'].iloc[idx_case]
            n_events_mean = n_events_by_case.iloc[idx_case]['n_events_mc']['mean']

            # The data to plot
            ind_fv = results_fv['case'] == case
            ind_mc = results_mc['case'] == case
            K_start = int(np.ceil(theta_start + 1))
            K_opt_fv = int(np.round(results_fv['theta'][ind_fv].iloc[-1])) + 1  # Optimum K found by the algorithm = closest integer to last theta + 1
            K_opt_mc = int(np.round(results_mc['theta'][ind_mc].iloc[-1])) + 1  # Optimum K found by the algorithm = closest integer to last theta + 1
            x_fv = results_fv['t_learn'][ind_fv]
            x_mc = results_mc['t_learn'][ind_mc]
            y_fv = results_fv['theta'][ind_fv]
            y_mc = results_mc['theta'][ind_mc]
            if subplotby == "method":
                axes[0][0].plot(x_fv, y_fv, color='green', linestyle=linestyles[idx_case], linewidth=linewidth)
                axes[0][n_subplots - 1].plot(x_mc, y_mc, color='red', linestyle='dashed', linewidth=linewidth)
            else:
                assert subplotby == "case"
                # When setting linestyles by method because each case possibly goes in a different subplot (when n_subplots > 1)
                axes[0][idx_case].plot(x_fv, y_fv, color='green', linestyle=linestyles_method[0], linewidth=linewidth)
                axes[0][idx_case].plot(x_mc, y_mc, color='red', linestyle=linestyles_method[1], linewidth=linewidth)
                axes[0][idx_case].set_title("Replication {}".format(idx_case+1))

            # Errors
            err_phi = results_fv['err_phi'][ind_fv].iloc[0]
            err_et = results_fv['err_et'][ind_fv].iloc[0]

            # legend[0] += ["{}) {}: FVRL (N={}, T={}, error(Phi)={:.0f}%, error(ET)={:.0f}%)" #, avg #events per learning step={})" \
            #                .format(idx_case+1, case_desc, N, T, err_phi*100, err_et*100)] #, n_events_et + n_events_fv)]
            # legend[n_subplots-1] += ["{}) {}: MC (comparable to FVRL case ({})" #, avg #events per learning step={})" \
            #                .format(idx_case+1, case_desc, idx_case+1)] #, n_events_et + n_events_fv)]
            # legend[0] += ["FVRL (N={}, T={}, expected error = {:.0f}%)\n(avg #events per learning step={:.0f})" \
            #                  .format(N, T, error*100, n_events_mean)]
            # legend[n_subplots-1] += ["MC (avg #events per learning step={:.0f})".format(n_events_mean)]
            legend[0] += ["N={}, T={}: expected error Phi = {:.0f}%, expected error E(T_A) = {:.0f}%)\n(avg #events per learning step={:.0f})" \
                              .format(N, T, error_nominal_phi * 100, error_nominal_et*100, n_events_mean)]

        for idx in range(n_subplots):
            axes[0][idx].set_xlabel('Learning step', fontsize=fontsize)
            axes[0][idx].set_ylabel('theta', fontsize=fontsize)
            for tick in axes[0][idx].xaxis.get_major_ticks():
                tick.label.set_fontsize(fontsize)
            for tick in axes[0][idx].yaxis.get_major_ticks():
                tick.label.set_fontsize(fontsize)
            axes[0][idx].set_ylim((0, np.max([axes[0][idx].get_ylim()[1], K_start, K_true])))
            axes[0][idx].axhline(theta_true, color='gray', linestyle='dashed')
            axes[0][idx].yaxis.set_major_locator(MaxNLocator(integer=True))
            axes[0][idx].set_aspect(1 / axes[0][idx].get_data_ratio())
            if set_special_axis_ticks:
                axes[0][idx].set_yticks(range(0, 36, 5))
            # axes[0][idx].legend(legend[idx] + ["Optimum theta"], fontsize='xx-large', loc='lower right')
            if show_textbox:
                axes[0][idx].text(axes[0][idx].get_xlim()[1] / 1.7, 5,
                                  "J/K={}, N={}, T={}\nAvg #events per step = {:.0f}\n\nK* = {}" \
                                  #\nEstimated K* (FV) = {}\nEstimated K* (MC) = {}" \
                                  .format(J_factor, N, T, n_events_mean, K_true),
                                  #.format(J_factor, N, T, n_events_mean, K_true, K_opt_fv, K_opt_mc),
                                  horizontalalignment='center',
                                  fontsize=fontsize, bbox={'facecolor': 'none', 'edgecolor': 'black'})
            # plt.title("# particles N = {}, Simulation time for P(T>t) and E(T_A) = {}, # learning steps = {}, Average number of events per learning step = {:.0f}" \
            #          .format(N, 0, 0, 0))

        # To avoid cut off of vertical axis label!!
        # Ref: https://stackoverflow.com/questions/6774086/why-is-my-xlabel-cut-off-in-my-matplotlib-plot
        plt.gcf().subplots_adjust(left=0.15)
        # Save leaving just a little margin (pad_inches=0.1), otherwise we would need to crop the generated figure
        # before including it in a paper.
        # Ref: https://stackoverflow.com/questions/36203597/remove-margins-from-a-matplotlib-figure
        plt.savefig(figfile, bbox_inches="tight", pad_inches=0.1)
        print("Save figure to file {}".format(figfile))
